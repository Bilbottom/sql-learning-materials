{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SQL Learning Materials","text":"<p>Success</p> <p>SQL scripts that demonstrate various features and concepts.</p> <p>This project contains a bunch of SQL learning materials aimed at different levels of experience and covering a variety of topics. It focuses on just writing <code>SELECT</code> statements so there will be very few resources for anything else.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>The data used in this project is from a couple of sources.</p> <p>The SQL Server instance has the ubiquitous AdventureWorks databases (the transactional one and the analytical one), which are available from various Microsoft pages:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/samples/adventureworks-install-configure</li> <li>https://github.com/Microsoft/sql-server-samples/releases/download/adventureworks</li> </ul> <p>The PostgreSQL instance has the similarly ubiquitous Sakila database, which is available from the following GitHub repo:</p> <ul> <li>https://github.com/jOOQ/sakila/</li> </ul> <p>All credit for the data goes to the respective owners, and these sources should be consulted for any documentation you need around the data.</p>"},{"location":"#contents","title":"Contents","text":"<p>This project has a few different series:</p> <ul> <li>From Excel to SQL, which is aimed at Excel users who want to learn SQL</li> <li>Everything about joins, which is a comprehensive guide to SQL joins</li> <li>Challenging SQL questions, which are SQL questions that really test your skills</li> </ul>"},{"location":"challenging-sql-problems/challenging-sql-problems/","title":"Challenging SQL problems","text":"<p>Warning</p> <p>These questions are not for people new to SQL! Most of them expect you to use advanced SQL techniques that aren't widely known.</p> <p>Note</p> <p>The database versions used for the solutions can be found at the top of the following page:</p> <ul> <li>https://github.com/Bilbottom/sql-learning-materials/blob/main/README.md</li> </ul>"},{"location":"challenging-sql-problems/challenging-sql-problems/#problems","title":"Problems","text":""},{"location":"challenging-sql-problems/challenging-sql-problems/#bronze-tier","title":"\ud83d\udfe4 Bronze Tier","text":"<p>As long as you know your database features and query patterns, these are fairly straightforward.</p> <ol> <li>Temperature anomaly detection</li> <li>Outstanding invoices</li> <li>Personalised customer emails</li> <li>Suspicious login activity</li> <li>Combining dimensions</li> <li>Customer churn</li> <li>Fibonacci sequence</li> <li>UK bank holidays</li> </ol>"},{"location":"challenging-sql-problems/challenging-sql-problems/#silver-tier","title":"\u26aa Silver Tier","text":"<p>These require a bit more thinking.</p> <ol> <li>Metric correlation</li> <li>Funnel analytics</li> <li>Bannable login activity</li> <li>Bus routes</li> <li>Decoding datelist ints</li> <li>Region precipitation</li> <li>Predicting values</li> <li>Mandelbrot set</li> <li>Customer sales running totals</li> </ol>"},{"location":"challenging-sql-problems/challenging-sql-problems/#gold-tier","title":"\ud83d\udfe1 Gold Tier","text":"<p>Expect to spend a bit of time on these.</p> <ol> <li>Loan repayment schedules</li> <li>Risk invasions</li> <li>Supply chain network</li> <li>Encoding datelist ints</li> <li>Travel plans</li> </ol>"},{"location":"challenging-sql-problems/challenging-sql-problems/#questions-by-topic","title":"Questions by topic","text":"<p>To see the questions grouped by topic, head over to the topics page.</p>"},{"location":"challenging-sql-problems/challenging-sql-problems/#looking-for-more","title":"Looking for more?","text":"<p>The repo Bilbottom/sql-models has a collection of custom SQL models, some of which come with exercises.</p> <p>For example, the loans model has exercises ranging from very easy to very hard:</p> <ul> <li>https://github.com/Bilbottom/sql-models/tree/main/sql_models/loans#exercises-</li> </ul>"},{"location":"challenging-sql-problems/topics/","title":"Questions by topic","text":"<p>Note: a question can appear in multiple topics.</p>"},{"location":"challenging-sql-problems/topics/#tag:asof-join","title":"asof join","text":"<ul> <li>            Combining dimensions \ud83d\udd17          </li> <li>            Outstanding invoices \ud83d\udcb1          </li> </ul>"},{"location":"challenging-sql-problems/topics/#tag:bitshift","title":"bitshift","text":"<ul> <li>            Customer churn \ud83d\udd04          </li> <li>            Decoding datelist ints \ud83d\udd13          </li> </ul>"},{"location":"challenging-sql-problems/topics/#tag:correlated-subquery","title":"correlated subquery","text":"<ul> <li>            Encoding datelist ints \ud83d\udd10          </li> </ul>"},{"location":"challenging-sql-problems/topics/#tag:correlation","title":"correlation","text":"<ul> <li>            Metric correlation \ud83d\udd00          </li> </ul>"},{"location":"challenging-sql-problems/topics/#tag:custom-axis","title":"custom axis","text":"<ul> <li>            Combining dimensions \ud83d\udd17          </li> <li>            Encoding datelist ints \ud83d\udd10          </li> <li>            Funnel analytics \u23ec          </li> <li>            Risk invasions \ud83d\udee1\ufe0f          </li> </ul>"},{"location":"challenging-sql-problems/topics/#tag:gaps-and-islands","title":"gaps and islands","text":"<ul> <li>            Bannable login activity \u274c          </li> <li>            Encoding datelist ints \ud83d\udd10          </li> <li>            Suspicious login activity \ud83e\udd14          </li> </ul>"},{"location":"challenging-sql-problems/topics/#tag:linear-regression","title":"linear regression","text":"<ul> <li>            Predicting values \ud83c\udfb1          </li> </ul>"},{"location":"challenging-sql-problems/topics/#tag:pivot-and-unpivot","title":"pivot and unpivot","text":"<ul> <li>            Metric correlation \ud83d\udd00          </li> <li>            Predicting values \ud83c\udfb1          </li> <li>            Region precipitation \u2614          </li> <li>            UK bank holidays \ud83d\udcc5          </li> </ul>"},{"location":"challenging-sql-problems/topics/#tag:recursive-cte","title":"recursive CTE","text":"<ul> <li>            Bus routes \ud83d\ude8c          </li> <li>            Decoding datelist ints \ud83d\udd13          </li> <li>            Fibonacci sequence \ud83d\udd22          </li> <li>            Loan repayment schedule \ud83d\udcb0          </li> <li>            Mandelbrot set \ud83c\udf00          </li> <li>            Risk invasions \ud83d\udee1\ufe0f          </li> <li>            Travel plans \ud83d\ude82          </li> </ul>"},{"location":"challenging-sql-problems/topics/#tag:rollup","title":"rollup","text":"<ul> <li>            Region precipitation \u2614          </li> </ul>"},{"location":"challenging-sql-problems/topics/#tag:semi-structured-data","title":"semi-structured data","text":"<ul> <li>            UK bank holidays \ud83d\udcc5          </li> </ul>"},{"location":"challenging-sql-problems/topics/#tag:string-similarity","title":"string similarity","text":"<ul> <li>            Personalised customer emails \ud83d\udce8          </li> </ul>"},{"location":"challenging-sql-problems/topics/#tag:window-function","title":"window function","text":"<ul> <li>            Bannable login activity \u274c          </li> <li>            Customer sales running totals \ud83d\udcc8          </li> <li>            Encoding datelist ints \ud83d\udd10          </li> <li>            Risk invasions \ud83d\udee1\ufe0f          </li> <li>            Suspicious login activity \ud83e\udd14          </li> <li>            Temperature anomaly detection \ud83d\udd0d          </li> </ul>"},{"location":"challenging-sql-problems/problems/bronze/combining-dimensions/","title":"Combining dimensions \ud83d\udd17","text":"<p>Scenario</p> <p>A human resources department has several dimension tables with information about their employees.</p> <p>For reporting purposes, they need to combine these dimensions into a single dimension table which shows all the information about each employee.</p> <p>Question</p> <p>Write a query which combines the three dimension tables below into a single dimension table.</p> <p>The output should have the columns from the three tables combined:</p> <ul> <li><code>employee_id</code></li> <li><code>valid_from</code></li> <li><code>valid_until</code></li> <li><code>date_of_birth</code></li> <li><code>gender</code></li> <li><code>ethnicity</code></li> <li><code>job_title</code></li> <li><code>salary</code></li> <li><code>email</code></li> <li><code>phone</code></li> </ul> <p>The number of rows in the output depends on the number of changes that need to be accounted for, and the <code>valid_from</code> and <code>valid_until</code> columns should be recalculated as necessary to account for these changes.</p> <p>Order the output by <code>employee_id</code> and <code>valid_from</code>.</p> Expand for the DDL <pre><code>create table dim_employee_demographics (\n    employee_id   int,\n    valid_from    date,\n    valid_until   date,\n    date_of_birth date,\n    gender        varchar,\n    ethnicity     varchar,\n    primary key (employee_id, valid_from)\n);\ninsert into dim_employee_demographics\nvalues\n    (1, '2021-06-13', '2022-06-08', '2004-02-18', 'Female',      'Malaysian'),\n    (2, '2021-10-19', '9999-12-31', '1963-12-12', 'Female',      'Navajo'),\n    (3, '2022-01-29', '9999-12-31', '2000-10-17', 'Genderqueer', 'White'),\n    (4, '2022-04-28', '2024-03-12', '1987-12-13', 'Male',        'Black'),\n    (1, '2022-06-09', '9999-12-31', '2004-02-18', 'Non-binary',  'Malaysian'),\n    (5, '2022-08-31', '9999-12-31', '1999-09-10', 'Female',      'Asian'),\n    (4, '2024-03-13', '9999-12-31', '1987-12-13', 'Female',      'Black')\n;\n\n\ncreate table dim_employee_career (\n    employee_id int,\n    valid_from  date,\n    valid_until date,\n    job_title   varchar,\n    salary      decimal(10, 2),\n    primary key (employee_id, valid_from)\n);\ninsert into dim_employee_career\nvalues\n    (1, '2021-06-13', '2023-05-27', 'Teacher',             5000.00),\n    (1, '2023-05-28', '9999-12-31', 'Teacher',             6000.00),\n    (2, '2021-10-19', '2023-11-26', 'Data Analyst',        4000.00),\n    (2, '2023-11-27', '2024-03-01', 'Data Analyst',        6500.00),\n    (2, '2024-03-02', '9999-12-31', 'Engineering Manager', 7000.00),\n    (3, '2022-01-29', '2023-04-02', 'Software Engineer',   6000.00),\n    (3, '2023-04-03', '9999-12-31', 'Software Engineer',   8000.00),\n    (4, '2022-06-12', '9999-12-31', 'Founder',                null)\n;\n\n\ncreate table dim_employee_contact (\n    employee_id int,\n    valid_from  date,\n    valid_until date,\n    email       varchar,\n    phone       varchar,\n    primary key (employee_id, valid_from)\n);\ninsert into dim_employee_contact\nvalues\n    (1, '2021-06-13', '2024-01-29', 'c.perot0@gmail.com',        null),\n    (1, '2024-01-30', '9999-12-31', 'c.perot0@gmail.com',        '1986474151'),\n    (2, '2021-10-19', '2024-04-04', null,                        null),\n    (2, '2024-04-05', '9999-12-31', 'hpicard1@bing.com',         null),\n    (4, '2022-06-12', '2022-12-01', 'tbayford3@hotmail.co.uk',   '01246 209863'),\n    (4, '2022-12-02', '2023-11-11', 'tbayford3@hotmail.co.uk',   '01752 492269'),\n    (4, '2023-11-12', '9999-12-31', 'tmacalinden@hotmail.co.uk', '01270 530950'),\n    (5, '2023-02-17', '9999-12-31', null,                        null)\n;\n</code></pre> <p>The \"dimensions\" in this context are Slowly Changing Dimensions of type 2 from the Star Schema modelling framework.</p> <p>The solution can be found at:</p> <ul> <li>combining-dimensions.md</li> </ul> Sample input <p>Employee Demographics Dimension</p> employee_id valid_from valid_until date_of_birth gender ethnicity 1 2021-07-12 9999-12-31 1995-02-24 Female Black 2 2023-12-07 9999-12-31 1999-12-12 Male Asian <p>Employee Career Dimension</p> employee_id valid_from valid_until job_title salary 1 2021-07-12 2023-02-18 Student 0.00 1 2023-02-19 9999-12-31 Pianist 2000.00 2 2023-12-08 9999-12-31 Paramedic 4000.00 <p>Employee Contact Dimension</p> employee_id valid_from valid_until email phone 1 2021-07-12 9999-12-31 abcde@gmail.com 123-456-789 2 2023-12-08 2023-12-31 null 01234 567890 2 2024-01-01 9999-12-31 something@mail.net 0300 123 456 <pre><code>with\n\ndim_employee_demographics(employee_id, valid_from, valid_until, date_of_birth, gender, ethnicity) as (\n    values\n        (1, '2021-07-12'::date, '9999-12-31'::date, '1995-02-24'::date, 'Female', 'White'),\n        (2, '2023-12-07'::date, '9999-12-31'::date, '1999-12-12'::date, 'Male',   'Asian')\n),\n\ndim_employee_career(employee_id, valid_from, valid_until, job_title, salary) as (\n    values\n        (1, '2021-07-12'::date, '2023-02-18'::date, 'Student',      0.00),\n        (1, '2023-02-19'::date, '9999-12-31'::date, 'Pianist',   2000.00),\n        (2, '2023-12-08'::date, '9999-12-31'::date, 'Paramedic', 4000.00)\n),\n\ndim_employee_contact(employee_id, valid_from, valid_until, email, phone) as (\n    values\n        (1, '2021-07-12'::date, '9999-12-31'::date, 'abcde@gmail.com',    '123-456-789'),\n        (2, '2023-12-08'::date, '2023-12-31'::date, null,                 '01234 567890'),\n        (2, '2024-01-01'::date, '9999-12-31'::date, 'something@mail.net', '0300 123 456')\n)\n</code></pre> Sample output employee_id valid_from valid_until date_of_birth gender ethnicity job_title salary email phone 1 2021-07-12 2023-02-18 1995-02-24 Female Black Student 0.00 abcde@gmail.com 123-456-789 1 2023-02-19 9999-12-31 1995-02-24 Female Black Pianist 2000.00 abcde@gmail.com 123-456-789 2 2023-12-07 2023-12-07 1999-12-12 Male Asian null null null null 2 2023-12-08 2023-12-31 1999-12-12 Male Asian Paramedic 4000.00 null 01234 567890 2 2024-01-01 9999-12-31 1999-12-12 Male Asian Paramedic 4000.00 something@mail.net 0300 123 456 <pre><code>solution(employee_id, valid_from, valid_until, date_of_birth, gender, ethnicity, job_title, salary, email, phone) as (\n    values\n        (1, '2021-07-12'::date, '2023-02-18'::date, '1995-02-24'::date, 'Female', 'Black', 'Student',      0.00, 'abcde@gmail.com',    '123-456-789'),\n        (1, '2023-02-19'::date, '9999-12-31'::date, '1995-02-24'::date, 'Female', 'Black', 'Pianist',   2000.00, 'abcde@gmail.com',    '123-456-789'),\n        (2, '2023-12-07'::date, '2023-12-07'::date, '1999-12-12'::date, 'Male',   'Asian', null,           null, null,                 null),\n        (2, '2023-12-08'::date, '2023-12-31'::date, '1999-12-12'::date, 'Male',   'Asian', 'Paramedic', 4000.00, null,                 '01234 567890'),\n        (2, '2024-01-01'::date, '9999-12-31'::date, '1999-12-12'::date, 'Male',   'Asian', 'Paramedic', 4000.00, 'something@mail.net', '0300 123 456')\n)\n</code></pre> Hint 1 <p>The <code>valid_from</code> values in the output will be each of the distinct <code>valid_from</code> values from the three tables for the given employee, so start by constructing an \"axis\" which is the distinct combination of all <code>employee_id</code> and <code>valid_from</code> values across the three tables.</p> Hint 2 <p>For databases that support it, use an <code>ASOF</code> join to add the latest information from each dimension to the \"axis\" for each <code>employee_id</code> and <code>valid_from</code> combination.</p> <p>For databases that don't support <code>ASOF</code> joins, calculate the <code>valid_until</code> values for the <code>valid_from</code> values in the \"axis\" and then join the dimension tables to the axis by where the axis <code>valid_from</code>-<code>valid_until</code> range in contained within the dimension <code>valid_from</code>-<code>valid_until</code> range.</p>","tags":["asof join","custom axis"]},{"location":"challenging-sql-problems/problems/bronze/customer-churn/","title":"Customer churn \ud83d\udd04","text":"<p>Scenario</p> <p>A social media platform wants to track customer churn by identifying users who have not been active recently.</p> <p>For performance reasons, the platform encodes the recent user activity in a \"datelist int\"; more details on this encoding can be found at:</p> <ul> <li>https://www.linkedin.com/pulse/datelist-int-efficient-data-structure-user-growth-max-sung/</li> </ul> <p>Question</p> <p>Given the user history below, identify the users who have churned: who were inactive for the last 7 days, but active in the 7 days before that.</p> <p>Note that the <code>last_update</code> column will always have the same date for all users.</p> <p>The output should have one row for each churned user, with the columns:</p> <ul> <li><code>user_id</code></li> <li><code>days_active_last_week</code> as the number of days the user was active in the last week</li> </ul> <p>Order the output by <code>user_id</code>.</p> Expand for the DDL <pre><code>create table user_history (\n    user_id          int primary key,\n    last_update      date not null,\n    activity_history bigint not null,\n);\ninsert into user_history\nvalues\n    (1, '2024-06-01',   1056256),\n    (2, '2024-06-01', 907289368),\n    (3, '2024-06-01', 201335032),\n    (4, '2024-06-01',   9769312),\n    (5, '2024-06-01', 246247510),\n    (6, '2024-06-01', 492660983)\n;\n</code></pre> <p>The solution can be found at:</p> <ul> <li>customer-churn.md</li> </ul> <p>A worked example is provided below to help illustrate the datelist int encoding.</p> Sample input user_id last_update activity_history 1 2024-03-01 81 2 2024-03-01 2688 3 2024-03-01 13144 <pre><code>with user_history(user_id, last_update, activity_history) as (\n    values\n        (1, '2024-03-01'::date,    81),\n        (2, '2024-03-01'::date,  2688),\n        (3, '2024-03-01'::date, 13144)\n)\n</code></pre> Sample output user_id days_active_last_week 2 3 <pre><code>solution(user_id, days_active_last_week) as (\n    values\n        (2, 3)\n)\n</code></pre> Hint 1 <p>Use \"bitwise and\" operations with powers of 2 to determine the user activity for some dates.</p> Hint 2 <p>Use the \"bitwise shift right\" operation to shift the activity history to the right by 7 days to make summarising the previous week easier.</p> Hint 3 <p>Use the <code>bit_count</code> function (or equivalent) to count the number of bits (days) in the given range.</p>","tags":["bitshift"]},{"location":"challenging-sql-problems/problems/bronze/customer-churn/#worked-example","title":"Worked example","text":"<p>To help understand the datelist int encoding, consider the following users:</p> user_id last_update activity_history 1 2024-03-01 81 2 2024-03-01 2688 3 2024-03-01 13144 <p>We'll walk through each user to understand which days they were active.</p>","tags":["bitshift"]},{"location":"challenging-sql-problems/problems/bronze/customer-churn/#user-1","title":"User 1","text":"<p>The <code>activity_history</code> for user 1 is <code>81</code>, which in binary is <code>1010001</code>.</p> <p>The rightmost bit represents the most recent day, which is the <code>last_update</code> date: <code>2024-03-01</code>.</p> <p>Since the rightmost bit is <code>1</code>, the user was active on that day.</p> <p>We can apply the same logic to the other bits:</p> <pre><code>1 0 1 0 0 0 1\n| | | | | | |\n| | | | | | \u2514-- 2024-03-01 (active)\n| | | | | \u2514---- 2024-02-29 (inactive)\n| | | | \u2514------ 2024-02-28 (inactive)\n| | | \u2514-------- 2024-02-27 (inactive)\n| | \u2514---------- 2024-02-26 (active)\n| \u2514------------ 2024-02-25 (inactive)\n\u2514-------------- 2024-02-24 (active)\n</code></pre> <p>Since there are no more bits on the left, we can assume that the user was inactive on those days.</p> <p>This user is therefore not a churned user since they were active at least once in the last 7 days.</p>","tags":["bitshift"]},{"location":"challenging-sql-problems/problems/bronze/customer-churn/#user-2","title":"User 2","text":"<p>The <code>activity_history</code> for user 2 is <code>2688</code>, which in binary is <code>101010000000</code>.</p> <p>We can expand the binary representation to understand the activity:</p> <pre><code>1 0 1 0 1 0 0 0 0 0 0 0\n| | | | | | | | | | | |\n| | | | | | | | | | | \u2514-- 2024-03-01 (inactive)\n| | | | | | | | | | \u2514---- 2024-02-29 (inactive)\n| | | | | | | | | \u2514------ 2024-02-28 (inactive)\n| | | | | | | | \u2514-------- 2024-02-27 (inactive)\n| | | | | | | \u2514---------- 2024-02-26 (inactive)\n| | | | | | \u2514------------ 2024-02-25 (inactive)\n| | | | | \u2514-------------- 2024-02-24 (inactive)\n| | | | \u2514---------------- 2024-02-23 (active)\n| | | \u2514------------------ 2024-02-22 (inactive)\n| | \u2514-------------------- 2024-02-21 (active)\n| \u2514---------------------- 2024-02-20 (inactive)\n\u2514------------------------ 2024-02-19 (active)\n</code></pre> <p>User 2 has churned since they were not active in the last 7 days (between <code>2024-02-24</code> and <code>2024-03-01</code>) but were active at least once in the 7 days before that.</p>","tags":["bitshift"]},{"location":"challenging-sql-problems/problems/bronze/customer-churn/#user-3","title":"User 3","text":"<p>The <code>activity_history</code> for user 3 is <code>13144</code>, which in binary is <code>11001101011000</code>.</p> <p>Again, we can expand the binary representation to understand the activity:</p> <pre><code>1 1 0 0 1 1 0 1 0 1 1 0 0 0\n| | | | | | | | | | | | | |\n| | | | | | | | | | | | | \u2514-- 2024-03-01 (inactive)\n| | | | | | | | | | | | \u2514---- 2024-02-29 (inactive)\n| | | | | | | | | | | \u2514------ 2024-02-28 (inactive)\n| | | | | | | | | | \u2514-------- 2024-02-27 (active)\n| | | | | | | | | \u2514---------- 2024-02-26 (active)\n| | | | | | | | \u2514------------ 2024-02-25 (inactive)\n| | | | | | | \u2514-------------- 2024-02-24 (active)\n| | | | | | \u2514---------------- 2024-02-23 (inactive)\n| | | | | \u2514------------------ 2024-02-22 (active)\n| | | | \u2514-------------------- 2024-02-21 (active)\n| | | \u2514---------------------- 2024-02-20 (inactive)\n| | \u2514------------------------ 2024-02-19 (inactive)\n| \u2514-------------------------- 2024-02-18 (active)\n\u2514---------------------------- 2024-02-17 (active)\n</code></pre> <p>Like user 1, user 3 is not a churned user since they were active at least once in the last 7 days.</p>","tags":["bitshift"]},{"location":"challenging-sql-problems/problems/bronze/fibonacci-sequence/","title":"Fibonacci sequence \ud83d\udd22","text":"<p>Info</p> <p>This is just for fun, there's no real-world application for this (as far as I know) \ud83d\ude1d</p> <p>Question</p> <p>Generate the first 45 terms of the Fibonacci sequence.</p> <p>The output should have one row per term in the sequence, with the columns:</p> <ul> <li><code>n</code> as the term number</li> <li><code>f_n</code> as the corresponding Fibonacci number</li> </ul> <p>Order the output by <code>n</code>.</p> <p>The Fibonacci sequence is defined as f<sub>n</sub> = f<sub>n-1</sub> + f<sub>n-2</sub>, where f<sub>1</sub> = f<sub>2</sub> = 1.</p> <p>For example:</p> <ul> <li>The third term is f<sub>3</sub> = f<sub>2</sub> + f<sub>1</sub> = 1 + 1 = 2</li> <li>The fourth term is f<sub>4</sub> = f<sub>3</sub> + f<sub>2</sub> = 2 + 1 = 3</li> <li>...</li> <li>The tenth term is f<sub>10</sub> = f<sub>9</sub> + f<sub>8</sub> = 34 + 21 = 55</li> </ul> <p>The solution can be found at:</p> <ul> <li>fibonacci-sequence.md</li> </ul> Sample input <p>Generate the first 10 terms of the Fibonacci sequence.</p> Sample output n f_n 1 1 2 1 3 2 4 3 5 5 6 8 7 13 8 21 9 34 10 55 <pre><code>solution(n, f_n) as (\n    values\n        (1,  1),\n        (2,  1),\n        (3,  2),\n        (4,  3),\n        (5,  5),\n        (6,  8),\n        (7,  13),\n        (8,  21),\n        (9,  34),\n        (10, 55)\n)\n</code></pre> Hint 1 <p>Use a recursive CTE to generate the sequence.</p> Hint 2 <p>Use the columns <code>n</code>, <code>f_n</code>, and <code>f_m</code> to keep track of the current term, the current Fibonacci number, and the previous Fibonacci number, respectively.</p>","tags":["recursive CTE"]},{"location":"challenging-sql-problems/problems/bronze/outstanding-invoices/","title":"Outstanding invoices \ud83d\udcb1","text":"<p>Scenario</p> <p>A consulting firm, based in the US, works with clients all over the world.</p> <p>They invoice their clients in USD and are interested in the total amount outstanding in each local currency.</p> <p>Question</p> <p>Calculate the total amount outstanding (<code>is_paid = false</code>) in each local currency for the invoices in the <code>invoices</code> table.</p> <p>Use the <code>exchange_rates</code> table to convert the invoice amount (USD) to the local currency, using the latest exchange rate at the time of the invoice.</p> <p>The output should have a row per <code>invoice_currency</code> with the columns:</p> <ul> <li><code>invoice_currency</code></li> <li><code>amount_outstanding</code> as the total amount outstanding in the local currency, rounded up to two decimal places</li> </ul> <p>Order the output by <code>invoice_currency</code>.</p> Expand for the DDL <pre><code>create table invoices (\n    invoice_id         integer primary key,\n    invoice_datetime   timestamp not null,\n    invoice_amount_usd numeric(10, 2) not null,\n    is_paid            boolean not null,\n    invoice_currency   varchar not null\n);\ninsert into invoices\nvalues\n    (87, '2024-05-02 14:59:21', 38599.00, true,  'INR'),\n    (88, '2024-05-05 15:01:22',   199.99, false, 'GBP'),\n    (89, '2024-05-06 02:24:34',  1200.50, false, 'INR'),\n    (90, '2024-05-07 11:59:59',   299.99, true,  'GBP'),\n    (91, '2024-05-18 15:48:17', 24000.00, true,  'INR'),\n    (92, '2024-05-23 05:12:22',  2499.50, false, 'USD'),\n    (93, '2024-05-24 23:35:34',    50.00, false, 'INR'),\n    (94, '2024-06-03 11:52:01',   150.00, true,  'GBP'),\n    (95, '2024-06-04 01:11:52',  8999.99, false, 'INR'),\n    (96, '2024-06-17 21:01:29',   720.00, false, 'USD'),\n    (97, '2024-06-18 00:12:50', 75000.00, false, 'GBP'),\n    (98, '2024-06-28 07:19:23',  8000.00, false, 'USD')\n;\n\n\ncreate table exchange_rates (\n    from_datetime timestamp,\n    from_currency varchar,\n    to_currency   varchar,\n    rate          numeric(10, 4) not null,\n    primary key (from_datetime, from_currency, to_currency)\n);\ninsert into exchange_rates\nvalues\n    ('2024-05-01 22:45:17', 'USD', 'INR', 83.4550),\n    ('2024-05-01 19:31:12', 'USD', 'GBP',  0.7983),\n    ('2024-05-06 15:17:07', 'USD', 'INR', 83.4750),\n    ('2024-05-10 06:48:57', 'USD', 'GBP',  0.7984),\n    ('2024-05-10 11:03:02', 'USD', 'INR', 83.5410),\n    ('2024-05-13 02:34:52', 'USD', 'INR', 83.4600),\n    ('2024-05-17 18:06:42', 'USD', 'GBP',  0.7872),\n    ('2024-05-17 22:20:47', 'USD', 'INR', 83.2800),\n    ('2024-05-20 09:38:32', 'USD', 'GBP',  0.7869),\n    ('2024-05-20 13:52:37', 'USD', 'INR', 83.2800),\n    ('2024-05-24 01:10:22', 'USD', 'GBP',  0.7849),\n    ('2024-05-24 05:24:27', 'USD', 'INR', 83.0600),\n    ('2024-05-27 16:42:12', 'USD', 'GBP',  0.7831),\n    ('2024-05-27 20:56:17', 'USD', 'INR', 83.1020),\n    ('2024-05-31 08:14:02', 'USD', 'GBP',  0.7846),\n    ('2024-05-31 12:28:07', 'USD', 'INR', 83.4240),\n    ('2024-06-03 00:45:52', 'USD', 'GBP',  0.7808),\n    ('2024-06-03 04:59:57', 'USD', 'INR', 83.0830),\n    ('2024-06-06 20:31:47', 'USD', 'INR', 83.4660),\n    ('2024-06-07 16:17:42', 'USD', 'INR', 83.5210),\n    ('2024-06-10 12:03:37', 'USD', 'GBP',  0.7854),\n    ('2024-06-14 03:35:27', 'USD', 'GBP',  0.7880),\n    ('2024-06-14 07:49:32', 'USD', 'INR', 83.5470),\n    ('2024-06-17 23:21:22', 'USD', 'GBP',  0.7870),\n    ('2024-06-18 19:07:17', 'USD', 'GBP',  0.7867),\n    ('2024-06-19 14:53:12', 'USD', 'INR', 83.3300),\n    ('2024-06-22 10:39:07', 'USD', 'GBP',  0.7860),\n    ('2024-06-23 06:25:02', 'USD', 'INR', 83.4600),\n    ('2024-06-24 02:10:57', 'USD', 'GBP',  0.7900),\n    ('2024-06-28 21:56:52', 'USD', 'GBP',  0.7909),\n    ('2024-06-29 17:42:47', 'USD', 'INR', 83.5680)\n;\n</code></pre> <p>The solution can be found at:</p> <ul> <li>outstanding-invoices.md</li> </ul> Sample input <p>Invoices</p> invoice_id invoice_datetime invoice_amount_usd is_paid invoice_currency 1 2024-06-03 11:52:01 100.00 true USD 2 2024-06-04 01:11:52 200.00 false INR 3 2024-06-17 21:01:29 300.00 false USD 4 2024-06-18 00:12:50 499.00 false GBP <p>Exchange Rates</p> from_datetime from_currency to_currency rate 2024-06-02 00:45:52 USD GBP 0.7808 2024-06-02 04:59:57 USD INR 83.0830 2024-06-14 03:35:27 USD GBP 0.7880 2024-06-14 07:49:32 USD INR 83.5470 2024-06-17 23:21:22 USD GBP 0.7870 2024-06-28 21:56:52 USD GBP 0.7909 2024-06-29 17:42:47 USD INR 83.5680 <pre><code>with\n\ninvoices(invoice_id, invoice_datetime, invoice_amount_usd, is_paid, invoice_currency) as (\n    values\n        (1, '2024-06-03 11:52:01'::timestamp, 100.00, true,  'USD'),\n        (2, '2024-06-04 01:11:52'::timestamp, 200.00, false, 'INR'),\n        (3, '2024-06-17 21:01:29'::timestamp, 300.00, false, 'USD'),\n        (4, '2024-06-18 00:12:50'::timestamp, 499.00, false, 'GBP')\n),\n\nexchange_rates(from_datetime, from_currency, to_currency, rate) as (\n    values\n        ('2024-06-02 00:45:52'::timestamp, 'USD', 'GBP',  0.7808),\n        ('2024-06-02 04:59:57'::timestamp, 'USD', 'INR', 83.0830),\n        ('2024-06-14 03:35:27'::timestamp, 'USD', 'GBP',  0.7880),\n        ('2024-06-14 07:49:32'::timestamp, 'USD', 'INR', 83.5470),\n        ('2024-06-17 23:21:22'::timestamp, 'USD', 'GBP',  0.7870),\n        ('2024-06-28 21:56:52'::timestamp, 'USD', 'GBP',  0.7909),\n        ('2024-06-29 17:42:47'::timestamp, 'USD', 'INR', 83.5680)\n)\n</code></pre> Sample output invoice_currency amount_outstanding GBP 392.72 INR 16616.60 USD 300.00 <pre><code>solution(invoice_currency, amount_outstanding) as (\n    values\n        ('GBP',   392.72),\n        ('INR', 16616.60),\n        ('USD',   300.00)\n)\n</code></pre> Hint 1 <p>For databases that support it, use an <code>ASOF</code> join to use the latest exchange rate for each invoice (as of the time of the invoice).</p> <p>For databases that don't support <code>ASOF</code> joins, use any other method to get the exchange rates for each invoice, such as a correlated subquery.</p> Hint 2 <p>Use the <code>CEIL</code> function (or equivalent) with the value to round multiplied by 100, then divide by 100 to round up to two decimal places.</p>","tags":["asof join"]},{"location":"challenging-sql-problems/problems/bronze/personalised-customer-emails/","title":"Personalised customer emails \ud83d\udce8","text":"<p>Scenario</p> <p>A B2B vendor is looking to email their customers about changes to their account.</p> <p>This bank's customers are companies, and these companies have multiple contacts associated with them. While the companies have an email address in the system, it's not clear which contact the email address corresponds to.</p> <p>The bank wants to email the company's email address, but to make it more personal, they want to use the first name of the corresponding contact in the email salutation; e.g. \"Hey John\".</p> <p>Question</p> <p>Given the customer details and relationships below, determine which first name to use in the email salutation for each company. If none of the contacts' names seem to be a good fit, use <code>NULL</code> instead.</p> <p>The output should have one row for each company, with the columns:</p> <ul> <li><code>company_name</code> as the name of the company</li> <li><code>company_email_address</code> as the email address of the company</li> <li><code>salutation_name</code> as the first name to use in the email salutation</li> </ul> <p>Order the output by <code>company_name</code>.</p> <p>Info</p> <p>There is no \"correct\" answer to this question: just provide a reasonable solution for the database that you're using.</p> Expand for the DDL <pre><code>create table customers (\n    customer_id   int primary key,\n    full_name     varchar not null,\n    first_name    varchar,\n    last_name     varchar,\n    email_address varchar\n);\ninsert into customers\nvalues\n    (1,  'Friends For Hire', null,       null,        'joe.trib@f4hire.com'),\n    (2,  'Rachel Green',     'Rachel',   'Green',     'rachel.green@gmail.com'),\n    (3,  'Monica Geller',    'Monica',   'Geller',    'gellerm@gmail.com'),\n    (4,  'Ross Geller',      'Ross',     'Geller',    null),\n    (5,  'Joey Tribbiani',   'Joey',     'Tribbiani', 'joe.tribbiani@gmail.com'),\n    (6,  'Chandler Bing',    'Chandler', 'Bing',      'c.bing@gmail.com'),\n    (7,  'Phoebe Buffay',    'Phoebe',   'Buffay',    null),\n    (8,  'Fractal Factory',  null,       null,        'billiam@fractal-factory.co.uk'),\n    (9,  'William Bloggs',   'William',  'Bloggs',    null),\n    (10, 'Joe Bloggs',       'Joe',      'Bloggs',    null),\n    (11, 'Some Company',     null,       null,        'admin@somecompany.com'),\n    (12, 'Zoe Goode',        'Penny',    'Lane',      'pink.lotus@gmail.com'),\n    (13, 'Leeroy Smythe',    'Leeroy',   'Smythe',    'lee.the.boss@gmail.com')\n;\n\ncreate table customer_relationships (\n    parent_customer_id int references customers(customer_id),\n    child_customer_id  int references customers(customer_id),\n    relationship_type  varchar not null,\n    primary key (parent_customer_id, child_customer_id)\n);\ninsert into customer_relationships\nvalues\n    (1,  2,  'Director'),\n    (1,  3,  'Shareholder'),\n    (1,  4,  'Shareholder'),\n    (1,  5,  'Director'),\n    (1,  6,  'Director'),\n    (1,  7,  'Director'),\n    (8,  9,  'Director'),\n    (8,  10, 'Director'),\n    (11, 12, 'Director'),\n    (11, 13, 'Director')\n;\n</code></pre> <p>The solution can be found at:</p> <ul> <li>personalised-customer-emails.md</li> </ul> Sample input <p>Customers</p> customer_id full_name first_name last_name email_address 1 Straw Hat Pirates null null king.luffy@strawhats.com 2 Monkey D Luffy Luffy Monkey null 3 Roronoa Zoro Zoro Roronoa null <p>Customer Relationships</p> parent_customer_id child_customer_id relationship_type 1 2 Captain 1 3 Swordsman <pre><code>with\n\ncustomers(customer_id, full_name, first_name, last_name, email_address) as (\n    values\n        (1,  'Straw Hat Pirates', null,    null,      'king.luffy@strawhats.com'),\n        (2,  'Monkey D Luffy',    'Luffy', 'Monkey',  null),\n        (3,  'Roronoa Zoro',      'Zoro',  'Roronoa', null)\n),\n\ncustomer_relationships(parent_customer_id, child_customer_id, relationship) as (\n    values\n        (1, 2, 'Captain'),\n        (1, 3, 'Swordsman')\n)\n</code></pre> Sample output company_name company_email_address salutation_name Straw Hat Pirates king.luffy@strawhats.com Luffy <pre><code>solution(company_name, company_email_address, salutation_name) as (\n    values\n        ('Straw Hat Pirates', 'king.luffy@strawhats.com', 'Luffy')\n)\n</code></pre> Hint 1 <p>Use a string similarity function like <code>DIFFERENCE</code> (SQL Server) or <code>JACCARD</code> (DuckDB) to compare the contact names to the company email address.</p> Hint 2 <p>Rank the contacts by similarity to the company email address and use the most similar one (above some threshold) as the salutation name.</p>","tags":["string similarity"]},{"location":"challenging-sql-problems/problems/bronze/suspicious-login-activity/","title":"Suspicious login activity \ud83e\udd14","text":"<p>Scenario</p> <p>A company is investigating suspicious login activity on their platform.</p> <p>Consecutive failed login attempts are considered suspicious once they reach a certain threshold, and the company wants to identify users who have reached this threshold.</p> <p>Their platform logs <code>login</code>, <code>logout</code>, and <code>login failed</code> events for each user.</p> <p>Question</p> <p>For the events below, identify the users who have <code>login failed</code> events at least five times in a row.</p> <p>Keep only the user ID and their greatest number of consecutive failed login attempts.</p> <p>The output should have a row for each user who meets this criterion, with the columns:</p> <ul> <li><code>user_id</code></li> <li><code>consecutive_failures</code> as the greatest number of consecutive failed login attempts for the user</li> </ul> <p>Order the output by <code>user_id</code>.</p> Expand for the DDL <pre><code>create table events (\n    event_id       integer primary key,\n    user_id        integer not null,\n    event_datetime timestamp not null,\n    event_type     varchar not null\n);\ninsert into events\nvalues\n    (1,  1, '2024-01-01 11:00:00', 'login'),\n    (2,  1, '2024-01-01 12:00:00', 'logout'),\n    (3,  1, '2024-01-03 03:00:00', 'login failed'),\n    (4,  1, '2024-01-03 03:01:00', 'login failed'),\n    (5,  1, '2024-01-03 03:02:00', 'login failed'),\n    (6,  1, '2024-01-03 03:05:00', 'login'),\n    (7,  2, '2024-01-03 10:00:00', 'login'),\n    (8,  2, '2024-01-03 15:00:00', 'logout'),\n    (9,  1, '2024-01-03 23:00:00', 'logout'),\n    (10, 2, '2024-01-04 22:00:00', 'login failed'),\n    (11, 2, '2024-01-04 22:05:00', 'login'),\n    (12, 3, '2024-01-05 20:00:00', 'login'),\n    (13, 3, '2024-01-06 04:00:00', 'logout'),\n    (14, 2, '2024-01-09 15:00:00', 'logout'),\n    (15, 3, '2024-01-11 21:00:00', 'login'),\n    (16, 1, '2024-01-12 12:00:00', 'login failed'),\n    (17, 1, '2024-01-12 13:00:00', 'login failed'),\n    (18, 1, '2024-01-12 23:00:00', 'login failed'),\n    (19, 2, '2024-01-13 10:00:00', 'login failed'),\n    (20, 2, '2024-01-13 10:05:00', 'login'),\n    (21, 2, '2024-01-13 15:00:00', 'logout'),\n    (22, 1, '2024-01-13 23:00:00', 'login failed'),\n    (23, 1, '2024-01-13 23:01:00', 'login failed'),\n    (24, 1, '2024-01-13 23:02:00', 'login failed'),\n    (25, 2, '2024-01-14 22:00:00', 'login'),\n    (26, 3, '2024-01-15 20:00:00', 'login'),\n    (27, 3, '2024-01-16 04:00:00', 'logout'),\n    (28, 2, '2024-01-19 15:00:00', 'logout'),\n    (29, 3, '2024-01-21 21:00:00', 'login'),\n    (30, 1, '2024-01-22 12:00:00', 'login failed'),\n    (31, 1, '2024-01-22 12:05:00', 'password reset'),\n    (32, 1, '2024-01-22 12:10:00', 'login'),\n    (33, 1, '2024-01-22 13:00:00', 'logout'),\n    (34, 1, '2024-01-23 03:00:00', 'login'),\n    (35, 2, '2024-01-23 10:00:00', 'login'),\n    (36, 2, '2024-01-23 15:00:00', 'logout'),\n    (37, 1, '2024-01-23 23:00:00', 'logout'),\n    (38, 2, '2024-01-24 22:00:00', 'login'),\n    (39, 3, '2024-01-25 20:00:00', 'login'),\n    (40, 3, '2024-01-26 04:00:00', 'logout'),\n    (41, 2, '2024-01-29 15:00:00', 'logout'),\n    (42, 3, '2024-01-30 21:00:00', 'login failed'),\n    (43, 3, '2024-01-30 21:01:00', 'login failed'),\n    (44, 3, '2024-01-30 21:02:00', 'login failed'),\n    (45, 3, '2024-01-30 21:03:00', 'login failed'),\n    (46, 3, '2024-01-30 21:04:00', 'login failed'),\n    (47, 3, '2024-01-30 21:05:00', 'password reset'),\n    (48, 3, '2024-01-30 21:06:00', 'password reset'),\n    (49, 3, '2024-01-30 21:07:00', 'password reset'),\n    (50, 3, '2024-01-30 21:08:00', 'password reset'),\n    (51, 3, '2024-01-30 21:09:00', 'password reset'),\n    (52, 3, '2024-01-30 21:10:00', 'password reset'),\n    (53, 3, '2024-01-31 23:55:00', 'login failed'),\n    (54, 3, '2024-01-31 23:56:00', 'login failed'),\n    (55, 3, '2024-01-31 23:57:00', 'login failed'),\n    (56, 3, '2024-01-31 23:58:00', 'login failed'),\n    (57, 3, '2024-01-31 23:59:00', 'login failed'),\n    (58, 3, '2024-02-01 00:00:00', 'login failed'),\n    (59, 3, '2024-02-01 00:01:00', 'login failed'),\n    (60, 3, '2024-02-01 00:02:00', 'login failed')\n;\n</code></pre> <p>The solution can be found at:</p> <ul> <li>suspicious-login-activity.md</li> </ul> Sample input event_id user_id event_datetime event_type 1 1 2024-01-01 03:00:00 login failed 2 1 2024-01-01 03:01:00 login failed 3 1 2024-01-01 03:02:00 login failed 4 1 2024-01-01 03:03:00 login failed 5 1 2024-01-01 03:04:00 login failed 6 1 2024-01-01 03:05:00 login 7 2 2024-01-01 10:00:00 login 8 2 2024-01-01 15:00:00 logout 9 2 2024-01-01 23:00:00 login failed <pre><code>with events(event_id, user_id, event_datetime, event_type) as (\n    values\n        (1, 1, '2024-01-01 03:00:00'::timestamp, 'login failed'),\n        (2, 1, '2024-01-01 03:01:00'::timestamp, 'login failed'),\n        (3, 1, '2024-01-01 03:02:00'::timestamp, 'login failed'),\n        (4, 1, '2024-01-01 03:03:00'::timestamp, 'login failed'),\n        (5, 1, '2024-01-01 03:04:00'::timestamp, 'login failed'),\n        (6, 1, '2024-01-01 03:05:00'::timestamp, 'login'),\n        (7, 2, '2024-01-01 10:00:00'::timestamp, 'login'),\n        (8, 2, '2024-01-01 15:00:00'::timestamp, 'logout'),\n        (9, 2, '2024-01-01 23:00:00'::timestamp, 'login failed')\n)\n</code></pre> Sample output user_id consecutive_failures 1 5 <pre><code>solution(user_id, consecutive_failures) as (\n    values\n        (1, 5)\n)\n</code></pre> Hint 1 <p>This is a typical \"gaps and islands\" problem.</p> Hint 2 <p>Use the difference between two <code>ROW_NUMBER()</code> functions to create a group for each user and event type. Partition on <code>user_id</code> for one and partition on both <code>user_id</code> and <code>event_type</code> for the other, ordering both by <code>event_id</code>.</p>","tags":["window function","gaps and islands"]},{"location":"challenging-sql-problems/problems/bronze/temperature-anomaly-detection/","title":"Temperature anomaly detection \ud83d\udd0d","text":"<p>Scenario</p> <p>Some scientists are studying temperature data from various sites.</p> <p>They are interested in identifying temperature readings that are significantly higher than the surrounding readings.</p> <p>Question</p> <p>Given the temperature data below, select the temperature readings that are at least 10% higher than the average of the previous 2 and following 2 readings for the same site.</p> <p>Do not include the current reading in the average calculation, and use the calculated average temperature as the denominator for the 10% calculation.</p> <p>If there are fewer than 2 readings before or 2 after the current reading, do not include the reading in the output.</p> <p>The output should only have the temperature readings above the threshold, with the columns:</p> <ul> <li><code>site_id</code></li> <li><code>reading_datetime</code></li> <li><code>temperature</code></li> <li><code>average_temperature</code> as the average of the 4 readings around the current reading (2 each side), rounded to 4 decimal places</li> <li><code>percentage_increase</code> as the percentage increase of the current reading over the <code>average_temperature</code>, rounded to 4 decimal places</li> </ul> <p>Order the output by <code>site_id</code> and <code>reading_datetime</code>.</p> Expand for the DDL <pre><code>create table readings (\n    site_id          integer,\n    reading_datetime timestamp,\n    temperature      decimal(5, 2) not null,\n    primary key (site_id, reading_datetime)\n);\ninsert into readings\nvalues\n    (1, '2021-01-01 03:26:23', 20.02),\n    (1, '2021-01-01 19:52:46', 20.17),\n    (1, '2021-01-02 02:01:17', 22.43),\n    (1, '2021-01-02 21:02:34', 19.91),\n    (1, '2021-01-03 04:12:56', 20.11),\n    (1, '2021-01-03 20:23:12', 20.22),\n    (1, '2021-01-04 05:34:23', 20.04),\n    (1, '2021-01-04 21:45:34', 22.69),\n    (1, '2021-01-05 06:56:45', 20.50),\n    (1, '2021-01-05 22:07:56', 20.32),\n    (1, '2021-01-06 07:18:07', 20.17),\n    (1, '2021-01-06 23:29:18', 23.58),\n    (2, '2021-01-01 04:30:10', 21.52),\n    (2, '2021-01-01 17:12:25', 21.48),\n    (2, '2021-01-02 01:59:43', 23.10),\n    (2, '2021-01-02 20:05:53', 18.19),\n    (2, '2021-01-03 03:17:12', 21.23),\n    (2, '2021-01-03 19:25:20', 21.27),\n    (2, '2021-01-04 04:33:34', 21.51),\n    (2, '2021-01-04 20:41:45', 21.49),\n    (2, '2021-01-05 05:49:56', 21.52),\n    (2, '2021-01-05 21:58:07', 21.48),\n    (2, '2021-01-06 07:06:18', 21.50),\n    (2, '2021-01-06 23:14:29', 21.52)\n;\n</code></pre> <p>The solution can be found at:</p> <ul> <li>temperature-anomaly-detection.md</li> </ul> Sample input site_id reading_datetime temperature 1 2021-06-01 02:12:31 26.17 1 2021-06-01 21:17:12 26.32 1 2021-06-02 01:19:56 29.58 1 2021-06-02 19:35:32 27.06 1 2021-06-03 03:14:53 26.26 1 2021-06-03 20:47:42 28.37 <pre><code>with readings(site_id, reading_datetime, temperature) as (\n    values\n        (1, '2021-06-01 02:12:31'::timestamp, 26.17),\n        (1, '2021-06-01 21:17:12'::timestamp, 26.32),\n        (1, '2021-06-02 01:19:56'::timestamp, 29.58),\n        (1, '2021-06-02 19:35:32'::timestamp, 27.06),\n        (1, '2021-06-03 03:14:53'::timestamp, 26.26),\n        (1, '2021-06-03 20:47:42'::timestamp, 28.37)\n)\n</code></pre> Sample output site_id reading_datetime temperature average_temperature percentage_increase 1 2021-06-02 01:19:56 29.58 26.4525 11.8231 <pre><code>solution(site_id, reading_datetime, temperature, average_temperature, percentage_increase) as (\n    values\n        (1, '2021-06-02 01:19:56'::timestamp, 29.58, 26.4525, 11.8231)\n)\n</code></pre> Hint 1 <p>Use a window function (or two!) to calculate the average temperature of the surrounding readings.</p> Hint 2 <p>Use another window function to identify rows with at least 4 surrounding readings (2 before and 2 after).</p>","tags":["window function"]},{"location":"challenging-sql-problems/problems/bronze/uk-bank-holidays/","title":"UK bank holidays \ud83d\udcc5","text":"<p>Note</p> <p>This question is specific to DuckDB:</p> <ul> <li>https://duckdb.org/</li> </ul> <p>Question</p> <p>Using DuckDB, parse the UK bank holiday endpoint into an SQL table.</p> <ul> <li>https://www.gov.uk/bank-holidays.json</li> </ul> <p>Each row in the output should correspond to a single event, and the column headers (below) should map directly to the JSON properties with the same names:</p> <ul> <li><code>division</code></li> <li><code>title</code></li> <li><code>date</code></li> <li><code>notes</code></li> <li><code>bunting</code></li> </ul> <p>Here's a starting point:</p> <pre><code>from 'https://www.gov.uk/bank-holidays.json'\n</code></pre> <p>The solution can be found at:</p> <ul> <li>uk-bank-holidays.md</li> </ul> Sample input <pre><code>with bank_holidays(\"england-and-wales\", \"scotland\", \"northern-ireland\") as (\n    values (\n        {\n            division: 'england-and-wales',\n            events: [\n                {title: 'New Year\u2019s Day', date: '2018-01-01', notes: '', bunting: true},\n                {title: 'Good Friday',    date: '2018-03-30', notes: '', bunting: false},\n            ],\n        },\n        {\n            division: 'scotland',\n            events: [\n                {title: 'New Year\u2019s Day', date: '2018-01-01', notes: '', bunting: true},\n                {title: '2nd January',    date: '2018-01-02', notes: '', bunting: true},\n            ],\n        },\n        {\n            division: 'northern-ireland',\n            events: [\n                {title: 'New Year\u2019s Day',   date: '2018-01-01', notes: '',               bunting: true},\n                {title: 'St Patrick\u2019s Day', date: '2018-03-19', notes: 'Substitute day', bunting: true},\n            ],\n        },\n    )\n)\n</code></pre> Sample output division title date notes bunting england-and-wales New Year\u2019s Day 2018-01-01 true england-and-wales Good Friday 2018-03-30 false scotland New Year\u2019s Day 2018-01-01 true scotland 2nd January 2018-01-02 true northern-ireland New Year\u2019s Day 2018-01-01 true northern-ireland St Patrick\u2019s Day 2018-03-19 Substitute day true <pre><code>solution(division, title, date, notes, bunting) as (\n    values\n        ('england-and-wales', 'New Year\u2019s Day',   '2018-01-01', '',               true),\n        ('england-and-wales', 'Good Friday',      '2018-03-30', '',               false),\n        ('scotland',          'New Year\u2019s Day',   '2018-01-01', '',               true),\n        ('scotland',          '2nd January',      '2018-01-02', '',               true),\n        ('northern-ireland',  'New Year\u2019s Day',   '2018-01-01', '',               true),\n        ('northern-ireland',  'St Patrick\u2019s Day', '2018-03-19', 'Substitute day', true)\n)\n</code></pre> Hint 1 <p>Use <code>UNPIVOT</code> to move the separate columns for each division into a single column.</p> Hint 2 <p>Use <code>UNNEST</code> to explode the event JSON into separate rows and columns.</p>","tags":["pivot and unpivot","semi-structured data"]},{"location":"challenging-sql-problems/problems/gold/encoding-datelist-ints/","title":"Encoding datelist ints \ud83d\udd10","text":"<p>Scenario</p> <p>Inspired by the modelling approach used by the social media platform from the customer churn/decoding datelist ints problems, the company from the suspicious login activity/bannable login activity problems want to implement the \"datelist integer\" for their users' login history.</p> <p>This company has also given us more information about how their system works: logins expire after 24 hours, and only manual user logouts are recorded in the events table.</p> <p>This means that:</p> <ul> <li>A consecutive login within 24 hours (inclusive) of a previous login keeps the user logged in.</li> <li>A logout event when the user is already logged out does nothing; it can be ignored.</li> </ul> <p>Question</p> <p>Encode the login history for each user into a datelist integer, where a day is flagged as <code>1</code> if the user was logged in at any point on that day, <code>0</code> otherwise.</p> <p>The encoding should be relative to the day of the latest event in the table.</p> <p>The output should have a single row per user with the columns:</p> <ul> <li><code>user_id</code></li> <li><code>last_update</code> as the date of the latest event (the date that the encoding is relative to)</li> <li><code>activity_history</code> as the datelist integer</li> </ul> <p>Order the output by <code>user_id</code>.</p> Expand for the DDL <pre><code>create table events (\n    event_id       integer primary key,\n    user_id        integer not null,\n    event_datetime timestamp not null,\n    event_type     varchar not null\n);\ninsert into events\nvalues\n    (1,  1, '2024-01-01 11:00:00', 'login'),\n    (2,  1, '2024-01-01 12:00:00', 'logout'),\n    (3,  1, '2024-01-03 03:00:00', 'login failed'),\n    (4,  1, '2024-01-03 03:01:00', 'login failed'),\n    (5,  1, '2024-01-03 03:02:00', 'login failed'),\n    (6,  1, '2024-01-03 03:05:00', 'login'),\n    (7,  2, '2024-01-03 10:00:00', 'login'),\n    (8,  2, '2024-01-03 15:00:00', 'logout'),\n    (9,  1, '2024-01-03 23:00:00', 'logout'),\n    (10, 2, '2024-01-04 22:00:00', 'login failed'),\n    (11, 2, '2024-01-04 22:05:00', 'login'),\n    (12, 3, '2024-01-05 20:00:00', 'login'),\n    (13, 3, '2024-01-06 04:00:00', 'logout'),\n    (14, 2, '2024-01-09 15:00:00', 'logout'),\n    (15, 3, '2024-01-11 21:00:00', 'login'),\n    (16, 1, '2024-01-12 12:00:00', 'login failed'),\n    (17, 1, '2024-01-12 13:00:00', 'login failed'),\n    (18, 1, '2024-01-12 23:00:00', 'login failed'),\n    (19, 2, '2024-01-13 10:00:00', 'login failed'),\n    (20, 2, '2024-01-13 10:05:00', 'login'),\n    (21, 2, '2024-01-13 15:00:00', 'logout'),\n    (22, 1, '2024-01-13 23:00:00', 'login failed'),\n    (23, 1, '2024-01-13 23:01:00', 'login failed'),\n    (24, 1, '2024-01-13 23:02:00', 'login failed'),\n    (25, 2, '2024-01-14 22:00:00', 'login'),\n    (26, 3, '2024-01-15 20:00:00', 'login'),\n    (27, 3, '2024-01-16 04:00:00', 'logout'),\n    (28, 2, '2024-01-19 15:00:00', 'logout'),\n    (29, 3, '2024-01-21 21:00:00', 'login'),\n    (30, 1, '2024-01-22 12:00:00', 'login failed'),\n    (31, 1, '2024-01-22 12:05:00', 'password reset'),\n    (32, 1, '2024-01-22 12:10:00', 'login'),\n    (33, 1, '2024-01-22 13:00:00', 'logout'),\n    (34, 1, '2024-01-23 03:00:00', 'login'),\n    (35, 2, '2024-01-23 10:00:00', 'login'),\n    (36, 2, '2024-01-23 15:00:00', 'logout'),\n    (37, 1, '2024-01-23 23:00:00', 'logout'),\n    (38, 2, '2024-01-24 22:00:00', 'login'),\n    (39, 3, '2024-01-25 20:00:00', 'login'),\n    (40, 3, '2024-01-26 04:00:00', 'logout'),\n    (41, 2, '2024-01-29 15:00:00', 'logout'),\n    (42, 3, '2024-01-30 21:00:00', 'login failed'),\n    (43, 3, '2024-01-30 21:01:00', 'login failed'),\n    (44, 3, '2024-01-30 21:02:00', 'login failed'),\n    (45, 3, '2024-01-30 21:03:00', 'login failed'),\n    (46, 3, '2024-01-30 21:04:00', 'login failed'),\n    (47, 3, '2024-01-30 21:05:00', 'password reset'),\n    (48, 3, '2024-01-30 21:06:00', 'password reset'),\n    (49, 3, '2024-01-30 21:07:00', 'password reset'),\n    (50, 3, '2024-01-30 21:08:00', 'password reset'),\n    (51, 3, '2024-01-30 21:09:00', 'password reset'),\n    (52, 3, '2024-01-30 21:10:00', 'password reset'),\n    (53, 3, '2024-01-31 23:55:00', 'login failed'),\n    (54, 3, '2024-01-31 23:56:00', 'login failed'),\n    (55, 3, '2024-01-31 23:57:00', 'login failed'),\n    (56, 3, '2024-01-31 23:58:00', 'login failed'),\n    (57, 3, '2024-01-31 23:59:00', 'login failed'),\n    (58, 3, '2024-02-01 00:00:00', 'login failed'),\n    (59, 3, '2024-02-01 00:01:00', 'login failed'),\n    (60, 3, '2024-02-01 00:02:00', 'login failed')\n;\n</code></pre> <p>The solution can be found at:</p> <ul> <li>encoding-datelist-ints.md</li> </ul> <p>A worked example is provided below to help illustrate the encoding.</p> Sample input event_id user_id event_datetime event_type 1 1 2024-01-01 01:03:00 login 2 1 2024-01-04 01:02:00 login 3 1 2024-01-05 01:01:00 login 4 1 2024-01-06 01:00:00 logout 5 1 2024-01-07 01:05:00 logout 6 1 2024-01-07 01:06:00 logout 7 2 2024-01-08 01:07:00 login 8 2 2024-01-09 01:08:00 login 9 2 2024-01-10 01:09:00 login 10 2 2024-01-10 01:10:00 logout 11 2 2024-01-11 01:11:00 logout 12 2 2024-01-12 01:12:00 logout <pre><code>with events(event_id, user_id, event_datetime, event_type) as (\n    values\n        (1,  1, '2024-01-01 01:03:00'::timestamp, 'login'),\n        (2,  1, '2024-01-04 01:02:00'::timestamp, 'login'),\n        (3,  1, '2024-01-05 01:01:00'::timestamp, 'login'),\n        (4,  1, '2024-01-06 01:00:00'::timestamp, 'logout'),\n        (5,  1, '2024-01-07 01:05:00'::timestamp, 'logout'),\n        (6,  1, '2024-01-07 01:06:00'::timestamp, 'logout'),\n        (7,  2, '2024-01-08 01:07:00'::timestamp, 'login'),\n        (8,  2, '2024-01-09 01:08:00'::timestamp, 'login'),\n        (9,  2, '2024-01-10 01:09:00'::timestamp, 'login'),\n        (10, 2, '2024-01-10 01:10:00'::timestamp, 'logout'),\n        (11, 2, '2024-01-11 01:11:00'::timestamp, 'logout'),\n        (12, 2, '2024-01-12 01:12:00'::timestamp, 'logout')\n)\n</code></pre> Sample output user_id last_update activity_history 1 2024-01-12 3520 2 2024-01-12 28 <pre><code>solution(user_id, last_update, activity_history) as (\n    values\n        (1, '2024-01-12'::date, 3520),\n        (2, '2024-01-12'::date,   28)\n)\n</code></pre> Hint 1 <p>First construct a table with one row per day per user, and a column which flags if the user was logged in at any point on that day.</p> Hint 2 <p>Multiply the flag column by a power of 2 to get the value component for that day, then sum them up to get the datelist integer.</p> <p>Note that the power of 2 should be the number of days between the current row's day and the latest event's day.</p>","tags":["window function","gaps and islands","correlated subquery","custom axis"]},{"location":"challenging-sql-problems/problems/gold/encoding-datelist-ints/#worked-example","title":"Worked example","text":"<p>To help illustrate the encoding, consider the following events in the Sample input.</p> <p>We'll walk through each of the events and how they contribute to different sessions.</p> <ol> <li>User 1 logs in at <code>2024-01-01 01:03:00</code>, starting the first session. Their next event is a login at <code>2024-01-04 01:02:00</code> which is over 24 hours; so the first session expires at <code>2024-01-02 01:03:00</code>.</li> <li>User 1 logs in at <code>2024-01-04 01:02:00</code>, starting the second session. Their next event is a login at <code>2024-01-05 01:01:00</code> which is under 24 hours; so the second session continues.</li> <li>User 1 logs in at <code>2024-01-05 01:01:00</code>, continuing the second session. Their next event is a logout at <code>2024-01-06 01:00:00</code> which is under 24 hours; so the second session ends at <code>2024-01-06 01:00:00</code>.</li> <li>User 1 logs out at <code>2024-01-06 01:00:00</code>, which we've already accounted for.</li> <li>User 1 logs out at <code>2024-01-07 01:05:00</code>, which does nothing.</li> <li>User 1 logs out at <code>2024-01-07 01:06:00</code>, which does nothing.</li> <li>User 2 logs in at <code>2024-01-08 01:07:00</code>, starting the third session. Their next event is a login at <code>2024-01-09 01:08:00</code> which is under 24 hours; so the third session continues.</li> <li>User 2 logs in at <code>2024-01-09 01:08:00</code>, continuing the third session. Their next event is a login at <code>2024-01-10 01:09:00</code> which is under 24 hours; so the third session continues.</li> <li>User 2 logs in at <code>2024-01-10 01:09:00</code>, continuing the third session. Their next event is a logout at <code>2024-01-10 01:10:00</code> which is under 24 hours; so the third session ends at <code>2024-01-10 01:10:00</code>.</li> <li>User 2 logs out at <code>2024-01-10 01:10:00</code>, which we've already accounted for.</li> <li>User 2 logs out at <code>2024-01-11 01:11:00</code>, which does nothing.</li> <li>User 2 logs out at <code>2024-01-12 01:12:00</code>, which does nothing.</li> </ol> <p>This gives us the following sessions:</p> user_id login_date logout_date 1 2024-01-01 01:03:00 2024-01-02 01:03:00 1 2024-01-04 01:02:00 2024-01-06 01:00:00 2 2024-01-08 01:07:00 2024-01-09 01:07:00 2 2024-01-09 01:08:00 2024-01-10 01:08:00 2 2024-01-10 01:09:00 2024-01-10 01:10:00 <p>The earliest and latest events in the <code>events</code> table are on <code>2024-01-01</code> and <code>2024-01-12</code> respectively, so the encoding is relative to the <code>2024-01-12</code>. We can plot the sessions on a timeline:</p> active_date user_1_is_active user_2_is_active 2024-01-01 1 0 2024-01-02 1 0 2024-01-03 0 0 2024-01-04 1 0 2024-01-05 1 0 2024-01-06 1 0 2024-01-07 0 0 2024-01-08 0 1 2024-01-09 0 1 2024-01-10 0 1 2024-01-11 0 0 2024-01-12 0 0 <p>Since the latest event is on <code>2024-01-12</code>, the encoding for <code>2024-01-12</code> uses 2<sup>0</sup>, the encoding for <code>2024-01-11</code> uses 2<sup>1</sup>, and so on. This gives us the following datelist integers:</p> <ul> <li>User 1: 2048 + 1024 + 256 + 128 + 64 = 3520</li> <li>User 2: 16 + 8 + 4 = 28</li> </ul>","tags":["window function","gaps and islands","correlated subquery","custom axis"]},{"location":"challenging-sql-problems/problems/gold/loan-repayment-schedule/","title":"Loan repayment schedule \ud83d\udcb0","text":"<p>Scenario</p> <p>A bank is trying to generate statements and forecasts for their loan customers.</p> <p>They need to know the repayment schedules for each loan so they can accurately communicate to their customers, as well as track the repayments and forecast their cash flow.</p> <p>Question</p> <p>For the loan details below, generate the loan repayment schedules for the loans.</p> <p>The output should have a row per loan per repayment, with the columns:</p> <ul> <li><code>loan_id</code></li> <li><code>repayment_number</code> as the repayment number</li> <li><code>repayment_date</code> as the date of the repayment</li> <li><code>interest</code> as the interest component of the repayment</li> <li><code>principal</code> as the principal component of the repayment</li> <li><code>total</code> as the total value of the repayment</li> <li><code>balance</code> as the outstanding balance after the repayment</li> </ul> <p>Order the output by <code>loan_id</code> and <code>repayment_number</code>.</p> Expand for the DDL <pre><code>create table loans (\n    loan_id       integer primary key,\n    loan_value    decimal(10, 2) not null,\n    interest_rate decimal(5, 4) not null,\n    repayments    integer not null,\n    start_date    date not null\n);\ninsert into loans\nvalues\n    (1,  80000.00, 0.020,  6, '2024-01-01'),\n    (2,  75000.00, 0.015, 12, '2024-01-02'),\n    (3, 100000.00, 0.010, 24, '2024-01-03')\n;\n</code></pre> <p>The loans have the following details:</p> <ul> <li><code>loan_id</code>: The unique identifier for the loan</li> <li><code>loan_value</code>: The total value of the loan</li> <li><code>interest_rate</code>: The monthly interest rate</li> <li><code>repayments</code>: The number of monthly repayments to make on the loan</li> <li><code>start_date</code>: The date the loan was taken out</li> </ul> <p>The repayments are due exactly one month after each other (no need to account for weekends or holidays), and the first repayment is due one month after the <code>start_date</code>. The <code>start_date</code> will never be on the 29th, 30th, or 31st of the month.</p> <p>For each loan, the monthly repayment will be for the same amount (except the final one) which you need to calculate, or check Hint 0 below. The monthly repayment must be rounded to two decimal places, but any rounding error should be accounted for in the final repayment so that the outstanding balance is exactly zero after the final repayment.</p> <p>Each repayment, the interest is calculated and added first, and then the repayment is subtracted from the balance. The interest is calculated on the current outstanding balance and rounded to two decimal places.</p> <p>A monthly repayment will be made up of two parts: the interest and the principal. The interest is calculated as described above, and the principal is the difference between the monthly repayment and the interest so is the amount that goes towards actually paying off the loan.</p> <p>The solution can be found at:</p> <ul> <li>loan-repayment-schedule.md</li> </ul> <p>A worked example is provided below to help illustrate the loan calculations.</p> Sample input loan_id loan_value interest_rate repayments start_date 1 10000.00 0.0100 6 2024-01-01 <pre><code>with loans(loan_id, loan_value, interest_rate, repayments, start_date) as (\n    values\n        (1, 10000.00, 0.0100, 6, '2024-01-01'::date);\n)\n</code></pre> Sample output loan_id repayment_number repayment_date interest principal total balance 1 1 2024-02-01 100.00 1625.48 1725.48 8374.52 1 2 2024-03-01 83.75 1641.73 1725.48 6732.79 1 3 2024-04-01 67.33 1658.15 1725.48 5074.64 1 4 2024-05-01 50.75 1674.73 1725.48 3399.91 1 5 2024-06-01 34.00 1691.48 1725.48 1708.43 1 6 2024-07-01 17.08 1708.43 1725.51 0.00 <pre><code>solution(loan_id, repayment_number, repayment_date, interest, principal, total, balance) as (\n    values\n        (1, 1, '2024-02-01'::date, 100.00, 1625.48, 1725.48, 8374.52),\n        (1, 2, '2024-03-01'::date,  83.75, 1641.73, 1725.48, 6732.79),\n        (1, 3, '2024-04-01'::date,  67.33, 1658.15, 1725.48, 5074.64),\n        (1, 4, '2024-05-01'::date,  50.75, 1674.73, 1725.48, 3399.91),\n        (1, 5, '2024-06-01'::date,  34.00, 1691.48, 1725.48, 1708.43),\n        (1, 6, '2024-07-01'::date,  17.08, 1708.43, 1725.51,    0.00)\n)\n</code></pre> Hint 0 <p>The formula for calculating the monthly repayment is:</p> <ul> <li><code>(1 + interest_rate)</code> to the power of <code>repayments</code> as <code>amortised_rate</code>, then</li> <li><code>loan_value * interest_rate * amortised_rate / (amortised_rate - 1)</code> as <code>monthly_repayment</code></li> </ul> Hint 1 <p>Use a recursive CTE to generate and calculate the rows for the repayment schedule.</p> Hint 2 <p>For the recursive CTE's anchor statement, start with a dummy row for each loan with only the loan value and the start date. Then, recursively calculate the interest, principal, and balance for each repayment in the recursive statement.</p> Hint 3 <p>Calculate the final repayment's details separately to account for any rounding errors.</p>","tags":["recursive CTE"]},{"location":"challenging-sql-problems/problems/gold/loan-repayment-schedule/#worked-example","title":"Worked example","text":"<p>To help illustrate the loan calculations, consider the loan in the Sample input.</p> <p>A loan with these details will have a monthly repayment value of 1,725.48 (rounded to 2 decimal places).</p> <p>Let's walk through a few repayments.</p>","tags":["recursive CTE"]},{"location":"challenging-sql-problems/problems/gold/loan-repayment-schedule/#the-first-repayment","title":"The first repayment","text":"<ul> <li>The first repayment is due on 2024-02-01</li> <li>The interest is calculated on the outstanding balance of 10,000.00</li> <li>The interest is 1%, so the interest for the month is 100.00 (10,000.00 * 0.01)</li> <li>The repayment is 1,725.48, so the outstanding balance after the repayment is 8,374.52 (10,000.00 + 100.00 - 1,725.48)</li> <li>We note that the principal component of the repayment is 1,625.48 (1,725.48 - 100.00)</li> </ul>","tags":["recursive CTE"]},{"location":"challenging-sql-problems/problems/gold/loan-repayment-schedule/#the-second-repayment","title":"The second repayment","text":"<ul> <li>The second repayment is due on 2024-03-01</li> <li>The interest is calculated on the outstanding balance of 8,374.52</li> <li>The interest is 1%, so the interest for the month is 83.75 (8,374.52 * 0.01)</li> <li>The repayment is 1,725.48, so the outstanding balance after the repayment is 6,732.79 (8,374.52 + 83.75 - 1,725.48)</li> <li>We note that the principal component of the repayment is 1,641.73 (1,725.48 - 83.75)</li> </ul>","tags":["recursive CTE"]},{"location":"challenging-sql-problems/problems/gold/loan-repayment-schedule/#the-third-fourth-and-fifth-repayments","title":"The third, fourth, and fifth repayments","text":"<ul> <li>The interest and principal components are calculated in the same way as above</li> <li>The outstanding balance after the fifth repayment is 1,708.43</li> </ul>","tags":["recursive CTE"]},{"location":"challenging-sql-problems/problems/gold/loan-repayment-schedule/#the-final-repayment","title":"The final repayment","text":"<ul> <li>The final repayment is due on 2024-07-01</li> <li>The interest is calculated on the outstanding balance of 1,708.43</li> <li>The interest is 1%, so the interest for the month is 17.08 (1,708.43 * 0.01)</li> <li>Since this is the final repayment and we want to account for any rounding errors, the repayment is the outstanding balance plus the interest: 1,725.51 (1,708.43 + 17.08)</li> </ul> <p>Therefore, the repayment schedule for this loan would look like:</p> loan_id repayment_number repayment_date interest principal total balance 1 1 2024-02-01 100.00 1625.48 1725.48 8374.52 1 2 2024-03-01 83.75 1641.73 1725.48 6732.79 1 3 2024-04-01 67.33 1658.15 1725.48 5074.64 1 4 2024-05-01 50.75 1674.73 1725.48 3399.91 1 5 2024-06-01 34.00 1691.48 1725.48 1708.43 1 6 2024-07-01 17.08 1708.43 1725.51 0.00","tags":["recursive CTE"]},{"location":"challenging-sql-problems/problems/gold/risk-invasions/","title":"Risk invasions \ud83d\udee1\ufe0f","text":"<p>Scenario</p> <p>You work for a company that has built a digital version of Risk, the board game. To improve the player experience, the product managers want to add a feature which shows the success likelihood of an invasion.</p> <p>Note</p> <p>If you don't know the invasion rules of Risk, check out the following video (06:34 to 08:45):</p> <ul> <li>https://youtu.be/Xo8RSozX6Ac?t=394</li> </ul> <p>Question</p> <p>As a proof of concept, write a query to calculate the outcome likelihoods of an invasion where the attacker is committing 8 troops and the defender is committing 6 troops.</p> <p>Assume that both the attacker and defender commit the maximum available troops that they can with each battle (attackers commit up to 3 troops, defenders commit up to 2 troops), and that the invasion continues until either the attacker or defender has no troops left.</p> <p>The output should have a row per possible outcome, which is the different combinations of attacker and defender troops remaining after the invasion has finished.</p> <p>The output should have the columns:</p> <ul> <li><code>attackers_remaining</code> as the number of troops the attacker has left</li> <li><code>defenders_remaining</code> as the number of troops the defender has left</li> <li><code>likelihood</code> as the probability of that outcome occurring</li> <li><code>attackers_win_likelihood</code> as the probability of the attacker winning the invasion</li> <li><code>defenders_win_likelihood</code> as the probability of the defender winning the invasion</li> </ul> <p>The likelihoods should be calculated to 10 decimal places.</p> <p>Order the output by <code>attackers_remaining</code> and <code>defenders_remaining</code>.</p> <p>The solution can be found at:</p> <ul> <li>risk-invasions.md</li> </ul> <p>A worked example is provided below to help illustrate the invasion steps.</p> Sample input <p>To not give away any spoilers, we'll tweak the rules for this example.</p> <p>Assume that attackers can commit up to 2 troops and defenders can commit up to 1 troop.</p> <p>Calculate the outcome likelihoods of an invasion where the attacker is committing 3 troops and the defender is committing 2 troops.</p> Sample output attackers_remaining defenders_remaining likelihood attackers_win_likelihood defenders_win_likelihood 0 1 0.1629735937 0.7334902409 0.2665097591 0 2 0.1035361654 0.7334902409 0.2665097591 1 0 0.1164097098 0.7334902409 0.2665097591 2 0 0.2821825544 0.7334902409 0.2665097591 3 0 0.3348979767 0.7334902409 0.2665097591 <pre><code>solution(attackers_remaining, defenders_remaining, likelihood, attackers_win_likelihood, defenders_win_likelihood) as (\n    values\n        (0, 1, 0.1629735937, 0.7334902409, 0.2665097591),\n        (0, 2, 0.1035361654, 0.7334902409, 0.2665097591),\n        (1, 0, 0.1164097098, 0.7334902409, 0.2665097591),\n        (2, 0, 0.2821825544, 0.7334902409, 0.2665097591),\n        (3, 0, 0.3348979767, 0.7334902409, 0.2665097591)\n)\n</code></pre> Hint 1 <p>Create (recursive) CTEs for:</p> <ul> <li>All possible outcomes of each die roll</li> <li>All possible outcomes of each battle</li> </ul> <p>...and calculate the likelihood of each row in each CTE.</p> Hint 2 <p>Use a recursive CTE to walk through all the possible outcomes of an invasion, calculating the likelihood of each outcome.</p> Hint 3 <p>Don't track battle outcomes as \"win/lose\". Instead, track the number of attacks won: an attacker with 3 dice against a defender with 2 dice can win 0, 1, or 2 attacks.</p>","tags":["window function","recursive CTE","custom axis"]},{"location":"challenging-sql-problems/problems/gold/risk-invasions/#worked-example","title":"Worked example","text":"<p>To help illustrate the invasion steps, consider the scenario in the Sample input.</p>","tags":["window function","recursive CTE","custom axis"]},{"location":"challenging-sql-problems/problems/gold/risk-invasions/#battle-scenarios","title":"Battle scenarios","text":"<p>Given that attackers can commit up to 2 troops and defenders can commit up to 1 troop, each battle has two scenarios:</p> <ol> <li>The attacker commits 2 troops and the defender commits 1 troop.</li> <li>The attacker commits 1 troop and the defender commits 1 troop.</li> </ol> <p>Note that we should only see the second scenario when the attacker has 1 troop remaining as we assume that the attacker commits the maximum available troops with each battle.</p> <p>First, we'll calculate the likelihood of the attacker winning in each scenario.</p>","tags":["window function","recursive CTE","custom axis"]},{"location":"challenging-sql-problems/problems/gold/risk-invasions/#attacker-commits-2-troops-and-defender-commits-1-troop","title":"Attacker commits 2 troops and defender commits 1 troop","text":"<p>Consider all the outcomes of each dice roll.</p> <p>There are 216 (6 * 6 * 6) possible outcomes: 6 for the attacker's first die, 6 for the attacker's second die, and 6 for the defender's die.</p> <p>The attacker wins if the attacker's highest die is greater than the defender's die.</p> <ul> <li>When the defender rolls a 6, the attacker always loses: this is 36 of the outcomes.</li> <li>When the defender rolls a 5, the attacker wins if the attacker rolls a 6: this is 11 of the outcomes, and the defender wins in the other 25 for this defender roll.</li> <li>When the defender rolls a 4, the attacker wins if the attacker rolls a 5 or 6: this is 20 of the outcomes, and the defender wins in the other 16 for this defender roll.</li> <li>When the defender rolls a 3, the attacker wins if the attacker rolls a 4, 5, or 6: this is 27 of the outcomes, and the defender wins in the other 9 for this defender roll.</li> <li>When the defender rolls a 2, the attacker wins if the attacker rolls a 3, 4, 5, or 6: this is 32 of the outcomes, and the defender wins in the other 4 for this defender roll.</li> <li>When the defender rolls a 1, the attacker wins if one of their rolls is not a 1: this is 35 of the outcomes, and the defender wins in the other 1 for this defender roll.</li> </ul> <p>The total number of outcomes where the attacker wins is 0 + 11 + 20 + 27 + 32 + 35 = 125, so the likelihood of the attacker winning is 125 / 216 = 0.578703704.</p>","tags":["window function","recursive CTE","custom axis"]},{"location":"challenging-sql-problems/problems/gold/risk-invasions/#attacker-commits-1-troop-and-defender-commits-1-troop","title":"Attacker commits 1 troop and defender commits 1 troop","text":"<p>Consider all the outcomes of each dice roll.</p> <p>There are 36 (6 * 6) possible outcomes: 6 for the attacker's die, and 6 for the defender's die.</p> <p>The attacker wins if the attacker's highest die is greater than the defender's die.</p> <ul> <li>When the defender rolls a 6, the attacker always loses: this is 6 of the outcomes.</li> <li>When the defender rolls a 5, the attacker wins if the attacker rolls a 6: this is 1 of the outcomes, and the defender wins in the other 5 for this defender roll.</li> <li>When the defender rolls a 4, the attacker wins if the attacker rolls a 5 or 6: this is 2 of the outcomes, and the defender wins in the other 4 for this defender roll.</li> <li>When the defender rolls a 3, the attacker wins if the attacker rolls a 4, 5, or 6: this is 3 of the outcomes, and the defender wins in the other 3 for this defender roll.</li> <li>When the defender rolls a 2, the attacker wins if the attacker rolls a 3, 4, 5, or 6: this is 4 of the outcomes, and the defender wins in the other 2 for this defender roll.</li> <li>When the defender rolls a 1, the attacker wins if one of their rolls is not a 1: this is 5 of the outcomes, and the defender wins in the other 1 for this defender roll.</li> </ul> <p>The total number of outcomes where the attacker wins is 0 + 1 + 2 + 3 + 4 + 5 = 15, so the likelihood of the attacker winning is 15 / 36 = 0.416666667.</p> <p>To summarise, we have the following likelihoods for each scenario:</p> attackers defenders attacks_won attacks_lost likelihood 1 1 1 0 0.4166666667 1 1 0 1 0.5833333333 2 1 1 0 0.5787037037 2 1 0 1 0.4212962963","tags":["window function","recursive CTE","custom axis"]},{"location":"challenging-sql-problems/problems/gold/risk-invasions/#invasion-outcomes","title":"Invasion outcomes","text":"<p>An invasion is a series of battles, so we can calculate the likelihood of each outcome by multiplying the likelihoods of each battle.</p> <p>Starting with the attacker committing 3 troops and the defender committing 2 troops, we have the following possible paths for the invasion:</p> <pre><code>flowchart LR\n    A3D2[\"3 Attackers\\n2 Defenders\"]\n    A2D2[\"2 Attackers\\n2 Defenders\"]\n    A1D2[\"1 Attacker\\n2 Defenders\"]\n    A0D2[\"0 Attackers\\n2 Defenders\"]\n    A3D1[\"3 Attackers\\n1 Defender\"]\n    A2D1[\"2 Attackers\\n1 Defender\"]\n    A1D1[\"1 Attacker\\n1 Defender\"]\n    A0D1[\"0 Attackers\\n1 Defender\"]\n    A3D0[\"3 Attackers\\n0 Defenders\"]\n    A2D0[\"2 Attackers\\n0 Defenders\"]\n    A1D0[\"1 Attacker\\n0 Defenders\"]\n\n    A3D2 --&gt;|\"attacker wins\\n(0.5787...)\"| A3D1\n    A3D2 --&gt;|\"defender wins\\n(0.4213...)\"| A2D2\n    A2D2 --&gt;|\"attacker wins\\n(0.5787...)\"| A2D1\n    A2D2 --&gt;|\"defender wins\\n(0.4213...)\"| A1D2\n    A3D1 ----&gt;|\"attacker wins\\n(0.5787...)\"| A3D0\n    A3D1 --&gt;|\"defender wins\\n(0.4213...)\"| A2D1\n    A2D1 ---&gt;|\"attacker wins\\n(0.5787...)\"| A2D0\n    A2D1 --&gt;|\"defender wins\\n(0.4213...)\"| A1D1\n\n    A1D2 --&gt;|\"attacker wins\\n(0.4167...)\"| A1D1\n    A1D2 ---&gt;|\"defender wins\\n(0.5833...)\"| A0D2\n    A1D1 --&gt;|\"attacker wins\\n(0.4167...)\"| A1D0\n    A1D1 --&gt;|\"defender wins\\n(0.5833...)\"| A0D1</code></pre> <p>The likelihood of each branch was calculated in the previous section. We can calculate the likelihood of each outcome by:</p> <ol> <li>Multiplying the likelihoods of each branch until we reach the outcome</li> <li>Summing the likelihoods of the paths that lead to the same outcome</li> </ol> <p>We'll walk through two examples.</p>","tags":["window function","recursive CTE","custom axis"]},{"location":"challenging-sql-problems/problems/gold/risk-invasions/#attacker-wins-with-3-troops-remaining","title":"Attacker wins with 3 troops remaining","text":"<p>There is only one path to this outcome: the attacker wins the first battle and the second battle. These each have a likelihood of 0.5787037037, so the likelihood of the attacker winning with 3 troops remaining is:</p> <ul> <li>0.5787037037 * 0.5787037037 = 0.3348979767.</li> </ul>","tags":["window function","recursive CTE","custom axis"]},{"location":"challenging-sql-problems/problems/gold/risk-invasions/#defender-wins-with-1-troop-remaining","title":"Defender wins with 1 troop remaining","text":"<p>There are three paths to this outcome:</p> <ol> <li>Attacker wins, defender wins, defender wins, defender wins</li> <li>Defender wins, attacker wins, defender wins, defender wins</li> <li>Defender wins, defender wins, attacker wins, defender wins</li> </ol> <p>Each path has the following likelihood, respectively:</p> <ul> <li>0.5787037037 * 0.4212962963 * 0.4212962963 * 0.5833333333 = 0.0599167624</li> <li>0.4212962963 * 0.5787037037 * 0.4212962963 * 0.5833333333 = 0.0599167624</li> <li>0.4212962963 * 0.4212962963 * 0.4166666667 * 0.5833333333 = 0.0431400689</li> </ul> <p>Therefore, the likelihood of the defender winning with 1 troop remaining is:</p> <ul> <li>0.0599167624 + 0.0599167624 + 0.0431400689 = 0.1629735937.</li> </ul> <p>Considering all the possible paths, we have the following likelihoods for the invasion:</p> attackers_remaining defenders_remaining likelihood 0 1 0.1629735937 0 2 0.1035361654 1 0 0.1164097098 2 0 0.2821825544 3 0 0.3348979767","tags":["window function","recursive CTE","custom axis"]},{"location":"challenging-sql-problems/problems/gold/risk-invasions/#invasion-winner","title":"Invasion winner","text":"<p>Now that we have the likelihoods of each outcome, we can calculate the likelihood of the attacker winning and the defender winning.</p> <p>The likelihood of the attacker winning is the sum of the likelihoods where the attacker has troops remaining:</p> <ul> <li>0.3348979767 + 0.2821825544 + 0.1164097098 = 0.7334902409</li> </ul> <p>Similarly, the likelihood of the defender winning is the sum of the likelihoods where the defender has troops remaining:</p> <ul> <li>0.1629735937 + 0.1035361654 = 0.2665097591</li> </ul> <p>As a sense check, we confirm that the likelihoods sum to 1 (accounting for rounding error):</p> <ul> <li>0.7334902409 + 0.2665097591 ~ 1</li> </ul> <p>Therefore, the final output is:</p> attackers_remaining defenders_remaining likelihood attackers_win_likelihood defenders_win_likelihood 0 1 0.1629735937 0.7334902409 0.2665097591 0 2 0.1035361654 0.7334902409 0.2665097591 1 0 0.1164097098 0.7334902409 0.2665097591 2 0 0.2821825544 0.7334902409 0.2665097591 3 0 0.3348979767 0.7334902409 0.2665097591","tags":["window function","recursive CTE","custom axis"]},{"location":"challenging-sql-problems/problems/gold/supply-chain-network/","title":"Supply chain network \ud83d\ude9b","text":"<p>Scenario</p> <p>A supermarket's supply chain has three main components: stores, depots, and suppliers.</p> <p>In general, stock is sent from a supplier to a depot, and then from the depot to a store; however, there are cases where suppliers send stock directly to stores and depots send stock to other depots.</p> <p>For example:</p> <pre><code>graph LR\n  supplier_2 ----&gt; store_6\n  supplier_2 ---&gt; depot_5\n  supplier_2 ---&gt; depot_4\n  supplier_1 --&gt; depot_3\n  supplier_1 ---&gt; depot_4\n  depot_5 --&gt; depot_4\n  depot_5 ---&gt; store_6\n  depot_5 ---&gt; store_7\n  depot_5 ---&gt; store_8\n  depot_4 ---&gt; store_6\n  depot_4 ---&gt; store_7\n  depot_4 ---&gt; store_8\n  depot_3 --&gt; depot_5\n  depot_3 ---&gt; store_6\n  depot_3 ---&gt; store_7\n  depot_3 ---&gt; store_8</code></pre> <p>Although the supermarket knows how much stock is transported between locations, it doesn't know how much of each stock came from each supplier.</p> <p>This makes it difficult to report various metrics to the suppliers, like the stock balances and sales volumes of their products.</p> <p>Question</p> <p>Determine what the most likely proportion of stock in a store at the end of each day is from each supplier.</p> <p>Assume that stock moves in a queue (first in, first out) in both the depots and the stores.</p> <p>The output should have a row per store per supplier per day, with the columns:</p> <ul> <li><code>stock_date</code></li> <li><code>store_id</code> as the ID of the store</li> <li><code>supplier_id</code> as the ID of the supplier</li> <li><code>stock_volume</code> as the derived volume of stock in the store from the supplier at the end of the day</li> <li><code>stock_proportion</code> as the derived proportion of stock in the store from the supplier. Express this as a percentage rounded to two decimal places</li> </ul> <p>Order the output by <code>stock_date</code>, <code>store_id</code>, and <code>supplier_id</code>.</p> <p>You can choose to show stores that have no stock from a supplier on a given day (i.e., you can show a row with a <code>stock_volume</code> of 0 or not show the row at all, whatever is easiest for you).</p> Expand for the DDL <pre><code>create table locations (\n    location_id   integer primary key,\n    location_type varchar check (location_type in ('supplier', 'depot', 'store'))\n);\ncreate table deliveries (\n    delivery_datetime timestamp,\n    from_location_id  integer references locations(location_id),\n    to_location_id    integer references locations(location_id),\n    product_id        integer,\n    quantity          integer not null,\n    primary key (delivery_datetime, from_location_id, to_location_id, product_id)\n);\ncreate table sales (\n    sale_datetime timestamp,\n    store_id      integer references locations(location_id),\n    product_id    integer,\n    quantity      integer not null,\n    primary key (sale_datetime, store_id, product_id)\n);\n\ninsert into locations\nvalues\n    (1, 'supplier'),\n    (2, 'supplier'),\n    (3, 'depot'),\n    (4, 'depot'),\n    (5, 'depot'),\n    (6, 'store'),\n    (7, 'store'),\n    (8, 'store')\n;\ninsert into deliveries\nvalues\n    ('2024-01-01 01:10:50', 1, 3, 1001, 25),\n    ('2024-01-01 01:23:53', 1, 4, 1001, 25),\n    ('2024-01-01 04:54:05', 2, 4, 1001, 20),\n    ('2024-01-01 16:23:50', 2, 5, 1001, 20),\n    ('2024-01-01 20:49:37', 2, 6, 1001, 10),\n    ('2024-01-02 04:46:17', 3, 7, 1001, 10),\n    ('2024-01-02 05:10:39', 3, 8, 1001, 10),\n    ('2024-01-02 09:44:57', 4, 6, 1001, 35),\n    ('2024-01-02 11:08:09', 5, 6, 1001, 10),\n    ('2024-01-02 11:47:35', 5, 7, 1001, 5),\n    ('2024-01-02 13:06:56', 5, 8, 1001, 5),\n    ('2024-01-02 14:18:25', 3, 5, 1001, 5),\n    ('2024-01-02 15:58:54', 1, 3, 1001, 30),\n    ('2024-01-02 18:22:16', 2, 4, 1001, 25),\n    ('2024-01-02 23:16:51', 2, 5, 1001, 25),\n    ('2024-01-03 12:43:57', 3, 6, 1001, 25),\n    ('2024-01-03 14:55:35', 4, 7, 1001, 20),\n    ('2024-01-03 15:49:15', 4, 8, 1001, 15),\n    ('2024-01-03 18:07:21', 5, 8, 1001, 20),\n    ('2024-01-03 18:12:31', 5, 4, 1001, 5),\n    ('2024-01-03 19:44:16', 1, 3, 1001, 20),\n    ('2024-01-03 19:37:32', 1, 4, 1001, 30),\n    ('2024-01-03 22:33:48', 2, 6, 1001, 20),\n    ('2024-01-04 02:46:31', 3, 6, 1001, 15),\n    ('2024-01-04 05:58:24', 3, 8, 1001, 10),\n    ('2024-01-04 06:04:52', 4, 7, 1001, 25),\n    ('2024-01-04 13:32:47', 4, 8, 1001, 5),\n    ('2024-01-04 19:32:47', 4, 6, 1001, 5),\n    ('2024-01-04 20:38:40', 5, 6, 1001, 5)\n;\ninsert into sales\nvalues\n    ('2024-01-02 07:12:21', 6, 1001,  2),\n    ('2024-01-02 09:51:01', 7, 1001,  4),\n    ('2024-01-02 10:55:42', 8, 1001,  9),\n    ('2024-01-02 11:21:10', 6, 1001, 19),\n    ('2024-01-02 15:02:20', 6, 1001,  1),\n    ('2024-01-02 16:18:00', 6, 1001,  1),\n    ('2024-01-02 18:47:13', 6, 1001,  9),\n    ('2024-01-02 19:15:12', 8, 1001,  5),\n    ('2024-01-02 20:38:01', 6, 1001, 14),\n    ('2024-01-03 07:00:27', 6, 1001,  3),\n    ('2024-01-03 08:56:40', 6, 1001,  1),\n    ('2024-01-03 09:40:07', 6, 1001,  4),\n    ('2024-01-03 10:21:06', 7, 1001,  4),\n    ('2024-01-03 12:31:10', 7, 1001,  6),\n    ('2024-01-03 15:56:56', 8, 1001,  5),\n    ('2024-01-03 17:49:04', 7, 1001, 12),\n    ('2024-01-03 18:02:34', 6, 1001, 12),\n    ('2024-01-03 20:19:42', 7, 1001,  7),\n    ('2024-01-03 20:28:00', 8, 1001, 15),\n    ('2024-01-04 13:07:02', 7, 1001, 24),\n    ('2024-01-04 14:03:39', 8, 1001, 16)\n;\n</code></pre> <p>The solution can be found at:</p> <ul> <li>supply-chain-network.md</li> </ul> <p>A worked example is provided below to help illustrate the stock movement within the locations.</p> Sample input <p>Locations</p> location_id location_type 1 supplier 2 supplier 3 depot 4 depot 5 store <p>Deliveries</p> delivery_date from_location_id to_location_id product_id quantity 2024-01-01 01:23:53 1 3 123 25 2024-01-01 06:27:54 2 4 123 25 2024-01-01 12:27:39 4 5 123 25 2024-01-01 17:12:59 1 3 123 30 2024-01-02 01:27:57 3 5 123 25 2024-01-02 05:16:08 3 4 123 30 2024-01-02 05:40:53 2 3 123 20 2024-01-02 07:29:53 1 4 123 30 2024-01-02 09:22:53 3 5 123 20 2024-01-02 18:28:39 4 5 123 60 <p>Sales</p> sale_datetime store_id product_id quantity 2024-01-01 14:56:12 5 123 5 2024-01-01 16:28:24 5 123 3 2024-01-01 16:35:38 5 123 4 2024-01-01 20:13:46 5 123 2 2024-01-02 09:37:11 5 123 12 2024-01-02 14:02:57 5 123 30 2024-01-02 14:21:39 5 123 3 2024-01-02 16:44:26 5 123 8 2024-01-02 18:28:37 5 123 2 <p>Network diagram</p> <pre><code>graph LR\n  supplier_1 --&gt; depot_3\n  supplier_1 --&gt; depot_4\n  supplier_2 --&gt; depot_3\n  supplier_2 --&gt; depot_4\n  depot_3 --&gt; depot_4\n  depot_3 --&gt; store_5\n  depot_4 --&gt; store_5</code></pre> <pre><code>with\n\nlocations(location_id, location_type) as (\n    values\n        (1, 'supplier'),\n        (2, 'supplier'),\n        (3, 'depot'),\n        (4, 'depot'),\n        (5, 'store')\n),\n\ndeliveries(delivery_date, from_location_id, to_location_id, product_id, quantity) as (\n    values\n        ('2024-01-01 01:23:53'::timestamp, 1, 3, 123, 25),\n        ('2024-01-01 06:27:54'::timestamp, 2, 4, 123, 25),\n        ('2024-01-01 12:27:39'::timestamp, 4, 5, 123, 25),\n        ('2024-01-01 17:12:59'::timestamp, 1, 3, 123, 30),\n        ('2024-01-02 01:27:57'::timestamp, 3, 5, 123, 25),\n        ('2024-01-02 05:16:08'::timestamp, 3, 4, 123, 30),\n        ('2024-01-02 05:40:53'::timestamp, 2, 3, 123, 20),\n        ('2024-01-02 07:29:53'::timestamp, 1, 4, 123, 30),\n        ('2024-01-02 09:22:53'::timestamp, 3, 5, 123, 20),\n        ('2024-01-02 18:28:39'::timestamp, 4, 5, 123, 60)\n),\n\nsales(sale_datetime, store_id, product_id, quantity) as (\n    values\n        ('2024-01-01 14:56:12'::timestamp, 5, 123,  5),\n        ('2024-01-01 16:28:24'::timestamp, 5, 123,  3),\n        ('2024-01-01 16:35:38'::timestamp, 5, 123,  4),\n        ('2024-01-01 20:13:46'::timestamp, 5, 123,  2),\n        ('2024-01-02 09:37:11'::timestamp, 5, 123, 12),\n        ('2024-01-02 14:02:57'::timestamp, 5, 123, 30),\n        ('2024-01-02 14:21:39'::timestamp, 5, 123,  3),\n        ('2024-01-02 16:44:26'::timestamp, 5, 123,  8),\n        ('2024-01-02 18:28:37'::timestamp, 5, 123,  2)\n)\n</code></pre> Sample output stock_date store_id supplier_id stock_volume stock_proportion 2024-01-01 5 1 0 0.00 2024-01-01 5 2 11 100.00 2024-01-02 5 1 30 49.18 2024-01-02 5 2 31 50.82 <pre><code>solution(stock_date, store_id, supplier_id, stock_volume, stock_proportion) as (\n    values\n        ('2024-01-01'::date, 5, 1,  0,   0.00),\n        ('2024-01-01'::date, 5, 2, 11, 100.00),\n        ('2024-01-02'::date, 5, 1, 30,  49.18),\n        ('2024-01-02'::date, 5, 2, 31,  50.82)\n)\n</code></pre> Hint 1 <p>(to be added)</p> Hint 2 <p>(to be added)</p>"},{"location":"challenging-sql-problems/problems/gold/supply-chain-network/#worked-example","title":"Worked example","text":"<p>To help illustrate the stock movement within the locations, consider the locations and deliveries in the Sample input.</p> <p>We'll walk through each of the deliveries and how they contribute to end-of-day stock levels.</p> <p>Since each delivery and sale correspond to the same product, we'll omit mentioning the product ID in the following walkthrough.</p>"},{"location":"challenging-sql-problems/problems/gold/supply-chain-network/#2024-01-01","title":"2024-01-01","text":"<p>First, consider the deliveries:</p> <ul> <li>Supplier 1 sends 25 units to Depot 3; Depot 3 has 25 units from Supplier 1 and 0 units from Supplier 2.</li> <li>Supplier 2 sends 25 units to Depot 4; Depot 4 has 0 units from Supplier 1 and 25 units from Supplier 2.</li> <li>Depot 4 sends 25 units to Store 5; all 25 units are originally from Supplier 2 so:<ul> <li>Store 5 has 0 units from Supplier 1 and 25 units from Supplier 2.</li> <li>Depot 4 has 0 units from either supplier.</li> </ul> </li> <li>Supplier 1 sends 30 units to Depot 3; Depot 3 has 55 units from Supplier 1 and 0 units from Supplier 2.</li> </ul> <p>Then the sales, which we can roll up to the end of the day:</p> <ul> <li>Store 5 sells 14 units throughout the day; all units are from Supplier 2 so Store 5 has 0 units from Supplier 1 and 11 units from Supplier 2.</li> </ul> <p>Therefore, at the end of 2024-01-01, the proportion for Store 5 is 100% from Supplier 2:</p> stock_date store_id supplier_id stock_volume stock_proportion 2024-01-01 5 1 0 0.00 2024-01-01 5 2 11 100.00"},{"location":"challenging-sql-problems/problems/gold/supply-chain-network/#2024-01-02","title":"2024-01-02","text":"<p>First, consider the deliveries:</p> <ul> <li>Depot 3 sends 25 units to Store 5; all 25 units are from Supplier 2 so:<ul> <li>Store 5 has 0 units from Supplier 1 and 36 units from Supplier 2.</li> <li>Depot 3 has 30 units from Supplier 1 and 0 units from Supplier 2.</li> </ul> </li> <li>Depot 3 sends 30 units to Depot 4; all 30 units are from Supplier 2 so:<ul> <li>Depot 4 has 0 units from Supplier 1 and 30 units from Supplier 2.</li> <li>Depot 3 has 0 units from either supplier.</li> </ul> </li> <li>Supplier 2 sends 20 units to Depot 3; Depot 3 has 20 units from Supplier 2 and 0 units from Supplier 1.</li> <li>Supplier 1 sends 30 units to Depot 4; Depot 4 has 30 units from Supplier 1 and 30 units from Supplier 2. The 30 units from Supplier 2 and first in the queue, followed by the 30 units from Supplier 1.</li> <li>Depot 3 sends 20 units to Store 5; all 20 units are from Supplier 2 so:<ul> <li>Store 5 has 0 units from Supplier 1 and 56 units from Supplier 2.</li> <li>Depot 3 has 0 units from either supplier.</li> </ul> </li> <li>Depot 4 sends 60 units to Store 5; 30 units are from Supplier 1 and 30 units are from Supplier 2 so:<ul> <li>Store 5 has 30 units from Supplier 1 and 86 units from Supplier 2. The existing 56 units from Supplier 2 are first in the queue, followed by the new 30 units from Supplier 2, followed by the 30 units from Supplier 1.</li> <li>Depot 4 has 0 units from either supplier.</li> </ul> </li> </ul> <p>Then the sales, which we can roll up to the end of the day:</p> <ul> <li>Store 5 sells 55 units throughout the day; all 86 units from Supplier 2 are first in the queue, so Store 5 has 30 units from Supplier 1 and 31 units from Supplier 2.</li> </ul> <p>Therefore, at the end of 2024-01-02, the proportion for Store 5 is 49.18% from Supplier 1 and 50.82% from Supplier 2:</p> stock_date store_id supplier_id stock_volume stock_proportion 2024-01-02 5 1 30 49.18 2024-01-02 5 2 31 50.82 <p>Combined with the output from 2024-01-01, the output is the same as the Sample output.</p>"},{"location":"challenging-sql-problems/problems/gold/travel-plans/","title":"Travel plans \ud83d\ude82","text":"<p>Scenario</p> <p>You're helping a client plan a trip from New York to Paris, and they want you to find the fastest and cheapest routes.</p> <p>They have collected journey information for routes that they are happy to take into two tables:</p> <ul> <li><code>routes_timetable</code></li> <li><code>routes_schedule</code></li> </ul> <p>The timetable table records individual routes with their full departure/arrival timestamps and cost. The schedule table records the schedule of repeated routes with their schedule definition.</p> <p>The earliest they can leave from New York is <code>2024-01-01 12:00:00-05:00</code>.</p> <p>Question</p> <p>Given the tables that your client has provided, find the fastest and cheapest route from New York to Paris, leaving after <code>2024-01-01 12:00:00-05:00</code>.</p> <p>Give a minimum of 30 minutes and a maximum of 6 hours for \"interchange\" time, which is the time between arrival and departure at the same location. All costs are in the same currency (with no currency specified by the client).</p> <p>The output should have at most two rows (the fastest/cheapest routes, which may be the same route), with the columns:</p> <ul> <li><code>route</code> which is each location in the route separated by a hyphen, e.g. <code>New York - London - Paris</code></li> <li><code>departure_datetime_utc</code> as the departure time (UTC) from New York</li> <li><code>arrival_datetime_utc</code> as the arrival time (UTC) in Paris</li> <li><code>duration</code> as the total duration of the journey</li> <li><code>cost</code> as the total cost of the journey</li> </ul> <p>Order the output by <code>arrival_datetime_utc</code>.</p> Expand for the DDL <pre><code>create table routes_schedule (\n    schedule_id        int primary key,\n    mode_of_transport  varchar not null,\n    from_location      varchar not null,\n    to_location        varchar not null,\n    earliest_departure timetz not null,\n    latest_departure   timetz not null,\n    frequency          time,  /* `null` means that it's daily */\n    duration           time not null,\n    cost               decimal(8, 2) not null,\n);\ninsert into routes_schedule\nvalues\n    (1, 'train', 'London St Pancras', 'London Gatwick',    '08:00:00+00:00', '20:00:00+00:00', '01:00:00', '00:30:00', 17.50),\n    (2, 'train', 'London St Pancras', 'London Gatwick',    '07:30:00+00:00', '22:30:00+00:00', '02:30:00', '01:15:00', 12.00),\n    (3, 'bus',   'London St Pancras', 'London Gatwick',    '06:15:00+00:00', '06:15:00+00:00', null,       '03:30:00',  6.75),\n    (4, 'bus',   'London St Pancras', 'London Gatwick',    '19:30:00+00:00', '19:30:00+00:00', null,       '03:30:00',  6.75),\n    (5, 'train', 'London Gatwick',    'London St Pancras', '09:00:00+00:00', '21:00:00+00:00', '01:00:00', '00:30:00', 17.50),\n    (6, 'train', 'London Gatwick',    'London St Pancras', '07:15:00+00:00', '22:15:00+00:00', '02:30:00', '01:15:00', 12.00),\n    (7, 'bus',   'London Gatwick',    'London St Pancras', '06:00:00+00:00', '06:00:00+00:00', null,       '03:30:00',  6.75),\n    (8, 'bus',   'London Gatwick',    'London St Pancras', '20:00:00+00:00', '20:00:00+00:00', null,       '03:30:00',  6.75)\n;\n\ncreate table routes_timetable (\n    route_id           int primary key,\n    mode_of_transport  varchar not null,\n    from_location      varchar not null,\n    to_location        varchar not null,\n    departure_datetime timestamptz not null,\n    arrival_datetime   timestamptz not null,\n    cost               decimal(8, 2) not null,\n);\ninsert into routes_timetable\nvalues\n    (1,  'boat',  'London St Pancras', 'Paris',          '2024-01-01 06:00:00+00:00', '2024-01-01 07:30:00+01:00',  45.00),\n    (2,  'plane', 'London Gatwick',    'New York',       '2024-01-01 13:05:00+00:00', '2024-01-01 20:55:00-05:00', 158.00),\n    (3,  'plane', 'London Gatwick',    'New York',       '2024-01-02 20:40:00+00:00', '2024-01-03 04:30:00-05:00', 147.00),\n    (4,  'plane', 'London St Pancras', 'Paris',          '2024-01-03 07:00:00+00:00', '2024-01-03 08:30:00+01:00',  70.00),\n    (5,  'plane', 'Paris',             'New York',       '2024-01-02 12:00:00+01:00', '2024-01-02 20:30:00-05:00', 180.00),\n    (6,  'plane', 'New York',          'London Gatwick', '2024-01-01 13:00:00-05:00', '2024-01-02 05:45:00+00:00', 160.00),\n    (7,  'boat',  'New York',          'London Gatwick', '2024-01-01 05:30:00-05:00', '2024-01-01 23:00:00+00:00', 195.00),\n    (8,  'boat',  'London St Pancras', 'Paris',          '2024-01-01 18:00:00+00:00', '2024-01-01 19:30:00+01:00',  95.00),\n    (9,  'boat',  'London St Pancras', 'Paris',          '2024-01-02 14:00:00+00:00', '2024-01-02 15:30:00+01:00',  40.00),\n    (10, 'plane', 'New York',          'Paris',          '2024-01-01 18:00:00-05:00', '2024-01-02 17:45:00+01:00', 279.00)\n;\n</code></pre> <p>The solution can be found at:</p> <ul> <li>travel-plans.md</li> </ul> Sample input <p>Routes Schedule</p> schedule_id mode_of_transport from_location to_location earliest_departure latest_departure frequency duration cost 1 train London Gatwick London St Pancras 09:00:00 +00:00 21:00:00 +00:00 01:00:00 00:30:00 12.25 2 bus London Gatwick London St Pancras 06:00:00 +00:00 06:00:00 +00:00 null 03:30:00 8.50 <p>Routes Timetable</p> route_id mode_of_transport from_location to_location departure_datetime arrival_datetime cost 1 boat New York London Gatwick 2024-01-01T09:30Z 2024-01-01T22:00Z 179.00 2 plane New York London Gatwick 2024-01-01T23:00Z 2024-01-02T10:45Z 125.00 3 boat London St Pancras Paris 2024-01-02T13:00Z 2024-01-02T13:30Z 75.00 <pre><code>with\n\nroutes_schedule(\n    schedule_id,\n    mode_of_transport,\n    from_location,\n    to_location,\n    earliest_departure,\n    latest_departure,\n    frequency,\n    duration,\n    cost\n) as (\n    values\n        (1, 'train', 'London Gatwick', 'London St Pancras', '09:00:00+00:00'::timetz, '21:00:00+00:00'::timetz, '01:00:00'::time, '00:30:00'::time, 12.25),\n        (2, 'bus',   'London Gatwick', 'London St Pancras', '06:00:00+00:00'::timetz, '06:00:00+00:00'::timetz, null::time,       '03:30:00'::time,  8.50)\n),\n\nroutes_timetable(\n    route_id,\n    mode_of_transport,\n    from_location,\n    to_location,\n    departure_datetime,\n    arrival_datetime,\n    cost\n) as (\n    values\n        (1, 'boat',  'New York',          'London Gatwick', '2024-01-01 04:30:00-05:00'::timestamptz, '2024-01-01 22:00:00+00:00'::timestamptz, 179.00),\n        (2, 'plane', 'New York',          'London Gatwick', '2024-01-01 18:00:00-05:00'::timestamptz, '2024-01-02 10:45:00+00:00'::timestamptz, 125.00),\n        (3, 'boat',  'London St Pancras', 'Paris',          '2024-01-02 13:00:00+00:00'::timestamptz, '2024-01-02 14:30:00+01:00'::timestamptz,  75.00)\n)\n</code></pre> Sample output route departure_datetime_utc arrival_datetime_utc duration cost New York - London Gatwick - London St Pancras - Paris 2024-01-01 23:00:00 2024-01-02 13:30:00 14:30:00 212.25 <pre><code>solution(route, departure_datetime_utc, arrival_datetime_utc, duration, cost) as (\n    values\n        ('New York - London Gatwick - London St Pancras - Paris', '2024-01-01 23:00:00'::timestamp, '2024-01-02 13:30:00'::timestamp, '14:30:00', 212.25);\n)\n</code></pre> Hint 1 <p>Expand the <code>routes_schedule</code> table into individual routes and then union with the <code>routes_timetable</code> table for a full list of routes to consider.</p> Hint 2 <p>Use a recursive CTE to build up the journey from New York to Paris, considering the interchange time between routes.</p>","tags":["recursive CTE"]},{"location":"challenging-sql-problems/problems/silver/bannable-login-activity/","title":"Bannable login activity \u274c","text":"<p>Scenario</p> <p>The same company from the suspicious login activity problem have decided to take a more proactive approach to their security.</p> <p>If a user has at least 3 consecutive <code>login failed</code> attempts in a day for 3 consecutive days, they are automatically banned.</p> <p>They may have other events between the consecutive failed login attempts.</p> <p>Question</p> <p>Determine the users who should be banned based on the above criteria.</p> <p>The output should have a row for each user who meets this criterion, with the columns:</p> <ul> <li><code>user_id</code></li> <li><code>ban_date</code> as the date of the third day of consecutive failed login attempts</li> </ul> <p>Order the output by <code>user_id</code>.</p> Expand for the DDL <pre><code>create table events (\n    event_id       integer primary key,\n    user_id        integer not null,\n    event_datetime timestamp not null,\n    event_type     varchar not null\n);\ninsert into events\nvalues\n    (1,  1, '2024-01-01 11:00:00', 'login'),\n    (2,  1, '2024-01-01 12:00:00', 'logout'),\n    (3,  1, '2024-01-03 03:00:00', 'login failed'),\n    (4,  1, '2024-01-03 03:01:00', 'login failed'),\n    (5,  1, '2024-01-03 03:02:00', 'login failed'),\n    (6,  1, '2024-01-03 03:05:00', 'login'),\n    (7,  2, '2024-01-03 10:00:00', 'login'),\n    (8,  2, '2024-01-03 15:00:00', 'logout'),\n    (9,  1, '2024-01-03 23:00:00', 'logout'),\n    (10, 2, '2024-01-04 22:00:00', 'login failed'),\n    (11, 2, '2024-01-04 22:05:00', 'login'),\n    (12, 3, '2024-01-05 20:00:00', 'login'),\n    (13, 3, '2024-01-06 04:00:00', 'logout'),\n    (14, 2, '2024-01-09 15:00:00', 'logout'),\n    (15, 3, '2024-01-11 21:00:00', 'login'),\n    (16, 1, '2024-01-12 12:00:00', 'login failed'),\n    (17, 1, '2024-01-12 13:00:00', 'login failed'),\n    (18, 1, '2024-01-12 23:00:00', 'login failed'),\n    (19, 2, '2024-01-13 10:00:00', 'login failed'),\n    (20, 2, '2024-01-13 10:05:00', 'login'),\n    (21, 2, '2024-01-13 15:00:00', 'logout'),\n    (22, 1, '2024-01-13 23:00:00', 'login failed'),\n    (23, 1, '2024-01-13 23:01:00', 'login failed'),\n    (24, 1, '2024-01-13 23:02:00', 'login failed'),\n    (25, 2, '2024-01-14 22:00:00', 'login'),\n    (26, 3, '2024-01-15 20:00:00', 'login'),\n    (27, 3, '2024-01-16 04:00:00', 'logout'),\n    (28, 2, '2024-01-19 15:00:00', 'logout'),\n    (29, 3, '2024-01-21 21:00:00', 'login'),\n    (30, 1, '2024-01-22 12:00:00', 'login failed'),\n    (31, 1, '2024-01-22 12:05:00', 'password reset'),\n    (32, 1, '2024-01-22 12:10:00', 'login'),\n    (33, 1, '2024-01-22 13:00:00', 'logout'),\n    (34, 1, '2024-01-23 03:00:00', 'login'),\n    (35, 2, '2024-01-23 10:00:00', 'login'),\n    (36, 2, '2024-01-23 15:00:00', 'logout'),\n    (37, 1, '2024-01-23 23:00:00', 'logout'),\n    (38, 2, '2024-01-24 22:00:00', 'login'),\n    (39, 3, '2024-01-25 20:00:00', 'login'),\n    (40, 3, '2024-01-26 04:00:00', 'logout'),\n    (41, 2, '2024-01-29 15:00:00', 'logout'),\n    (42, 3, '2024-01-30 21:00:00', 'login failed'),\n    (43, 3, '2024-01-30 21:01:00', 'login failed'),\n    (44, 3, '2024-01-30 21:02:00', 'login failed'),\n    (45, 3, '2024-01-30 21:03:00', 'login failed'),\n    (46, 3, '2024-01-30 21:04:00', 'login failed'),\n    (47, 3, '2024-01-30 21:05:00', 'password reset'),\n    (48, 3, '2024-01-30 21:06:00', 'password reset'),\n    (49, 3, '2024-01-30 21:07:00', 'password reset'),\n    (50, 3, '2024-01-30 21:08:00', 'password reset'),\n    (51, 3, '2024-01-30 21:09:00', 'password reset'),\n    (52, 3, '2024-01-30 21:10:00', 'password reset'),\n    (53, 3, '2024-01-31 23:55:00', 'login failed'),\n    (54, 3, '2024-01-31 23:56:00', 'login failed'),\n    (55, 3, '2024-01-31 23:57:00', 'login failed'),\n    (56, 3, '2024-01-31 23:58:00', 'login failed'),\n    (57, 3, '2024-01-31 23:59:00', 'login failed'),\n    (58, 3, '2024-02-01 00:00:00', 'login failed'),\n    (59, 3, '2024-02-01 00:01:00', 'login failed'),\n    (60, 3, '2024-02-01 00:02:00', 'login failed')\n;\n</code></pre> <p>The solution can be found at:</p> <ul> <li>bannable-login-activity.md</li> </ul> Sample input event_id user_id event_datetime event_type 1 1 2024-01-01 03:00:00 login failed 2 1 2024-01-01 03:01:00 login failed 3 1 2024-01-01 03:02:00 login failed 4 1 2024-01-01 11:00:00 login 5 1 2024-01-01 12:00:00 logout 6 2 2024-01-01 15:00:00 login 7 2 2024-01-01 18:00:00 logout 8 1 2024-01-02 03:00:00 login failed 9 1 2024-01-02 03:01:00 login failed 10 1 2024-01-02 03:02:00 login failed 11 1 2024-01-03 03:00:00 login failed 12 1 2024-01-03 03:01:00 login failed 13 1 2024-01-03 03:02:00 login failed <pre><code>with events(event_id, user_id, event_datetime, event_type) as (\n    values\n        (1,  1, '2024-01-01 03:00:00'::timestamp, 'login failed'),\n        (2,  1, '2024-01-01 03:01:00'::timestamp, 'login failed'),\n        (3,  1, '2024-01-01 03:02:00'::timestamp, 'login failed'),\n        (4,  1, '2024-01-01 11:00:00'::timestamp, 'login'),\n        (5,  1, '2024-01-01 12:00:00'::timestamp, 'logout'),\n        (6,  2, '2024-01-01 15:00:00'::timestamp, 'login'),\n        (7,  2, '2024-01-01 18:00:00'::timestamp, 'logout'),\n        (8,  1, '2024-01-02 03:00:00'::timestamp, 'login failed'),\n        (9,  1, '2024-01-02 03:01:00'::timestamp, 'login failed'),\n        (10, 1, '2024-01-02 03:02:00'::timestamp, 'login failed'),\n        (11, 1, '2024-01-03 03:00:00'::timestamp, 'login failed'),\n        (12, 1, '2024-01-03 03:01:00'::timestamp, 'login failed'),\n        (13, 1, '2024-01-03 03:02:00'::timestamp, 'login failed')\n)\n</code></pre> Sample output user_id ban_date 1 2024-01-03 <pre><code>solution(user_id, ban_date) as (\n    values\n        (1, '2024-01-03'::date)\n)\n</code></pre> Hint 1 <p>Like Suspicious login activity, use window functions to determine the sets of consecutive events.</p> Hint 2 <p>For databases that support it, use <code>RANGE INTERVAL '3 DAYS' PRECEDING</code> in a window function to summarise the previous three days.</p> <p>For databases that don't support it, use whatever method you prefer to determine the consecutive days -- for example:</p> <ul> <li>Construct a complete date axis to use <code>ROWS BETWEEN 2 PRECEDING AND CURRENT ROW</code> to summarise the previous three days.</li> <li>Use an inner join to the same table to summarise events from the previous three days.</li> <li>Use a correlated subquery to summarise events from the previous three days.</li> </ul>","tags":["window function","gaps and islands"]},{"location":"challenging-sql-problems/problems/silver/bus-routes/","title":"Bus routes \ud83d\ude8c","text":"<p>Scenario</p> <p>A transport company records their bus routes in a table where each row represents a leg of a journey, but they want to know the full route for each bus.</p> <p>Question</p> <p>Compute the full route for each bus.</p> <p>Use the following stops as the first stop for each bus:</p> <ul> <li><code>Old Street</code> for <code>bus_id = 1</code></li> <li><code>Hillside</code> for <code>bus_id = 2</code></li> <li><code>Birch Park</code> for <code>bus_id = 3</code></li> </ul> <p>The output should have a single row per bus with the columns:</p> <ul> <li><code>bus_id</code></li> <li><code>route</code> which is each bus stop in the route separated by a hyphen, e.g. <code>First Stop - Middle Stop - Final Stop</code></li> </ul> <p>Order the output by <code>bus_id</code>.</p> Expand for the DDL <pre><code>create table bus_stops (\n    bus_id    int,\n    from_stop varchar,\n    to_stop   varchar,\n    primary key (bus_id, from_stop)\n);\ninsert into bus_stops\nvalues\n    (1, 'Bakers March',   'West Quay Stop'),\n    (3, 'Birch Park',     'Farfair'),\n    (1, 'Cavendish Road', 'Bakers March'),\n    (3, 'Cavendish Road', 'Birch Park'),\n    (1, 'Crown Street',   'Leather Lane'),\n    (3, 'Farfair',        'Golden Lane'),\n    (2, 'Fellows Road',   'Riverside'),\n    (2, 'Furlong Reach',  'Hillside'),\n    (3, 'Golden Lane',    'Goose Green'),\n    (1, 'Goose Green',    'Crown Street'),\n    (3, 'Goose Green',    'Sailors Rest'),\n    (2, 'Hillside',       'Fellows Road'),\n    (2, 'Laddersmith',    'Furlong Reach'),\n    (1, 'Leather Lane',   'Old Street'),\n    (1, 'Old Street',     'Cavendish Road'),\n    (2, 'Riverside',      'Laddersmith'),\n    (3, 'Sailors Rest',   'Cavendish Road'),\n    (1, 'West Quay Stop', 'Goose Green')\n;\n</code></pre> <p>The solution can be found at:</p> <ul> <li>bus-routes.md</li> </ul> Sample input <p>Compute the full route for each bus, using the following stops as the first stop for each bus:</p> <ul> <li><code>Stop A</code> for <code>bus_id = 1</code></li> <li><code>First Stop</code> for <code>bus_id = 2</code></li> </ul> bus_id from_stop to_stop 1 Stop A Stop B 1 Stop B Stop C 1 Stop C Stop A 2 First Street Second Street 2 Second Street Third Street 2 Third Street First Street <pre><code>with bus_stops(bus_id, from_stop, to_stop) as (\n    values\n        (1, 'Stop A',        'Stop B'),\n        (1, 'Stop B',        'Stop C'),\n        (1, 'Stop C',        'Stop A'),\n        (2, 'First Street',  'Second Street'),\n        (2, 'Second Street', 'Third Street'),\n        (2, 'Third Street',  'First Street')\n)\n</code></pre> Sample output bus_id route 1 Stop A - Stop B - Stop C 2 First Street - Second Street - Third Street <pre><code>solution(bus_id, route) as (\n    values\n        (1, 'Stop A - Stop B - Stop C'),\n        (2, 'First Street - Second Street - Third Street');\n)\n</code></pre> Hint 1 <p>Use a recursive CTE to put the bus stops in order for each bus.</p> Hint 2 <p>These routes are cyclical, so include a condition to stop the recursion when you reach the first stop for each bus again.</p>","tags":["recursive CTE"]},{"location":"challenging-sql-problems/problems/silver/customer-sales-running-totals/","title":"Customer sales running totals \ud83d\udcc8","text":"<p>Scenario</p> <p>A retail company is interested in understanding the sales performance of some specific customers throughout June 2014.</p> <p>Question</p> <p>Using only the <code>Sales.SalesOrderHeader</code> table in the AdventureWorks database, calculate the running total of <code>TotalDue</code> per customer for the customers with <code>CustomerID</code> values of <code>11176</code>, <code>11091</code>, and <code>11287</code>.</p> <p>The output should have a row per customer for each day in June 2014, but the running totals should include all the historic sales for the customers.</p> <p>The output should have 90 rows (30 days in June for 3 customers) and the columns:</p> <ul> <li><code>BalanceDate</code> as the date that the end-of-day balance corresponds to</li> <li><code>CustomerID</code></li> <li><code>RunningTotal</code> as the sum of the <code>TotalDue</code> values up to and including the <code>BalanceDate</code></li> </ul> <p>Order the output by <code>BalanceDate</code> and <code>CustomerID</code>.</p> <p>Note</p> <p>You can access this table on the db&lt;&gt;fiddle website at:</p> <ul> <li>https://dbfiddle.uk/8VEWSCRd</li> </ul> <p>Since the rows corresponding to 2014-06-01 should include the historic sales, the rows for 2014-06-01 should be:</p> BalanceDate CustomerID RunningTotal 2014-06-01 11091 1243.5234 2014-06-01 11176 1222.8820 2014-06-01 11287 1115.2109 <p>However, you should calculate this yourself (don't copy the above values).</p> <p>The solution can be found at:</p> <ul> <li>customer-sales-running-totals.md</li> </ul> Sample input CustomerID OrderDate TotalDue 1 2014-03-14 00:00:00 23153.2339 1 2014-04-16 00:00:00 1457.3288 1 2014-04-21 00:00:00 36865.8012 1 2014-05-12 00:00:00 32474.9324 1 2014-05-04 00:00:00 472.3108 1 2014-06-03 00:00:00 27510.4109 1 2014-06-08 00:00:00 16158.6961 1 2014-06-16 00:00:00 5694.8564 1 2014-06-21 00:00:00 6876.3649 1 2014-06-28 00:00:00 40487.7233 <pre><code>with SalesOrderHeader(CustomerID, OrderDate, TotalDue) as (\n    values\n        (1, '2014-03-14 00:00:00.000', 23153.2339),\n        (1, '2014-04-16 00:00:00.000',  1457.3288),\n        (1, '2014-04-21 00:00:00.000', 36865.8012),\n        (1, '2014-05-12 00:00:00.000', 32474.9324),\n        (1, '2014-05-04 00:00:00.000',   472.3108),\n        (1, '2014-06-03 00:00:00.000', 27510.4109),\n        (1, '2014-06-08 00:00:00.000', 16158.6961),\n        (1, '2014-06-16 00:00:00.000',  5694.8564),\n        (1, '2014-06-21 00:00:00.000',  6876.3649),\n        (1, '2014-06-28 00:00:00.000', 40487.7233)\n)\n</code></pre> Sample output BalanceDate CustomerID RunningTotal 2014-06-01 1 94423.6071 2014-06-02 1 94423.6071 2014-06-03 1 121934.0180 2014-06-04 1 121934.0180 2014-06-05 1 121934.0180 2014-06-06 1 121934.0180 2014-06-07 1 121934.0180 2014-06-08 1 138092.7141 2014-06-09 1 138092.7141 2014-06-10 1 138092.7141 2014-06-11 1 138092.7141 2014-06-12 1 138092.7141 2014-06-13 1 138092.7141 2014-06-14 1 138092.7141 2014-06-15 1 138092.7141 2014-06-16 1 143787.5705 2014-06-17 1 143787.5705 2014-06-18 1 143787.5705 2014-06-19 1 143787.5705 2014-06-20 1 143787.5705 2014-06-21 1 150663.9354 2014-06-22 1 150663.9354 2014-06-23 1 150663.9354 2014-06-24 1 150663.9354 2014-06-25 1 150663.9354 2014-06-26 1 150663.9354 2014-06-27 1 150663.9354 2014-06-28 1 191151.6587 2014-06-29 1 191151.6587 2014-06-30 1 191151.6587 <pre><code>with solution(BalanceDate, CustomerID, RunningTotal) as (\n    values\n        ('2014-06-01', 1,  94423.6071),\n        ('2014-06-02', 1,  94423.6071),\n        ('2014-06-03', 1, 121934.0180),\n        ('2014-06-04', 1, 121934.0180),\n        ('2014-06-05', 1, 121934.0180),\n        ('2014-06-06', 1, 121934.0180),\n        ('2014-06-07', 1, 121934.0180),\n        ('2014-06-08', 1, 138092.7141),\n        ('2014-06-09', 1, 138092.7141),\n        ('2014-06-10', 1, 138092.7141),\n        ('2014-06-11', 1, 138092.7141),\n        ('2014-06-12', 1, 138092.7141),\n        ('2014-06-13', 1, 138092.7141),\n        ('2014-06-14', 1, 138092.7141),\n        ('2014-06-15', 1, 138092.7141),\n        ('2014-06-16', 1, 143787.5705),\n        ('2014-06-17', 1, 143787.5705),\n        ('2014-06-18', 1, 143787.5705),\n        ('2014-06-19', 1, 143787.5705),\n        ('2014-06-20', 1, 143787.5705),\n        ('2014-06-21', 1, 150663.9354),\n        ('2014-06-22', 1, 150663.9354),\n        ('2014-06-23', 1, 150663.9354),\n        ('2014-06-24', 1, 150663.9354),\n        ('2014-06-25', 1, 150663.9354),\n        ('2014-06-26', 1, 150663.9354),\n        ('2014-06-27', 1, 150663.9354),\n        ('2014-06-28', 1, 191151.6587),\n        ('2014-06-29', 1, 191151.6587),\n        ('2014-06-30', 1, 191151.6587);\n)\n</code></pre> Hint 1 <p>Use a recursive CTE (or equivalent) to generate the June 2014 date axis, and then join the customers' sales to it.</p> Hint 2 <p>Use the <code>SUM</code> function with the <code>OVER</code> clause to calculate the running total.</p>","tags":["window function"]},{"location":"challenging-sql-problems/problems/silver/decoding-datelist-ints/","title":"Decoding datelist ints \ud83d\udd13","text":"<p>Scenario</p> <p>The same social media platform from the customer churn problem wants to view their user activity in a more human-readable format.</p> <p>Question</p> <p>Explode the user activity into a table with one row per day and a column for each user, showing whether they were active.</p> <p>The number of rows to show in the output should be the number of days in the <code>activity_history</code> column for the user with the most days.</p> <p>Just like in the customer churn problem, the <code>last_update</code> column will always have the same date for all users.</p> <p>The output should have a single row per day with the columns:</p> <ul> <li><code>active_date</code> as the date of the activity, starting from the <code>last_update</code> date</li> <li><code>user_X</code> which is <code>1</code> if <code>user_id = X</code> was active on that day, <code>0</code> otherwise</li> </ul> <p>...where <code>X</code> is each <code>user_id</code> in the activity table.</p> <p>Order the output by <code>active_date</code>.</p> Expand for the DDL <pre><code>create table user_history (\n    user_id          int primary key,\n    last_update      date not null,\n    activity_history bigint not null,\n);\ninsert into user_history\nvalues\n    (1, '2024-06-01',   1056256),\n    (2, '2024-06-01', 907289368),\n    (3, '2024-06-01', 201335032),\n    (4, '2024-06-01',   9769312),\n    (5, '2024-06-01', 246247510),\n    (6, '2024-06-01', 492660983)\n;\n</code></pre> <p>The solution can be found at:</p> <ul> <li>decoding-datelist-ints.md</li> </ul> Sample input user_id last_update activity_history 1 2024-03-01 81 2 2024-03-01 2688 3 2024-03-01 13144 <pre><code>with user_history(user_id, last_update, activity_history) as (\n    values\n        (1, '2024-03-01'::date,    81),\n        (2, '2024-03-01'::date,  2688),\n        (3, '2024-03-01'::date, 13144)\n)\n</code></pre> Sample output active_date user_1 user_2 user_3 2024-02-17 0 0 1 2024-02-18 0 0 1 2024-02-19 0 1 0 2024-02-20 0 0 0 2024-02-21 0 1 1 2024-02-22 0 0 1 2024-02-23 0 1 0 2024-02-24 1 0 1 2024-02-25 0 0 0 2024-02-26 1 0 1 2024-02-27 0 0 1 2024-02-28 0 0 0 2024-02-29 0 0 0 2024-03-01 1 0 0 <pre><code>solution(active_date, user_1, user_2, user_3) as (\n    values\n        ('2024-02-17'::date, 0, 0, 1),\n        ('2024-02-18'::date, 0, 0, 1),\n        ('2024-02-19'::date, 0, 1, 0),\n        ('2024-02-20'::date, 0, 0, 0),\n        ('2024-02-21'::date, 0, 1, 1),\n        ('2024-02-22'::date, 0, 0, 1),\n        ('2024-02-23'::date, 0, 1, 0),\n        ('2024-02-24'::date, 1, 0, 1),\n        ('2024-02-25'::date, 0, 0, 0),\n        ('2024-02-26'::date, 1, 0, 1),\n        ('2024-02-27'::date, 0, 0, 1),\n        ('2024-02-28'::date, 0, 0, 0),\n        ('2024-02-29'::date, 0, 0, 0),\n        ('2024-03-01'::date, 1, 0, 0)\n)\n</code></pre> Hint 1 <p>Use a recursive CTE to construct a table with the dates.</p> Hint 2 <p>Use the \"bitwise and\" operation to determine if a user was active on a given day.</p>","tags":["recursive CTE","bitshift"]},{"location":"challenging-sql-problems/problems/silver/funnel-analytics/","title":"Funnel analytics \u23ec","text":"<p>Scenario</p> <p>A bank is interested in understanding the conversion rates of their mortgage application process.</p> <p>Their application funnel consists of the following stages, in order:</p> <ol> <li>Full application</li> <li>Decision</li> <li>Documentation</li> <li>Valuation inspection</li> <li>Valuation made</li> <li>Valuation submitted</li> <li>Solicitation</li> <li>Funds released</li> </ol> <p>Question</p> <p>The table <code>applications</code> tracks the dates that each mortgage application reached each stage.</p> <p>Calculate the conversion rates between each stage for each cohort (defined below).</p> <p>The output should have a row per cohort and stage, with the columns:</p> <ul> <li><code>cohort</code> as the month that the applications were started; e.g., an application started on <code>2024-01-15</code> would be cohort <code>2024-01</code>.</li> <li><code>stage</code></li> <li><code>mortgages</code> as the number of mortgages that reached the stage</li> <li><code>step_rate</code> as the percentage of mortgages that reached the stage compared to the previous stage</li> <li><code>total_rate</code> as the percentage of mortgages that reached the stage compared to the first stage</li> </ul> <p>Round the <code>step_rate</code> and <code>total_rate</code> to two decimal places.</p> <p>Note that each cohort should have all the stages, even if there are no mortgages that reached that stage -- the <code>mortgages</code> column should be <code>0</code> in that case.</p> <p>Order the output by <code>cohort</code> and the <code>stage</code> order (e.g. <code>full application</code> should come before <code>decision</code>, and so on).</p> Expand for the DDL <pre><code>create table applications (\n    event_id    int primary key,\n    event_date  date not null,\n    mortgage_id int not null,\n    stage       varchar not null\n);\ninsert into applications\nvalues\n    (1,  '2024-01-02', 1,  'full application'),\n    (2,  '2024-01-06', 1,  'decision'),\n    (3,  '2024-01-12', 1,  'documentation'),\n    (4,  '2024-01-14', 1,  'valuation inspection'),\n    (5,  '2024-01-16', 2,  'full application'),\n    (6,  '2024-01-17', 3,  'full application'),\n    (7,  '2024-01-19', 2,  'decision'),\n    (8,  '2024-01-25', 2,  'documentation'),\n    (9,  '2024-01-27', 1,  'valuation made'),\n    (10, '2024-01-27', 4,  'full application'),\n    (11, '2024-01-29', 3,  'decision'),\n    (12, '2024-02-02', 1,  'valuation submitted'),\n    (13, '2024-02-03', 4,  'decision'),\n    (14, '2024-02-04', 5,  'full application'),\n    (15, '2024-02-05', 4,  'documentation'),\n    (16, '2024-02-06', 4,  'valuation inspection'),\n    (17, '2024-02-09', 6,  'full application'),\n    (18, '2024-02-11', 5,  'decision'),\n    (19, '2024-02-11', 7,  'full application'),\n    (20, '2024-02-12', 2,  'valuation inspection'),\n    (21, '2024-02-12', 6,  'decision'),\n    (22, '2024-02-12', 7,  'decision'),\n    (23, '2024-02-13', 2,  'valuation made'),\n    (24, '2024-02-13', 6,  'documentation'),\n    (25, '2024-02-14', 2,  'valuation submitted'),\n    (26, '2024-02-15', 6,  'valuation inspection'),\n    (27, '2024-02-16', 4,  'valuation made'),\n    (28, '2024-02-17', 5,  'documentation'),\n    (29, '2024-02-19', 4,  'valuation submitted'),\n    (30, '2024-02-20', 5,  'valuation inspection'),\n    (31, '2024-02-21', 8,  'full application'),\n    (32, '2024-02-22', 5,  'valuation made'),\n    (33, '2024-02-23', 9,  'full application'),\n    (34, '2024-02-25', 6,  'valuation made'),\n    (35, '2024-02-27', 5,  'valuation submitted'),\n    (36, '2024-02-27', 6,  'valuation submitted'),\n    (37, '2024-02-29', 9,  'decision'),\n    (38, '2024-02-29', 10, 'full application'),\n    (39, '2024-03-01', 9,  'documentation'),\n    (40, '2024-03-02', 8,  'decision'),\n    (41, '2024-03-05', 11, 'full application'),\n    (42, '2024-03-07', 9,  'valuation inspection'),\n    (43, '2024-03-07', 12, 'full application'),\n    (44, '2024-03-08', 9,  'valuation made'),\n    (45, '2024-03-08', 13, 'full application'),\n    (46, '2024-03-10', 10, 'decision'),\n    (47, '2024-03-12', 12, 'decision'),\n    (48, '2024-03-15', 10, 'documentation'),\n    (49, '2024-03-15', 13, 'decision'),\n    (50, '2024-03-16', 11, 'decision'),\n    (51, '2024-03-17', 13, 'documentation'),\n    (52, '2024-03-18', 9,  'valuation submitted'),\n    (53, '2024-03-18', 10, 'valuation inspection'),\n    (54, '2024-03-20', 13, 'valuation inspection'),\n    (55, '2024-03-21', 10, 'valuation made'),\n    (56, '2024-03-22', 13, 'valuation made'),\n    (57, '2024-03-27', 10, 'valuation submitted'),\n    (58, '2024-03-28', 13, 'valuation submitted'),\n    (59, '2024-04-12', 6,  'solicitation'),\n    (60, '2024-04-17', 6,  'funds released'),\n    (61, '2024-04-26', 1,  'solicitation'),\n    (62, '2024-05-02', 1,  'funds released'),\n    (63, '2024-05-17', 5,  'solicitation'),\n    (64, '2024-05-28', 5,  'funds released'),\n    (65, '2024-06-03', 9,  'solicitation'),\n    (66, '2024-06-04', 9,  'funds released')\n;\n</code></pre> <p>The solution can be found at:</p> <ul> <li>funnel-analytics.md</li> </ul> Sample input event_id event_date mortgage_id stage 1 2024-01-02 1 full application 2 2024-01-06 1 decision 3 2024-01-12 1 documentation 4 2024-01-14 1 valuation inspection 5 2024-01-27 1 valuation made 6 2024-02-02 1 valuation submitted 7 2024-04-26 1 solicitation <pre><code>with applications(event_id, event_date, mortgage_id, stage) as (\n    values\n        (1, '2024-01-02'::date, 1, 'full application'),\n        (2, '2024-01-06'::date, 1, 'decision'),\n        (3, '2024-01-12'::date, 1, 'documentation'),\n        (4, '2024-01-14'::date, 1, 'valuation inspection'),\n        (5, '2024-01-27'::date, 1, 'valuation made'),\n        (6, '2024-02-02'::date, 1, 'valuation submitted'),\n        (7, '2024-04-26'::date, 1, 'solicitation')\n)\n</code></pre> Sample output cohort stage mortgages step_rate total_rate 2024-01 full application 1 100.00 100.00 2024-01 decision 1 100.00 100.00 2024-01 documentation 1 100.00 100.00 2024-01 valuation inspection 1 100.00 100.00 2024-01 valuation made 1 100.00 100.00 2024-01 valuation submitted 1 100.00 100.00 2024-01 solicitation 1 100.00 100.00 2024-01 funds released 0 0.00 0.00 <pre><code>solution(cohort, stage, mortgages, step_rate, total_rate) as (\n    values\n        ('2024-01', 'full application',     1, 100.00, 100.00),\n        ('2024-01', 'decision',             1, 100.00, 100.00),\n        ('2024-01', 'documentation',        1, 100.00, 100.00),\n        ('2024-01', 'valuation inspection', 1, 100.00, 100.00),\n        ('2024-01', 'valuation made',       1, 100.00, 100.00),\n        ('2024-01', 'valuation submitted',  1, 100.00, 100.00),\n        ('2024-01', 'solicitation',         1, 100.00, 100.00),\n        ('2024-01', 'funds released',       0,   0.00,   0.00)\n)\n</code></pre> Hint 1 <p>Determine each row's cohort before calculating the rates.</p> Hint 2 <p>Use window functions to compare the current row with the historic rows.</p>","tags":["custom axis"]},{"location":"challenging-sql-problems/problems/silver/mandelbrot-set/","title":"Mandelbrot set \ud83c\udf00","text":"<p>Info</p> <p>This is just for fun, there's no real-world application for this (as far as I know) \ud83d\ude1d</p> <p>Question</p> <p>Plot an image of the Mandelbrot set.</p> <p>Start with a 51x51 grid of points ranging from -2 to 2 on both the x and y axes. A sample of some points in the grid are:</p> (-2, 2) (-1.92, 2) ... (1.92, 2) (2, 2) (-2, 1.92) (-1.92, 1.92) ... (1.92, 1.92) (2, 1.92) ... ... (0, 0) ... ... (-2, -1.92) (-1.92, -1.92) ... (1.92, -1.92) (2, -1.92) (-2, -2) (-1.92, -2) ... (1.92, -2) (2, -2) <p>Apply the Mandelbrot set formula to each point in the grid for 100 iterations. If the point remains inside this grid after 100 iterations, mark it with a <code>\u2022</code>. If it diverges, mark it with a space.</p> <p>Once you know which points are in the set, return a single cell with a string representation of the grid.</p> <p>For example, an output might look like the following string:</p> <pre><code>          \u2022\n         \u2022\n         \u2022\u2022\n       \u2022\u2022\u2022\u2022\u2022\n     \u2022 \u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n     \u2022 \u2022\u2022\u2022\u2022\u2022\n       \u2022\u2022\u2022\u2022\u2022\n         \u2022\u2022\n         \u2022\n          \u2022\n</code></pre> <p>The Mandelbrot set is a set of complex numbers <code>c</code> for which the function <code>f(z) = z^2 + c</code> does not diverge when iterated from <code>z = 0</code>.</p> <p>We can plot the Mandelbrot set by considering the complex plane as a grid of coordinates. Given two points <code>(a, b)</code> and <code>(c, d)</code>, we can define addition and multiplication as:</p> <ul> <li>Addition: <code>(a, b) + (c, d) = (a + c, b + d)</code></li> <li>Multiplication: <code>(a, b) * (c, d) = (a * c - b * d, a * d + b * c)</code></li> </ul> <p>Note that, by considering the complex plane as a grid of coordinates, the complex number <code>0</code> is represented as <code>(0, 0)</code>.</p> <p>The solution can be found at:</p> <ul> <li>mandelbrot-set.md</li> </ul> <p>A worked example is provided below to help illustrate the Mandelbrot set calculations.</p> Sample input <p>Plot an image of the Mandelbrot set on a 21x21 grid.</p> Sample output <pre><code>          \u2022\n         \u2022\n         \u2022\u2022\n       \u2022\u2022\u2022\u2022\u2022\n     \u2022 \u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n     \u2022 \u2022\u2022\u2022\u2022\u2022\n       \u2022\u2022\u2022\u2022\u2022\n         \u2022\u2022\n         \u2022\n          \u2022\n</code></pre> <pre><code>solution(mandelbrot_set) as (\n    select concat_ws(e'\\n',\n        '                     ',\n        '                     ',\n        '                     ',\n        '                     ',\n        '                     ',\n        '          \u2022          ',\n        '         \u2022           ',\n        '         \u2022\u2022          ',\n        '       \u2022\u2022\u2022\u2022\u2022         ',\n        '     \u2022 \u2022\u2022\u2022\u2022\u2022         ',\n        '\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022         ',\n        '     \u2022 \u2022\u2022\u2022\u2022\u2022         ',\n        '       \u2022\u2022\u2022\u2022\u2022         ',\n        '         \u2022\u2022          ',\n        '         \u2022           ',\n        '          \u2022          ',\n        '                     ',\n        '                     ',\n        '                     ',\n        '                     ',\n        '                     '\n    )\n)\n</code></pre> Hint 1 <p>Generate the grid of points into a table with columns <code>x</code> and <code>y</code> ranging from -2 to 2 in steps of 0.08.</p> Hint 2 <p>Use a recursive CTE to iterate over the grid points for 100 iterations, applying the Mandelbrot set formula to each point.</p>","tags":["recursive CTE"]},{"location":"challenging-sql-problems/problems/silver/mandelbrot-set/#worked-examples","title":"Worked examples","text":"<p>To help illustrate the Mandelbrot set calculations, consider the following points:</p> <ul> <li>(-2, 2)</li> <li>(-2, 0)</li> <li>(-1, 0)</li> <li>(0, 0)</li> <li>(2, 0)</li> <li>(1.04, 1.04)</li> </ul> <p>Let's walk through the steps for these.</p> <p>We'll use two additional details:</p> <ol> <li>For a complex number <code>z = (a, b)</code>, the square of <code>z</code> is <code>z^2 = (a^2 - b^2, 2ab)</code></li> <li>Any complex number that exceeds -2 or 2 on either axis will diverge</li> </ol>","tags":["recursive CTE"]},{"location":"challenging-sql-problems/problems/silver/mandelbrot-set/#-2-2","title":"(-2, 2)","text":"<p>In the function <code>f(z) = z^2 + c</code>, we start with <code>z = (0, 0)</code> and <code>c = (-2, 2)</code>. The first two iterations are:</p> <ul> <li><code>f((0, 0)) = (0, 0)^2 + (-2, 2) = (-2, 2)</code></li> <li><code>f((-2, 2)) = (-2, 2)^2 + (-2, 2) = (4 - 4, 2 * -2 * 2) + (-2, 2) = (0, -8) + (-2, 2) = (-2, -6)</code></li> </ul> <p>After the second iteration, the resulting point is outside the grid, so it will diverge; therefore, the point <code>(-2, 2)</code> is not in the set.</p>","tags":["recursive CTE"]},{"location":"challenging-sql-problems/problems/silver/mandelbrot-set/#-2-0","title":"(-2, 0)","text":"<p>In the function <code>f(z) = z^2 + c</code>, we start with <code>z = (0, 0)</code> and <code>c = (-2, 0)</code>. The first few iterations are:</p> <ul> <li><code>f((0, 0)) = (0, 0)^2 + (-2, 0) = (-2, 0)</code></li> <li><code>f((-2, 0)) = (-2, 0)^2 + (-2, 0) = (4 - 0, 2 * -2 * 0) + (-2, 0) = (4, 0) + (-2, 0) = (2, 0)</code></li> <li><code>f((2, 0)) = (2, 0)^2 + (-2, 0) = (4 - 0, 2 * 2 * 0) + (-2, 0) = (4, 0) + (-2, 0) = (2, 0)</code></li> </ul> <p>After the second iteration, the resulting point remains the same and within the grid; therefore, the point <code>(-2, 0)</code> is in the set.</p>","tags":["recursive CTE"]},{"location":"challenging-sql-problems/problems/silver/mandelbrot-set/#0-0","title":"(0, 0)","text":"<p>In the function <code>f(z) = z^2 + c</code>, we start with <code>z = (0, 0)</code> and <code>c = (0, 0)</code>. The first iteration is:</p> <ul> <li><code>f((0, 0)) = (0, 0)^2 + (0, 0) = (0, 0)</code></li> </ul> <p>The resulting point remains the same after the first iteration, so it will not diverge; therefore, the point <code>(0, 0)</code> is in the set.</p>","tags":["recursive CTE"]},{"location":"challenging-sql-problems/problems/silver/mandelbrot-set/#2-0","title":"(2, 0)","text":"<p>In the function <code>f(z) = z^2 + c</code>, we start with <code>z = (0, 0)</code> and <code>c = (2, 0)</code>. The first two iterations are:</p> <ul> <li><code>f((0, 0)) = (0, 0)^2 + (2, 0) = (2, 0)</code></li> <li><code>f((2, 0)) = (2, 0)^2 + (2, 0) = (4 - 0, 2 * 2 * 0) + (2, 0) = (4, 0) + (2, 0) = (6, 0)</code></li> </ul> <p>After the second iteration, the resulting point is outside the grid, so it will diverge; therefore, the point <code>(2, 0)</code> is not in the set.</p>","tags":["recursive CTE"]},{"location":"challenging-sql-problems/problems/silver/mandelbrot-set/#104-104","title":"(1.04, 1.04)","text":"<p>In the function <code>f(z) = z^2 + c</code>, we start with <code>z = (0, 0)</code> and <code>c = (1.04, 1.04)</code>. The first two iterations are:</p> <ul> <li><code>f((0, 0)) = (0, 0)^2 + (1.04, 1.04) = (1.04, 1.04)</code></li> <li><code>f((1.04, 1.04))</code><code>= (1.04, 1.04)^2 + (1.04, 1.04)</code><code>= (1.0816 - 1.0816, 2 * 1.04 * 1.04) + (1.04, 1.04)</code><code>= (0, 2.1632) + (1.04, 1.04)</code><code>= (1.04, 3.2032)</code></li> </ul> <p>After the second iteration, the resulting point is outside the grid, so it will diverge; therefore, the point <code>(1.04, 1.04)</code> is not in the set.</p>","tags":["recursive CTE"]},{"location":"challenging-sql-problems/problems/silver/metric-correlation/","title":"Metric correlation \ud83d\udd00","text":"<p>Scenario</p> <p>A company has banded their customers into segments and calculated several metrics for each customer.</p> <p>The company wants to know which pairs of metrics are most correlated within each segment.</p> <p>Question</p> <p>For each customer segment, find the highest correlated pair of metrics.</p> <p>The correlation should be rounded to four decimal places, and the output should keep ties in the (rounded) correlation. Don't compare a metric to itself \ud83d\ude04</p> <p>The output should have a row per segment and metric pair, with the columns:</p> <ul> <li><code>segment</code></li> <li><code>metric_pair</code> as the metric pair in the format <code>metric_1, metric_2</code>. Put the (lexicographically) lower metric name on the left using your database's default collation</li> <li><code>correlation</code> as the correlation between the two metrics, rounded to four decimal places</li> </ul> <p>Order the output by <code>segment</code> and <code>metric_pair</code>.</p> Expand for the DDL <pre><code>create table metrics (\n    customer_id integer primary key,\n    segment     integer not null,\n    metric_1    integer not null,\n    metric_2    integer not null,\n    metric_3    integer not null,\n    metric_4    integer not null,\n    metric_5    integer not null\n);\ninsert into metrics\nvalues\n    (  1,  7,  17, 93, 23, 110,  41),\n    (  2,  4,  22, 67, 38,  89,  37),\n    (  3,  6,  62, 30, 90,  92, 178),\n    (  4,  1,  20, 82, 87, 102,  37),\n    (  5,  1,   7, 34, 50,  41,  77),\n    (  6,  8,  49,  6, 24,  55,  19),\n    (  7,  2,  19, 64, 63,  83,   9),\n    (  8,  4,  46, 59, 21, 105,  64),\n    (  9,  1,  28, 74, 10, 103,  59),\n    ( 10,  6,  92, 77, 28, 170,  52),\n    ( 11,  7,  27, 78, 33, 106,  77),\n    ( 12,  4,  10, 17, 97,  28,  88),\n    ( 13,  6,  47, 74, 37, 122,  76),\n    ( 14,  1,  93,  7, 69, 100,   8),\n    ( 15,  4,  93, 38, 71, 131,  71),\n    ( 16,  7,  21, 58, 43,  80,  74),\n    ( 17,  1,  37,  7, 54,  44,  15),\n    ( 18,  2,  58, 94, 62, 152,  99),\n    ( 19,  7,  89, 52, 88, 142,  71),\n    ( 20,  5, 100,  7, 48, 107,  99),\n    ( 21,  7,  50, 89, 31, 139,  69),\n    ( 22,  5,   2, 28, 40,  29,  89),\n    ( 23,  2,  78, 74, 52, 153,  39),\n    ( 24,  3,  61, 65, 27, 126,  51),\n    ( 25, 10,  48, 98, 93, 145,  93),\n    ( 26,  9,  60, 21, 54,  81,  85),\n    ( 27,  4,  19, 48, 19,  67,  25),\n    ( 28,  3,  56, 26, 53,  83,  98),\n    ( 29,  4,  32, 43, 86,  75,  67),\n    ( 30,  6,  41,  1, 66,  43, 132),\n    ( 31,  7,  97, 32, 91, 129,   4),\n    ( 32,  0,  91, 85, 52, 176,  65),\n    ( 33, 10,  59, 66, 18, 125,  88),\n    ( 34,  5,  83, 35, 77, 118, 153),\n    ( 35,  2,  38, 52, 32,  89,  24),\n    ( 36,  0,  10,  0, 75,  10,  62),\n    ( 37,  6,  19, 92, 31, 111,  53),\n    ( 38,  7,  99,  3, 24, 102,  71),\n    ( 39,  2,  54, 41, 61,  96,  17),\n    ( 40,  6,  75, 61, 50, 136, 108),\n    ( 41,  4,  88, 46, 68, 133,  38),\n    ( 42,  0,  87, 60, 32, 146,  27),\n    ( 43,  9,  41, 25, 91,  65,  34),\n    ( 44,  7,  76, 28, 39, 103,  56),\n    ( 45,  2,  18, 14, 13,  31,  29),\n    ( 46,  8,  26, 38,  1,  64,  53),\n    ( 47, 10,  90, 53, 30, 143,  56),\n    ( 48,  4,   3, 20, 57,  23,  32),\n    ( 49,  7,  28,  5, 71,  34,  98),\n    ( 50,  0,  32, 37, 30,  69,  82),\n    ( 51,  3,  69, 35, 63, 105, 125),\n    ( 52,  4,  67, 70, 41, 136,  70),\n    ( 53,  8,  53, 68, 67, 122,  58),\n    ( 54,  8,  14, 71, 95,  85,  13),\n    ( 55,  5,  55, 88, 67, 143, 133),\n    ( 56,  7,  42, 80,  8, 121,   2),\n    ( 57, 10,  69, 79, 30, 148,  99),\n    ( 58,  3,  87, 57, 69, 144, 132),\n    ( 59,  7,  59, 68, 80, 128,  58),\n    ( 60,  5,  34, 36, 59,  70, 118),\n    ( 61,  6,  54, 94, 25, 148,  44),\n    ( 62, 10,  62, 45, 68, 107,   7),\n    ( 63,  6,  97, 98, 99, 195, 201),\n    ( 64,  0,  64, 25, 43,  89,  25),\n    ( 65,  7,  15, 38, 51,  53,  21),\n    ( 66,  3,  37, 15,  1,  51,   2),\n    ( 67,  8,  34, 64, 12,  98,  68),\n    ( 68,  3,  85, 58, 30, 143,  58),\n    ( 69,  7,  21, 90, 79, 111,  55),\n    ( 70,  5,  43, 64, 18, 107,  41),\n    ( 71,  5,  53, 85, 22, 138,  45),\n    ( 72, 10,  67, 49, 70, 117,  11),\n    ( 73, 10,  97,  5,  6, 102,  11),\n    ( 74,  7,  26, 59, 42,  85,  55),\n    ( 75,  6,   5, 25, 92,  30, 176),\n    ( 76,  4,  76, 26,  3, 102,  95),\n    ( 77,  1,  18, 53, 95,  71,   8),\n    ( 78,  0,  79, 51, 82, 130,  74),\n    ( 79,  1,  72, 63,  3, 136,  48),\n    ( 80,  9,  45, 24,  5,  70,  47),\n    ( 81,  0,  46, 40, 19,  86,  73),\n    ( 82,  8,  34, 72, 17, 107,  54),\n    ( 83,  7,  36, 54, 47,  90,   3),\n    ( 84,  3,  50, 57,  8, 107,  15),\n    ( 85,  2,  66, 11,  7,  77,  53),\n    ( 86,  0,   0, 82, 95,  83,   5),\n    ( 87,  4,  13, 38, 16,  51,  14),\n    ( 88,  6,  61, 10, 31,  72,  56),\n    ( 89,  7,  21, 17, 34,  38,  34),\n    ( 90,  3,  77, 31, 96, 108, 197),\n    ( 91,  3,  90, 27, 44, 117,  79),\n    ( 92,  0,  87, 24, 64, 110,  38),\n    ( 93,  6,  90, 32, 98, 122, 193),\n    ( 94, 10,  82, 65, 19, 147,  48),\n    ( 95,  1,  58, 93, 68, 152,   9),\n    ( 96,  7,  42, 43, 16,  84,  27),\n    ( 97,  5,  29, 31,  1,  60,   9),\n    ( 98,  2,  49, 70, 58, 119,   6),\n    ( 99,  6,  51, 51, 86, 102, 162),\n    (100,  4,  18, 35, 85,  53,  30)\n;\n</code></pre> <p>The solution can be found at:</p> <ul> <li>metric-correlation.md</li> </ul> Sample input customer_id segment metric_1 metric_2 metric_3 metric_4 metric_5 1 1 21 58 66 79 29 2 0 70 55 79 125 2 3 1 68 55 10 123 70 4 1 20 62 59 82 25 5 0 42 9 80 51 13 6 1 26 89 17 115 66 7 1 45 51 90 96 17 8 0 4 52 47 56 61 9 0 57 48 82 105 40 10 1 17 93 45 109 76 <pre><code>with metrics(customer_id, segment, metric_1, metric_2, metric_3, metric_4, metric_5) as (\n    values\n        ( 1, 1, 21, 58, 66,  79, 29),\n        ( 2, 0, 70, 55, 79, 125,  2),\n        ( 3, 1, 68, 55, 10, 123, 70),\n        ( 4, 1, 20, 62, 59,  82, 25),\n        ( 5, 0, 42,  9, 80,  51, 13),\n        ( 6, 1, 26, 89, 17, 115, 66),\n        ( 7, 1, 45, 51, 90,  96, 17),\n        ( 8, 0,  4, 52, 47,  56, 61),\n        ( 9, 0, 57, 48, 82, 105, 40),\n        (10, 1, 17, 93, 45, 109, 76)\n)\n</code></pre> Sample output segment metric_pair correlation 0 metric_1, metric_3 0.9051 1 metric_4, metric_5 0.8357 <pre><code>solution(segment, metric_pair, correlation) as (\n    values\n        (0, 'metric_1, metric_3', 0.9051),\n        (1, 'metric_4, metric_5', 0.8357)\n)\n</code></pre> Hint 1 <p>Use a correlation function, usually called <code>CORR</code>, to calculate the correlation between two metrics.</p> <p>If you're using a database that doesn't have a built-in correlation function, you can try to calculate it manually -- but I'd instead recommend skipping this question.</p> Hint 2 <p>To get every pair of metrics \"side by side\" for the <code>CORR</code> function, unpivot the table so that each metric is in its own row and then join the resulting table to itself on the <code>segment</code> and <code>customer_id</code> columns.</p>","tags":["pivot and unpivot","correlation"]},{"location":"challenging-sql-problems/problems/silver/predicting-values/","title":"Predicting values \ud83c\udfb1","text":"<p>Scenario</p> <p>Some students are studying Anscombe's quartet and have been asked to predict the <code>y</code> values for a given set of <code>x</code> values for each of the four datasets using linear regression.</p> <p>Question</p> <p>For each of the four datasets in Anscombe's quartet, use linear regression to predict the <code>y</code> values for <code>x</code> values <code>16</code>, <code>17</code>, and <code>18</code>.</p> <p>The output should have a row for each <code>x</code> value (<code>16</code>, <code>17</code>, <code>18</code>), with the columns:</p> <ul> <li><code>x</code></li> <li><code>dataset_1</code> as the predicted value for dataset 1, rounded to 1 decimal place</li> <li><code>dataset_2</code> as the predicted value for dataset 2, rounded to 1 decimal place</li> <li><code>dataset_3</code> as the predicted value for dataset 3, rounded to 1 decimal place</li> <li><code>dataset_4</code> as the predicted value for dataset 4, rounded to 1 decimal place</li> </ul> <p>Order the output by <code>x</code>.</p> Expand for the DDL <pre><code>create table anscombes_quartet (\n    dataset_1__x int,\n    dataset_1__y decimal(5, 2),\n    dataset_2__x int,\n    dataset_2__y decimal(5, 2),\n    dataset_3__x int,\n    dataset_3__y decimal(5, 2),\n    dataset_4__x int,\n    dataset_4__y decimal(5, 2),\n);\ninsert into anscombes_quartet\nvalues\n    (10,  8.04, 10, 9.14, 10,  7.46,  8,  6.58),\n    ( 8,  6.95,  8, 8.14,  8,  6.77,  8,  5.76),\n    (13,  7.58, 13, 8.74, 13, 12.74,  8,  7.71),\n    ( 9,  8.81,  9, 8.77,  9,  7.11,  8,  8.84),\n    (11,  8.33, 11, 9.26, 11,  7.81,  8,  8.47),\n    (14,  9.96, 14, 8.10, 14,  8.84,  8,  7.04),\n    ( 6,  7.24,  6, 6.13,  6,  6.08,  8,  5.25),\n    ( 4,  4.26,  4, 3.10,  4,  5.39, 19, 12.50),\n    (12, 10.84, 12, 9.13, 12,  8.15,  8,  5.56),\n    ( 7,  4.82,  7, 7.26,  7,  6.42,  8,  7.91),\n    ( 5,  5.68,  5, 4.74,  5,  5.73,  8,  6.89)\n;\n</code></pre> <p>There are plenty of resources online that walk through the maths behind linear regression, such as:</p> <ul> <li>https://www.youtube.com/watch?v=GAmzwIkGFgE</li> </ul> <p>The solution can be found at:</p> <ul> <li>predicting-values.md</li> </ul> Sample input <p>Use linear regression to predict the <code>y</code> values for <code>x</code> values <code>6</code> and <code>8</code>, using the following datasets:</p> dataset_1__x dataset_1__y dataset_2__x dataset_2__y 1 2.00 1 9.12 2 4.00 3 31.18 3 6.00 5 55.27 4 8.00 7 61.12 <pre><code>with datasets(dataset_1__x, dataset_1__y, dataset_2__x, dataset_2__y) as (\n    values\n        (1, 2.00, 1,  9.12),\n        (2, 4.00, 3, 31.18),\n        (3, 6.00, 5, 55.27),\n        (4, 8.00, 7, 61.12)\n)\n</code></pre> Sample output x dataset_1 dataset_2 6 12 57.2 8 16 75.2 <pre><code>solution(x, dataset_1, dataset_2) as (\n    values\n        (6, 12, 57.2),\n        (8, 16, 75.2)\n)\n</code></pre> Hint 1 <p>Unpivot the datasets so that you have a table with headers <code>dataset</code>, <code>x</code>, and <code>y</code>, then apply the linear regression, and finally pivot the results back.</p> Hint 2 <p>For databases that support them, use the <code>regr_slope</code> and <code>regr_intercept</code> functions (or equivalent) to calculate the slope and intercept of the regression line. Otherwise, you'll need to calculate these manually \ud83d\ude04</p>","tags":["pivot and unpivot","linear regression"]},{"location":"challenging-sql-problems/problems/silver/region-precipitation/","title":"Region precipitation \u2614","text":"<p>Scenario</p> <p>Analysts are using a weather dataset to understand the precipitation levels in different regions.</p> <p>They have a dataset with the average precipitation levels per month for various \"grids\" in the UK.</p> <p>A \"grid\" consists of a region and a location, joined together with a hyphen. For example, <code>AC-27</code> is a grid; the region is <code>AC</code> and the location is <code>27</code>.</p> <p>For grids that they are studying, they need the average of the monthly precipitation levels. Note that \"average\" here is just the mean of the values, not a weighted average or anything more complex.</p> <p>They are also aware that the dataset has some gaps in it, so to fill some values for the grids not in the dataset, they use the following logic:</p> <ul> <li>If the grid exists in the dataset, use the average of the monthly precipitation levels for the grid.</li> <li>If the grid doesn't exist but other grids in the same region do, use the average of the monthly precipitation levels for the grids in the same region.</li> <li>Otherwise, use the average of the monthly precipitation levels for the whole dataset.</li> </ul> <p>In each case, round the average to six decimal places.</p> <p>Question</p> <p>Find the average precipitation levels for the grids below:</p> <ul> <li><code>AC-27</code></li> <li><code>AQ-54</code></li> <li><code>AQ-55</code></li> <li><code>AQ-56</code></li> <li><code>BK-45</code></li> <li><code>BK-77</code></li> <li><code>BR-18</code></li> <li><code>X-17</code></li> </ul> <p>The output should have a row for each of the grids above, with the columns:</p> <ul> <li><code>grid_id</code></li> <li><code>average_precipitation</code> as the average precipitation level for the grid, using the logic above</li> </ul> <p>Order the output by <code>grid_id</code>.</p> Expand for the DDL <pre><code>create table precipitation (\n    grid_id      varchar primary key,\n    pr_january   numeric(14, 9),\n    pr_february  numeric(14, 9),\n    pr_march     numeric(14, 9),\n    pr_april     numeric(14, 9),\n    pr_may       numeric(14, 9),\n    pr_june      numeric(14, 9),\n    pr_july      numeric(14, 9),\n    pr_august    numeric(14, 9),\n    pr_september numeric(14, 9),\n    pr_october   numeric(14, 9),\n    pr_november  numeric(14, 9),\n    pr_december  numeric(14, 9)\n);\ninsert into precipitation\nvalues\n    ('AC-60',  77.313659668,  62.118778229,  62.963756561,  56.346126556,  56.507827759,  69.902297974,  76.416755676,  89.419967651,  70.767288208,  91.376182556,  94.308906555,  92.126197815),\n    ('AC-27', 185.549865723, 158.017700195, 135.172912598,  93.645156860,  78.538978577,  80.773406982,  94.989349365, 108.333747864, 135.921539307, 162.479400635, 162.757064819, 177.592956543),\n    ('AC-62',  81.204818726,  69.791305542,  64.281105042,  58.210914612,  60.880943298,  69.017036438,  80.740959167,  85.535209656,  68.737182617,  84.757614136,  90.352188110,  84.427131653),\n    ('AQ-54', 220.434295654, 175.195220947, 154.287719727, 116.504539490, 110.188606262, 119.045143127, 136.076614380, 157.373947144, 142.956726074, 210.527175903, 220.573928833, 225.163681030),\n    ('AQ-56', 143.682556152, 115.634277344,  99.110107422,  76.086784363,  79.841613770,  83.851272583,  95.768104553, 108.682968140,  99.521377563, 145.173202515, 142.895782471, 146.305435181),\n    ('AQ-89', 191.861526489, 142.057312012, 126.669769287, 102.783355713, 103.405494690, 107.407104492, 132.361694336, 149.649581909, 137.866043091, 196.463287354, 197.636199951, 211.932723999),\n    ('BK-93',  95.801376343,  66.791961670,  52.871055603,  54.724102020,  54.952892303,  52.194873810,  58.420711517,  64.284889221,  63.347404480, 100.553070068,  99.549606323,  99.715141296),\n    ('BK-77',  50.038841248,  38.004615784,  36.342658997,  41.668888092,  48.747943878,  55.387825012,  60.916931152,  63.336502075,  49.749183655,  63.458656311,  56.233974457,  53.735862732),\n    ('BK-89',  59.065109253,  44.379257202,  36.788879395,  42.614151001,  46.291542053,  52.152229309,  47.220512390,  57.204319000,  51.107784271,  67.685745239,  65.681800842,  59.279094696),\n    ('X-57',  120.805274963,  99.440124512,  90.794105530,  75.126007080,  77.335937500,  83.300735474,  95.619758606, 100.679786682,  92.218643188, 112.163681030, 121.698951721, 130.179748535),\n    ('X-64',  110.184608459,  70.485427856,  83.828720093,  67.215316772,  83.502983093,  84.731437683,  76.147949219,  85.476707458,  78.607589722, 108.458976746, 117.972114563, 111.607467651),\n    ('X-59',  150.341644287, 124.240226746, 110.852073669,  88.458465576,  89.581214905,  98.376708984, 105.874870300, 113.212600708, 106.017417908, 135.823379517, 147.751571655, 155.859909058)\n;\n</code></pre> <p>The solution can be found at:</p> <ul> <li>region-precipitation.md</li> </ul> <p>A worked example is provided below to help illustrate the average calculations.</p> Sample input <p>Find the average precipitation levels for the grids:</p> <ul> <li><code>AB-12</code></li> <li><code>AB-99</code></li> <li><code>Z-17</code></li> </ul> <p>...given the precipitation levels below:</p> grid_id pr_january pr_february pr_march pr_april pr_may pr_june pr_july pr_august pr_september pr_october pr_november pr_december AB-12 98.654982000 95.465774000 93.622460000 94.100401000 87.123098000 67.165477000 54.468731000 55.012740000 57.335890000 67.232145000 85.332001000 92.165432000 AB-34 154.119868000 125.977546000 101.024456000 134.523452000 99.456788000 95.025468000 92.135497000 93.653200000 98.126477000 103.332032000 111.360141000 125.216407000 C-56 56.963354000 76.455462000 61.879871000 87.666547000 85.931607000 83.636598000 51.258741000 65.165441000 71.636687000 94.654210000 92.632147000 101.300156000 <pre><code>with precipitation(grid_id, pr_january, pr_february, pr_march, pr_april, pr_may, pr_june, pr_july, pr_august, pr_september, pr_october, pr_november, pr_december) as (\n    values\n        ('AB-12',  98.654982000,  95.465774000,  93.622460000,  94.100401000, 87.123098000, 67.165477000, 54.468731000, 55.012740000, 57.335890000,  67.232145000,  85.332001000,  92.165432000),\n        ('AB-34', 154.119868000, 125.977546000, 101.024456000, 134.523452000, 99.456788000, 95.025468000, 92.135497000, 93.653200000, 98.126477000, 103.332032000, 111.360141000, 125.216407000),\n        ('C-56',   56.963354000,  76.455462000,  61.879871000,  87.666547000, 85.931607000, 83.636598000, 51.258741000, 65.165441000, 71.636687000,  94.654210000,  92.632147000, 101.300156000)\n)\n</code></pre> Sample output grid_id average_precipitation AB-12 78.973261 AB-99 95.067936 Z-17 89.189202 <pre><code>solution(grid_id, average_precipitation) as (\n    values\n        ('AB-12', 78.973261),\n        ('AB-99', 95.067936),\n        ('Z-17',  89.189202)\n)\n</code></pre> Hint 1 <p>Create a lookup table with the average precipitation levels for each grid in the dataset, as well as rolled up averages (by using <code>ROLLUP</code>) for each region and the entire dataset.</p> Hint 2 <p>Use the <code>GROUPING</code> (DuckDB) or <code>GROUPING_ID</code> (SQL Server) function to determine if a row is a total, subtotal, or grand total, and then join the lookup table to the dataset three times\u2014one for each total type\u2014using this group ID (and any other required columns).</p> <p>Success</p> <p>If you're interested in using some real (and complete!) Met Office data, you can find plenty at:</p> <ul> <li>https://climatedataportal.metoffice.gov.uk/</li> </ul>","tags":["pivot and unpivot","rollup"]},{"location":"challenging-sql-problems/problems/silver/region-precipitation/#worked-examples","title":"Worked examples","text":"<p>To help illustrate the average calculations, consider the following grids:</p> <ul> <li><code>AC-27</code></li> <li><code>AC-28</code></li> <li><code>Z-14</code></li> </ul> <p>Let's briefly walk through the average calculation for each of these grids.</p>","tags":["pivot and unpivot","rollup"]},{"location":"challenging-sql-problems/problems/silver/region-precipitation/#ac-27","title":"<code>AC-27</code>","text":"<p>The dataset has precipitation levels for <code>AC-27</code>, so we use the mean of the twelve monthly precipitation levels for <code>AC-27</code>: this is <code>131.147673</code>.</p>","tags":["pivot and unpivot","rollup"]},{"location":"challenging-sql-problems/problems/silver/region-precipitation/#ac-28","title":"<code>AC-28</code>","text":"<p>The dataset doesn't have precipitation levels for <code>AC-28</code>, but it does have precipitation levels for other <code>AC</code> locations (namely, <code>AC-27</code>, <code>AC-60</code>, and <code>AC-62</code>). We use the mean of the monthly precipitation levels for each grid in the <code>AC</code> region: this is <code>93.646562</code>.</p>","tags":["pivot and unpivot","rollup"]},{"location":"challenging-sql-problems/problems/silver/region-precipitation/#z-14","title":"<code>Z-14</code>","text":"<p>The dataset doesn't have precipitation levels for <code>Z-14</code>, and it doesn't have precipitation levels for any other <code>Z</code> locations. We use the mean of the monthly precipitation levels for all grids in the dataset: this is <code>99.378632</code>.</p>","tags":["pivot and unpivot","rollup"]},{"location":"challenging-sql-problems/solutions/bronze/combining-dimensions/","title":"Combining dimensions \ud83d\udd17","text":"<p>Tip</p> <p>Solution to the following problem:</p> <ul> <li>combining-dimensions.md</li> </ul>"},{"location":"challenging-sql-problems/solutions/bronze/combining-dimensions/#result-set","title":"Result Set","text":"<p>Regardless of the database, the result set should look like:</p> employee_id valid_from valid_until date_of_birth gender ethnicity job_title salary email phone 1 2021-06-13 2022-06-08 2004-02-18 Female Malaysian Teacher 5000.00 c.perot0@gmail.com null 1 2022-06-09 2023-05-27 2004-02-18 Non-binary Malaysian Teacher 5000.00 c.perot0@gmail.com null 1 2023-05-28 2024-01-29 2004-02-18 Non-binary Malaysian Teacher 6000.00 c.perot0@gmail.com null 1 2024-01-30 9999-12-31 2004-02-18 Non-binary Malaysian Teacher 6000.00 c.perot0@gmail.com 1986474151 2 2021-10-19 2023-11-26 1963-12-12 Female Navajo Data Analyst 4000.00 null null 2 2023-11-27 2024-03-01 1963-12-12 Female Navajo Data Analyst 6500.00 null null 2 2024-03-02 2024-04-04 1963-12-12 Female Navajo Engineering Manager 7000.00 null null 2 2024-04-05 9999-12-31 1963-12-12 Female Navajo Engineering Manager 7000.00 hpicard1@bing.com null 3 2022-01-29 2023-04-02 2000-10-17 Genderqueer White Software Engineer 6000.00 null null 3 2023-04-03 9999-12-31 2000-10-17 Genderqueer White Software Engineer 8000.00 null null 4 2022-04-28 2022-06-11 1987-12-13 Male Black null null null null 4 2022-06-12 2022-12-01 1987-12-13 Male Black Founder null tbayford3@hotmail.co.uk 01246 209863 4 2022-12-02 2023-11-11 1987-12-13 Male Black Founder null tbayford3@hotmail.co.uk 01752 492269 4 2023-11-12 2024-03-12 1987-12-13 Male Black Founder null tmacalinden@hotmail.co.uk 01270 530950 4 2024-03-13 9999-12-31 1987-12-13 Female Black Founder null tmacalinden@hotmail.co.uk 01270 530950 5 2022-08-31 2023-02-16 1999-09-10 Female Asian null null null null 5 2023-02-17 9999-12-31 1999-09-10 Female Asian null null null null Expand for the DDL <pre><code>solution(employee_id, valid_from, valid_until, date_of_birth, gender, ethnicity, job_title, salary, email, phone) as (\n    values\n        (1, '2021-06-13'::date, '2022-06-08'::date, '2004-02-18'::date, 'Female',      'Malaysian', 'Teacher',             5000.00, 'c.perot0@gmail.com',        null),\n        (1, '2022-06-09'::date, '2023-05-27'::date, '2004-02-18'::date, 'Non-binary',  'Malaysian', 'Teacher',             5000.00, 'c.perot0@gmail.com',        null),\n        (1, '2023-05-28'::date, '2024-01-29'::date, '2004-02-18'::date, 'Non-binary',  'Malaysian', 'Teacher',             6000.00, 'c.perot0@gmail.com',        null),\n        (1, '2024-01-30'::date, '9999-12-31'::date, '2004-02-18'::date, 'Non-binary',  'Malaysian', 'Teacher',             6000.00, 'c.perot0@gmail.com',        '1986474151'),\n        (2, '2021-10-19'::date, '2023-11-26'::date, '1963-12-12'::date, 'Female',      'Navajo',    'Data Analyst',        4000.00, null,                        null),\n        (2, '2023-11-27'::date, '2024-03-01'::date, '1963-12-12'::date, 'Female',      'Navajo',    'Data Analyst',        6500.00, null,                        null),\n        (2, '2024-03-02'::date, '2024-04-04'::date, '1963-12-12'::date, 'Female',      'Navajo',    'Engineering Manager', 7000.00, null,                        null),\n        (2, '2024-04-05'::date, '9999-12-31'::date, '1963-12-12'::date, 'Female',      'Navajo',    'Engineering Manager', 7000.00, 'hpicard1@bing.com',         null),\n        (3, '2022-01-29'::date, '2023-04-02'::date, '2000-10-17'::date, 'Genderqueer', 'White',     'Software Engineer',   6000.00, null,                        null),\n        (3, '2023-04-03'::date, '9999-12-31'::date, '2000-10-17'::date, 'Genderqueer', 'White',     'Software Engineer',   8000.00, null,                        null),\n        (4, '2022-04-28'::date, '2022-06-11'::date, '1987-12-13'::date, 'Male',        'Black',     null,                     null, null,                        null),\n        (4, '2022-06-12'::date, '2022-12-01'::date, '1987-12-13'::date, 'Male',        'Black',     'Founder',                null, 'tbayford3@hotmail.co.uk',   '01246 209863'),\n        (4, '2022-12-02'::date, '2023-11-11'::date, '1987-12-13'::date, 'Male',        'Black',     'Founder',                null, 'tbayford3@hotmail.co.uk',   '01752 492269'),\n        (4, '2023-11-12'::date, '2024-03-12'::date, '1987-12-13'::date, 'Male',        'Black',     'Founder',                null, 'tmacalinden@hotmail.co.uk', '01270 530950'),\n        (4, '2024-03-13'::date, '9999-12-31'::date, '1987-12-13'::date, 'Female',      'Black',     'Founder',                null, 'tmacalinden@hotmail.co.uk', '01270 530950'),\n        (5, '2022-08-31'::date, '2023-02-16'::date, '1999-09-10'::date, 'Female',      'Asian',     null,                     null, null,                        null),\n        (5, '2023-02-17'::date, '9999-12-31'::date, '1999-09-10'::date, 'Female',      'Asian',     null,                     null, null,                        null)\n)\n</code></pre>"},{"location":"challenging-sql-problems/solutions/bronze/combining-dimensions/#solution","title":"Solution","text":"<p>Some SQL solutions per database are provided below.</p> <p>DuckDB</p> <pre><code>with axis as (\n        select employee_id, valid_from\n        from dim_employee_demographics\n    union\n        select employee_id, valid_from\n        from dim_employee_career\n    union\n        select employee_id, valid_from\n        from dim_employee_contact\n)\n\nselect\n    employee_id,\n    valid_from,\n    lead(valid_from - 1, 1, '9999-12-31') over (\n        partition by employee_id\n        order by valid_from\n    ) as valid_until,\n\n    dim_employee_demographics.date_of_birth,\n    dim_employee_demographics.gender,\n    dim_employee_demographics.ethnicity,\n\n    dim_employee_career.job_title,\n    dim_employee_career.salary,\n\n    dim_employee_contact.email,\n    dim_employee_contact.phone\nfrom axis\n    asof left join dim_employee_demographics\n        using (employee_id, valid_from)\n    asof left join dim_employee_career\n        using (employee_id, valid_from)\n    asof left join dim_employee_contact\n        using (employee_id, valid_from)\norder by\n    employee_id,\n    valid_from\n</code></pre>"},{"location":"challenging-sql-problems/solutions/bronze/customer-churn/","title":"Customer churn \ud83d\udd04","text":"<p>Tip</p> <p>Solution to the following problem:</p> <ul> <li>customer-churn.md</li> </ul>"},{"location":"challenging-sql-problems/solutions/bronze/customer-churn/#result-set","title":"Result Set","text":"<p>Regardless of the database, the result set should look like:</p> user_id days_active_last_week 1 4 Expand for the DDL <pre><code>solution(user_id, days_active_last_week) as (\n    values\n        (1, 4)\n)\n</code></pre>"},{"location":"challenging-sql-problems/solutions/bronze/customer-churn/#solution","title":"Solution","text":"<p>Some SQL solutions per database are provided below.</p> <p>DuckDB</p> <pre><code>select\n    user_id,\n    bit_count((activity_history &gt;&gt; 7) &amp; (power(2, 7)::int - 1)) as days_active_last_week\nfrom user_history\nwhere 1=1\n    /* Active last week... */\n    and (activity_history &gt;&gt; 7) &amp; (power(2, 7)::int - 1) &gt; 0\n    /* ...and inactive this week */\n    and activity_history &amp; (power(2, 7)::int - 1) = 0\n</code></pre>"},{"location":"challenging-sql-problems/solutions/bronze/fibonacci-sequence/","title":"Fibonacci sequence \ud83d\udd22","text":"<p>Tip</p> <p>Solution to the following problem:</p> <ul> <li>fibonacci-sequence.md</li> </ul>"},{"location":"challenging-sql-problems/solutions/bronze/fibonacci-sequence/#result-set","title":"Result Set","text":"<p>Regardless of the database, the result set should look like:</p> n f_n 1 1 2 1 3 2 4 3 5 5 6 8 7 13 8 21 9 34 10 55 11 89 12 144 13 233 14 377 15 610 16 987 17 1597 18 2584 19 4181 20 6765 21 10946 22 17711 23 28657 24 46368 25 75025 26 121393 27 196418 28 317811 29 514229 30 832040 31 1346269 32 2178309 33 3524578 34 5702887 35 9227465 36 14930352 37 24157817 38 39088169 39 63245986 40 102334155 41 165580141 42 267914296 43 433494437 44 701408733 45 1134903170 Expand for the DDL <pre><code>solution(n, f_n) as (\n    values\n        (1,           1),\n        (2,           1),\n        (3,           2),\n        (4,           3),\n        (5,           5),\n        (6,           8),\n        (7,          13),\n        (8,          21),\n        (9,          34),\n        (10,         55),\n        (11,         89),\n        (12,        144),\n        (13,        233),\n        (14,        377),\n        (15,        610),\n        (16,        987),\n        (17,       1597),\n        (18,       2584),\n        (19,       4181),\n        (20,       6765),\n        (21,      10946),\n        (22,      17711),\n        (23,      28657),\n        (24,      46368),\n        (25,      75025),\n        (26,     121393),\n        (27,     196418),\n        (28,     317811),\n        (29,     514229),\n        (30,     832040),\n        (31,    1346269),\n        (32,    2178309),\n        (33,    3524578),\n        (34,    5702887),\n        (35,    9227465),\n        (36,   14930352),\n        (37,   24157817),\n        (38,   39088169),\n        (39,   63245986),\n        (40,  102334155),\n        (41,  165580141),\n        (42,  267914296),\n        (43,  433494437),\n        (44,  701408733),\n        (45, 1134903170)\n)\n</code></pre>"},{"location":"challenging-sql-problems/solutions/bronze/fibonacci-sequence/#solution","title":"Solution","text":"<p>Some SQL solutions per database are provided below.</p> <p>DuckDB, SQLite, PostgreSQL, Snowflake</p> <pre><code>with recursive fibonacci(n, f_n, f_m) as (\n        select 1, 1, 0\n    union all\n        select\n            n + 1,\n            f_n + f_m,\n            f_n\n        from fibonacci\n        where n &lt; 45\n)\n\nselect n, f_n\nfrom fibonacci\norder by n\n</code></pre> <p>SQL Server</p> <pre><code>with fibonacci(n, f_n, f_m) as (\n        select 1, 1, 0\n    union all\n        select\n            n + 1,\n            f_n + f_m,\n            f_n\n        from fibonacci\n        where n &lt; 45\n)\n\nselect n, f_n\nfrom fibonacci\norder by n\n</code></pre>"},{"location":"challenging-sql-problems/solutions/bronze/outstanding-invoices/","title":"Outstanding invoices \ud83d\udcb1","text":"<p>Tip</p> <p>Solution to the following problem:</p> <ul> <li>outstanding-invoices.md</li> </ul>"},{"location":"challenging-sql-problems/solutions/bronze/outstanding-invoices/#result-set","title":"Result Set","text":"<p>Regardless of the database, the result set should look like:</p> invoice_currency amount_outstanding GBP 59184.66 INR 852086.90 USD 11219.50 Expand for the DDL <pre><code>solution(invoice_currency, amount_outstanding) as (\n    values\n        ('GBP',  59184.66),\n        ('INR', 852086.90),\n        ('USD',  11219.50)\n)\n</code></pre>"},{"location":"challenging-sql-problems/solutions/bronze/outstanding-invoices/#solution","title":"Solution","text":"<p>Some SQL solutions per database are provided below.</p> <p>DuckDB</p> <pre><code>select\n    invoices.invoice_currency,\n    ceil(100 * sum(invoices.invoice_amount_usd * coalesce(exchange_rates.rate, 1))) / 100 as amount_outstanding\nfrom invoices\n    asof left join exchange_rates\n        on  invoices.invoice_datetime &gt;= exchange_rates.from_datetime\n        and invoices.invoice_currency = exchange_rates.to_currency\n        and exchange_rates.from_currency = 'USD'\nwhere invoices.is_paid = false\ngroup by invoices.invoice_currency\norder by invoices.invoice_currency\n</code></pre>"},{"location":"challenging-sql-problems/solutions/bronze/personalised-customer-emails/","title":"Personalised customer emails \ud83d\udce8","text":"<p>Tip</p> <p>Solution to the following problem:</p> <ul> <li>personalised-customer-emails.md</li> </ul>"},{"location":"challenging-sql-problems/solutions/bronze/personalised-customer-emails/#result-set","title":"Result Set","text":"<p>Regardless of the database, the result set should look like:</p> company_name company_email_address salutation_name Fractal Factory billiam@fractal-factory.co.uk William Friends For Hire joe.trib@f4hire.com Joey Some Company admin@somecompany.com <code>null</code> Expand for the DDL <pre><code>solution(company_name, company_email_address, salutation_name) as (\n    values\n        ('Fractal Factory',  'billiam@fractal-factory.co.uk', 'William'),\n        ('Friends For Hire', 'joe.trib@f4hire.com',           'Joey'),\n        ('Some Company',     'admin@somecompany.com',         null)\n)\n</code></pre>"},{"location":"challenging-sql-problems/solutions/bronze/personalised-customer-emails/#solution","title":"Solution","text":"<p>Some SQL solutions per database are provided below.</p> <p>DuckDB</p> <p>This DuckDB solution uses the Jaccard similarity with a 25% match threshold, but this isn't the only way to solve this problem.</p> <pre><code>with contact_preferences as (\n    select\n        company.full_name as company_name,\n        company.email_address as company_email_address,\n        individual.first_name,\n\n        /* Could also use `jaro_similarity`, `jaro_winkler_similarity`, `levenshtein` */\n        jaccard(\n            lower(split_part(company.email_address, '@', 1)),\n            lower(individual.full_name)\n        ) as similarity,\n\n        row_number() over(\n            partition by company_name\n            order by similarity desc\n        ) as contact_preference\n    from customer_relationships as relationships\n        left join customers as company\n            on relationships.parent_customer_id = company.customer_id\n        left join customers as individual\n            on relationships.child_customer_id = individual.customer_id\n)\n\nselect\n    company_name,\n    company_email_address,\n    /* Set a match threshold of 25% */\n    if(similarity &gt;= 0.25, first_name, null) as salutation_name\nfrom contact_preferences\nwhere contact_preference = 1\norder by company_name\n</code></pre> <p>SQL Server</p> <p>This SQL Server solution uses the Soundex differences with a 3 (out of 4) match threshold, but this isn't the only way to solve this problem.</p> <pre><code>with contact_preferences as (\n    select\n        company.full_name as company_name,\n        company.email_address as company_email_address,\n        individual.first_name,\n        individual.last_name,\n\n        difference(\n            left(company.email_address, -1 + charindex('@', company.email_address)),\n            individual.full_name\n        ) as similarity,\n\n        row_number() over(\n            partition by company.full_name\n            order by difference(\n                left(company.email_address, -1 + charindex('@', company.email_address)),\n                individual.full_name\n            ) desc\n        ) as contact_preference\n    from customer_relationships as relationships\n        left join customers as company\n            on relationships.parent_customer_id = company.customer_id\n        left join customers as individual\n            on relationships.child_customer_id = individual.customer_id\n)\n\nselect\n    company_name,\n    company_email_address,\n    /* Set a match threshold of 3 */\n    iif(similarity &gt;= 3, first_name, null) as salutation_name\nfrom contact_preferences\nwhere contact_preference = 1\norder by company_name\n</code></pre>"},{"location":"challenging-sql-problems/solutions/bronze/suspicious-login-activity/","title":"Suspicious login activity \ud83e\udd14","text":"<p>Tip</p> <p>Solution to the following problem:</p> <ul> <li>suspicious-login-activity.md</li> </ul>"},{"location":"challenging-sql-problems/solutions/bronze/suspicious-login-activity/#result-set","title":"Result Set","text":"<p>Regardless of the database, the result set should look like:</p> user_id consecutive_failures 1 7 3 8 Expand for the DDL <pre><code>solution(user_id, consecutive_failures) as (\n    values\n        (1, 7),\n        (3, 8)\n)\n</code></pre>"},{"location":"challenging-sql-problems/solutions/bronze/suspicious-login-activity/#solution","title":"Solution","text":"<p>Some SQL solutions per database are provided below.</p> <p>DuckDB</p> <pre><code>with event_groups as (\n    select\n        *,\n        (0\n            + row_number() over (partition by user_id             order by event_id)\n            - row_number() over (partition by user_id, event_type order by event_id)\n        ) as event_group\n    from events\n)\n\nselect\n    user_id,\n    count(*) as consecutive_failures\nfrom event_groups\nwhere event_type = 'login failed'\ngroup by user_id, event_group\nhaving consecutive_failures &gt;= 5\nqualify consecutive_failures = max(consecutive_failures) over (partition by user_id)\norder by user_id\n</code></pre> <p>SQL Server</p> <pre><code>with\n\nevent_groups as (\n    select\n        *,\n        (0\n            + row_number() over (partition by user_id             order by event_id)\n            - row_number() over (partition by user_id, event_type order by event_id)\n        ) as event_group\n    from events\n),\n\nconsecutive_failures as (\n    select\n        user_id,\n        count(*) as consecutive_failures,\n        max(count(*)) over (partition by user_id) as max_consecutive_failures\n    from event_groups\n    where event_type = 'login failed'\n    group by user_id, event_group\n    having count(*) &gt;= 5\n)\n\nselect\n    user_id,\n    consecutive_failures\nfrom consecutive_failures\nwhere 1=1\n    and consecutive_failures &gt;= 5\n    and consecutive_failures = max_consecutive_failures\norder by user_id\n</code></pre>"},{"location":"challenging-sql-problems/solutions/bronze/temperature-anomaly-detection/","title":"Temperature anomaly detection \ud83d\udd0d","text":"<p>Tip</p> <p>Solution to the following problem:</p> <ul> <li>temperature-anomaly-detection.md</li> </ul>"},{"location":"challenging-sql-problems/solutions/bronze/temperature-anomaly-detection/#result-set","title":"Result Set","text":"<p>Regardless of the database, the result set should look like:</p> site_id reading_datetime temperature average_temperature percentage_increase 1 2021-01-02 02:01:17 22.43 20.0525 11.8564 1 2021-01-04 21:45:34 22.69 20.2700 11.9388 2 2021-01-02 01:59:43 23.10 20.6050 12.1087 Expand for the DDL <pre><code>solution(site_id, reading_datetime, temperature, average_temperature, percentage_increase) as (\n    values\n        (1, '2021-01-02 02:01:17'::timestamp, 22.43, 20.0525, 11.8564),\n        (1, '2021-01-04 21:45:34'::timestamp, 22.69, 20.2700, 11.9388),\n        (2, '2021-01-02 01:59:43'::timestamp, 23.10, 20.6050, 12.1087)\n)\n</code></pre>"},{"location":"challenging-sql-problems/solutions/bronze/temperature-anomaly-detection/#solution","title":"Solution","text":"<p>Some SQL solutions per database are provided below.</p> <p>DuckDB</p> <pre><code>with temperatures as (\n    select\n        site_id,\n        reading_datetime,\n        temperature,\n        avg(temperature) over rows_around_site_reading as average_temperature\n    from readings\n    window rows_around_site_reading as (\n        partition by site_id\n        order by reading_datetime\n        rows between 2 preceding\n                 and 2 following\n             exclude current row\n    )\n    qualify 4 = count(*) over rows_around_site_reading\n)\n\nselect\n    site_id,\n    reading_datetime,\n    temperature,\n    round(average_temperature, 4) as average_temperature,\n    round(100.0 * (temperature - average_temperature) / average_temperature, 4) as percentage_increase\nfrom temperatures\nwhere percentage_increase &gt; 10\norder by\n    site_id,\n    reading_datetime\n</code></pre> <p>SQLite, PostgreSQL</p> <pre><code>with temperatures as (\n    select\n        site_id,\n        reading_datetime,\n        temperature,\n        avg(temperature) over rows_around_site_reading as average_temperature,\n        count(*) over rows_around_site_reading as count_of_rows\n    from readings\n    window rows_around_site_reading as (\n        partition by site_id\n        order by reading_datetime\n        rows between 2 preceding\n                 and 2 following\n             exclude current row\n    )\n)\n\nselect\n    site_id,\n    reading_datetime,\n    temperature,\n    round(average_temperature, 4) as average_temperature,\n    round(100.0 * (temperature - average_temperature) / average_temperature, 4) as percentage_increase\nfrom temperatures\nwhere 1=1\n    and count_of_rows = 4\n    and (temperature - average_temperature) / average_temperature &gt; 0.1\norder by\n    site_id,\n    reading_datetime\n</code></pre> <p>Snowflake</p> <pre><code>with temperatures as (\n    select\n        site_id,\n        reading_datetime,\n        temperature,\n        sum(temperature) over (\n            partition by site_id\n            order by reading_datetime rows between 2 preceding and 2 following\n        ) as sum_temps\n    from readings\n    qualify 5 = count(*) over (\n        partition by site_id\n        order by reading_datetime rows between 2 preceding and 2 following\n    )\n)\n\nselect\n    * exclude (sum_temps),\n    round((sum_temps - temperature) / 4, 4) as average_temperature,\n    round(100 * (temperature - average_temperature) / temperature, 4) as percentage_increase\nfrom temperatures\nwhere percentage_increase &gt; 10\norder by\n    site_id,\n    reading_datetime\n</code></pre> <p>SQL Server</p> <pre><code>with temperatures as (\n    select\n        site_id,\n        reading_datetime,\n        temperature,\n        (sum(temperature) over rows_around_site_reading - temperature) / 4 as average_temperature,\n        count(*) over rows_around_site_reading as count_of_rows\n    from readings\n    window rows_around_site_reading as (\n        partition by site_id\n        order by reading_datetime rows between 2 preceding and 2 following\n    )\n)\n\nselect\n    site_id,\n    reading_datetime,\n    temperature,\n    round(average_temperature, 4) as average_temperature,\n    round(100.0 * (temperature - average_temperature) / temperature, 4) as percentage_increase\nfrom temperatures\nwhere 1=1\n    and count_of_rows = 5\n    and (temperature - average_temperature) / temperature &gt; 0.1\norder by\n    site_id,\n    reading_datetime\n</code></pre>"},{"location":"challenging-sql-problems/solutions/bronze/uk-bank-holidays/","title":"UK bank holidays \ud83d\udcc5","text":"<p>Tip</p> <p>Solution to the following problem:</p> <ul> <li>uk-bank-holidays.md</li> </ul>"},{"location":"challenging-sql-problems/solutions/bronze/uk-bank-holidays/#result-set","title":"Result Set","text":"<p>A reduced result set (as at 2024-04-13) should look like:</p> division title date notes bunting england-and-wales New Year\u2019s Day 2018-01-01 true england-and-wales Good Friday 2018-03-30 false england-and-wales Easter Monday 2018-04-02 true england-and-wales Early May bank holiday 2018-05-07 true england-and-wales Spring bank holiday 2018-05-28 true england-and-wales Summer bank holiday 2018-08-27 true england-and-wales Christmas Day 2018-12-25 true england-and-wales Boxing Day 2018-12-26 true england-and-wales New Year\u2019s Day 2019-01-01 true ... ... ... ... ... Expand for the DDL <pre><code>solution(division, title, date, notes, bunting)(\n    values\n        ('england-and-wales', 'New Year\u2019s Day',         '2018-01-01', '', true),\n        ('england-and-wales', 'Good Friday',            '2018-03-30', '', false),\n        ('england-and-wales', 'Easter Monday',          '2018-04-02', '', true),\n        ('england-and-wales', 'Early May bank holiday', '2018-05-07', '', true),\n        ('england-and-wales', 'Spring bank holiday',    '2018-05-28', '', true),\n        ('england-and-wales', 'Summer bank holiday',    '2018-08-27', '', true),\n        ('england-and-wales', 'Christmas Day',          '2018-12-25', '', true),\n        ('england-and-wales', 'Boxing Day',             '2018-12-26', '', true),\n        ('england-and-wales', 'New Year\u2019s Day',         '2019-01-01', '', true)\n)\n</code></pre>"},{"location":"challenging-sql-problems/solutions/bronze/uk-bank-holidays/#solution","title":"Solution","text":"<p>The solution for DuckDB is provided below.</p> <p>DuckDB</p> <pre><code>select\n    division,\n    unnest(events.events, recursive:=true)\nfrom (\n    unpivot 'https://www.gov.uk/bank-holidays.json'\n    on \"england-and-wales\", \"scotland\", \"northern-ireland\"\n    into\n        name division\n        value events\n)\n</code></pre>"},{"location":"challenging-sql-problems/solutions/gold/encoding-datelist-ints/","title":"Encoding datelist ints \ud83d\udd10","text":"<p>Tip</p> <p>Solution to the following problem:</p> <ul> <li>encoding-datelist-ints.md</li> </ul>"},{"location":"challenging-sql-problems/solutions/gold/encoding-datelist-ints/#result-set","title":"Result Set","text":"<p>Regardless of the database, the result set should look like:</p> user_id last_update activity_history 1 2024-02-01 2684356096 2 2024-02-01 940442496 3 2024-02-01 204672192 Expand for the DDL <pre><code>solution(user_id, last_update, activity_history) as (\n    values\n        (1, '2024-02-01'::date, 2684356096),\n        (2, '2024-02-01'::date,  940442496),\n        (3, '2024-02-01'::date,  204672192)\n)\n</code></pre>"},{"location":"challenging-sql-problems/solutions/gold/encoding-datelist-ints/#solution","title":"Solution","text":"<p>Some SQL solutions per database are provided below.</p> <p>DuckDB</p> <pre><code>with\n\nsession_datetimes as (\n    select\n        event_id,\n        user_id,\n        event_datetime as login_datetime,\n        coalesce(\n            (\n                select min(event_datetime)\n                from events as innr\n                where 1=1\n                    and events.user_id = innr.user_id\n                    and innr.event_datetime &gt; events.event_datetime\n                    and innr.event_datetime &lt;= events.event_datetime + interval '1 day'\n                    and innr.event_type = 'logout'\n            ),\n            events.event_datetime + interval '1 day'\n        ) as logout_datetime,\n    from events\n    where event_type = 'login'\n),\n\nevent_groups as (\n    select\n        *,\n        sum(is_new_session::int) over (order by login_datetime) as session_id\n    from (\n        select\n            *,\n            login_datetime &gt;= lag(logout_datetime, 1, login_datetime) over (\n                partition by user_id\n                order by login_datetime\n            ) as is_new_session\n        from session_datetimes\n    )\n),\n\nsessions as (\n    select\n        user_id,\n        min(login_datetime)::date as login_date,\n        max(logout_datetime)::date as logout_date\n    from event_groups\n    group by user_id, session_id\n),\n\ndates(active_date) as (\n    select unnest(generate_series(\n        (select min(event_datetime)::date from events),\n        (select max(event_datetime)::date from events),\n        interval '1 day'\n    ))\n),\n\nactivity(user_id, active_date, is_active) as (\n    select\n        users.user_id,\n        dates.active_date::date as active_date,\n        exists(\n            select *\n            from sessions\n            where 1=1\n                and users.user_id = sessions.user_id\n                and dates.active_date between sessions.login_date\n                                          and sessions.logout_date\n        )::int as is_active,\n        row_number() over (\n            partition by users.user_id\n            order by dates.active_date desc\n        ) as step\n    from (select distinct user_id from sessions) as users\n        cross join dates\n)\n\nselect\n    user_id,\n    max(active_date) as last_update,\n    sum(is_active * power(2, step - 1)) as activity_history\nfrom activity\ngroup by user_id\norder by user_id\n</code></pre>"},{"location":"challenging-sql-problems/solutions/gold/loan-repayment-schedule/","title":"Loan repayment schedule \ud83d\udcb0","text":"<p>Tip</p> <p>Solution to the following problem:</p> <ul> <li>loan-repayment-schedule.md</li> </ul>"},{"location":"challenging-sql-problems/solutions/gold/loan-repayment-schedule/#result-set","title":"Result Set","text":"<p>Regardless of the database, the result set should look like:</p> loan_id repayment_number repayment_date interest principal total balance 1 1 2024-02-01 1600.00 12682.06 14282.06 67317.94 1 2 2024-03-01 1346.36 12935.70 14282.06 54382.24 1 3 2024-04-01 1087.64 13194.42 14282.06 41187.82 1 4 2024-05-01 823.76 13458.30 14282.06 27729.52 1 5 2024-06-01 554.59 13727.47 14282.06 14002.05 1 6 2024-07-01 280.04 14002.05 14282.09 0.00 2 1 2024-02-02 1125.00 5751.00 6876.00 69249.00 2 2 2024-03-02 1038.74 5837.26 6876.00 63411.74 2 3 2024-04-02 951.18 5924.82 6876.00 57486.92 2 4 2024-05-02 862.30 6013.70 6876.00 51473.22 2 5 2024-06-02 772.10 6103.90 6876.00 45369.32 2 6 2024-07-02 680.54 6195.46 6876.00 39173.86 2 7 2024-08-02 587.61 6288.39 6876.00 32885.47 2 8 2024-09-02 493.28 6382.72 6876.00 26502.75 2 9 2024-10-02 397.54 6478.46 6876.00 20024.29 2 10 2024-11-02 300.36 6575.64 6876.00 13448.65 2 11 2024-12-02 201.73 6674.27 6876.00 6774.38 2 12 2025-01-02 101.62 6774.38 6876.00 0.00 3 1 2024-02-03 1000.00 3707.35 4707.35 96292.65 3 2 2024-03-03 962.93 3744.42 4707.35 92548.23 3 3 2024-04-03 925.48 3781.87 4707.35 88766.36 3 4 2024-05-03 887.66 3819.69 4707.35 84946.67 3 5 2024-06-03 849.47 3857.88 4707.35 81088.79 3 6 2024-07-03 810.89 3896.46 4707.35 77192.33 3 7 2024-08-03 771.92 3935.43 4707.35 73256.90 3 8 2024-09-03 732.57 3974.78 4707.35 69282.12 3 9 2024-10-03 692.82 4014.53 4707.35 65267.59 3 10 2024-11-03 652.68 4054.67 4707.35 61212.92 3 11 2024-12-03 612.13 4095.22 4707.35 57117.70 3 12 2025-01-03 571.18 4136.17 4707.35 52981.53 3 13 2025-02-03 529.82 4177.53 4707.35 48804.00 3 14 2025-03-03 488.04 4219.31 4707.35 44584.69 3 15 2025-04-03 445.85 4261.50 4707.35 40323.19 3 16 2025-05-03 403.23 4304.12 4707.35 36019.07 3 17 2025-06-03 360.19 4347.16 4707.35 31671.91 3 18 2025-07-03 316.72 4390.63 4707.35 27281.28 3 19 2025-08-03 272.81 4434.54 4707.35 22846.74 3 20 2025-09-03 228.47 4478.88 4707.35 18367.86 3 21 2025-10-03 183.68 4523.67 4707.35 13844.19 3 22 2025-11-03 138.44 4568.91 4707.35 9275.28 3 23 2025-12-03 92.75 4614.60 4707.35 4660.68 3 24 2026-01-03 46.61 4660.68 4707.29 0.00 Expand for the DDL <pre><code>solution(loan_id, repayment_number, repayment_date, interest, principal, total, balance) as (\n    values\n        (1,  1, '2024-02-01'::date, 1600.00, 12682.06, 14282.06, 67317.94),\n        (1,  2, '2024-03-01'::date, 1346.36, 12935.70, 14282.06, 54382.24),\n        (1,  3, '2024-04-01'::date, 1087.64, 13194.42, 14282.06, 41187.82),\n        (1,  4, '2024-05-01'::date,  823.76, 13458.30, 14282.06, 27729.52),\n        (1,  5, '2024-06-01'::date,  554.59, 13727.47, 14282.06, 14002.05),\n        (1,  6, '2024-07-01'::date,  280.04, 14002.05, 14282.09,     0.00),\n        (2,  1, '2024-02-02'::date, 1125.00,  5751.00,  6876.00, 69249.00),\n        (2,  2, '2024-03-02'::date, 1038.74,  5837.26,  6876.00, 63411.74),\n        (2,  3, '2024-04-02'::date,  951.18,  5924.82,  6876.00, 57486.92),\n        (2,  4, '2024-05-02'::date,  862.30,  6013.70,  6876.00, 51473.22),\n        (2,  5, '2024-06-02'::date,  772.10,  6103.90,  6876.00, 45369.32),\n        (2,  6, '2024-07-02'::date,  680.54,  6195.46,  6876.00, 39173.86),\n        (2,  7, '2024-08-02'::date,  587.61,  6288.39,  6876.00, 32885.47),\n        (2,  8, '2024-09-02'::date,  493.28,  6382.72,  6876.00, 26502.75),\n        (2,  9, '2024-10-02'::date,  397.54,  6478.46,  6876.00, 20024.29),\n        (2, 10, '2024-11-02'::date,  300.36,  6575.64,  6876.00, 13448.65),\n        (2, 11, '2024-12-02'::date,  201.73,  6674.27,  6876.00,  6774.38),\n        (2, 12, '2025-01-02'::date,  101.62,  6774.38,  6876.00,     0.00),\n        (3,  1, '2024-02-03'::date, 1000.00,  3707.35,  4707.35, 96292.65),\n        (3,  2, '2024-03-03'::date,  962.93,  3744.42,  4707.35, 92548.23),\n        (3,  3, '2024-04-03'::date,  925.48,  3781.87,  4707.35, 88766.36),\n        (3,  4, '2024-05-03'::date,  887.66,  3819.69,  4707.35, 84946.67),\n        (3,  5, '2024-06-03'::date,  849.47,  3857.88,  4707.35, 81088.79),\n        (3,  6, '2024-07-03'::date,  810.89,  3896.46,  4707.35, 77192.33),\n        (3,  7, '2024-08-03'::date,  771.92,  3935.43,  4707.35, 73256.90),\n        (3,  8, '2024-09-03'::date,  732.57,  3974.78,  4707.35, 69282.12),\n        (3,  9, '2024-10-03'::date,  692.82,  4014.53,  4707.35, 65267.59),\n        (3, 10, '2024-11-03'::date,  652.68,  4054.67,  4707.35, 61212.92),\n        (3, 11, '2024-12-03'::date,  612.13,  4095.22,  4707.35, 57117.70),\n        (3, 12, '2025-01-03'::date,  571.18,  4136.17,  4707.35, 52981.53),\n        (3, 13, '2025-02-03'::date,  529.82,  4177.53,  4707.35, 48804.00),\n        (3, 14, '2025-03-03'::date,  488.04,  4219.31,  4707.35, 44584.69),\n        (3, 15, '2025-04-03'::date,  445.85,  4261.50,  4707.35, 40323.19),\n        (3, 16, '2025-05-03'::date,  403.23,  4304.12,  4707.35, 36019.07),\n        (3, 17, '2025-06-03'::date,  360.19,  4347.16,  4707.35, 31671.91),\n        (3, 18, '2025-07-03'::date,  316.72,  4390.63,  4707.35, 27281.28),\n        (3, 19, '2025-08-03'::date,  272.81,  4434.54,  4707.35, 22846.74),\n        (3, 20, '2025-09-03'::date,  228.47,  4478.88,  4707.35, 18367.86),\n        (3, 21, '2025-10-03'::date,  183.68,  4523.67,  4707.35, 13844.19),\n        (3, 22, '2025-11-03'::date,  138.44,  4568.91,  4707.35,  9275.28),\n        (3, 23, '2025-12-03'::date,   92.75,  4614.60,  4707.35,  4660.68),\n        (3, 24, '2026-01-03'::date,   46.61,  4660.68,  4707.29,     0.00)\n)\n</code></pre>"},{"location":"challenging-sql-problems/solutions/gold/loan-repayment-schedule/#solution","title":"Solution","text":"<p>Some SQL solutions per database are provided below.</p> <p>DuckDB</p> <pre><code>with recursive\n\nmonthly_repayment_value as (\n    select\n        loan_id,\n        power(1 + interest_rate, repayments) as amortised_rate,\n        round(\n            (loan_value * interest_rate * amortised_rate) / (amortised_rate - 1),\n            2\n        ) as monthly_repayment\n    from loans\n),\n\nschedule as (\n        select\n            /* loan details */\n            loans.loan_id,\n            loans.interest_rate,\n            loans.repayments,\n            monthly_repayment_value.monthly_repayment,\n\n            /* repayment details */\n            0 as repayment_number,\n            loans.start_date as repayment_date,\n            0::decimal(10, 2) as starting_balance,\n            0::decimal(10, 2) as interest,\n            0::decimal(10, 2) as principal,\n            0::decimal(10, 2) as total,\n            loans.loan_value as remaining_balance\n        from loans\n            inner join monthly_repayment_value\n                using (loan_id)\n    union all\n        select\n            loan_id,\n            interest_rate,\n            repayments,\n            monthly_repayment,\n\n            repayment_number + 1,\n            repayment_date + interval '1 month',\n            remaining_balance,\n            round(remaining_balance * interest_rate, 2) as interest_,\n            monthly_repayment - interest_ as principal_,\n            monthly_repayment,\n            remaining_balance - principal_\n        from schedule\n        where repayment_number &lt; repayments\n)\n\nselect\n    loan_id,\n    repayment_number,\n    repayment_date,\n    interest,\n\n    /* adjust the final repayment with the rounding error */\n    if(repayment_number = repayments, starting_balance, principal) as principal,\n    if(repayment_number = repayments, starting_balance + interest, total) as total,\n    if(repayment_number = repayments, 0, remaining_balance) as balance\nfrom schedule\nwhere repayment_number &gt; 0\norder by\n    loan_id,\n    repayment_number\n</code></pre>"},{"location":"challenging-sql-problems/solutions/gold/risk-invasions/","title":"Risk invasions \ud83d\udee1\ufe0f","text":"<p>Tip</p> <p>Solution to the following problem:</p> <ul> <li>risk-invasions.md</li> </ul>"},{"location":"challenging-sql-problems/solutions/gold/risk-invasions/#result-set","title":"Result Set","text":"<p>Regardless of the database, the result set should look like:</p> attackers_remaining defenders_remaining likelihood attackers_win_likelihood defenders_win_likelihood 0 1 0.0444861546 0.7295558279 0.2704441690 0 2 0.0811874068 0.7295558279 0.2704441690 0 3 0.0613334164 0.7295558279 0.2704441690 0 4 0.0473588523 0.7295558279 0.2704441690 0 5 0.0248517821 0.7295558279 0.2704441690 0 6 0.0112265568 0.7295558279 0.2704441690 1 0 0.0317758219 0.7295558279 0.2704441690 2 0 0.0656099405 0.7295558279 0.2704441690 3 0 0.1082686491 0.7295558279 0.2704441690 4 0 0.1294309856 0.7295558279 0.2704441690 5 0 0.1283258874 0.7295558279 0.2704441690 6 0 0.1230138225 0.7295558279 0.2704441690 7 0 0.0917943963 0.7295558279 0.2704441690 8 0 0.0513363246 0.7295558279 0.2704441690 Expand for the DDL <pre><code>solution(attackers_remaining, defenders_remaining, likelihood, attackers_win_likelihood, defenders_win_likelihood) as (\n    values\n        (0, 1, 0.0444861546, 0.7295558279, 0.2704441690),\n        (0, 2, 0.0811874068, 0.7295558279, 0.2704441690),\n        (0, 3, 0.0613334164, 0.7295558279, 0.2704441690),\n        (0, 4, 0.0473588523, 0.7295558279, 0.2704441690),\n        (0, 5, 0.0248517821, 0.7295558279, 0.2704441690),\n        (0, 6, 0.0112265568, 0.7295558279, 0.2704441690),\n        (1, 0, 0.0317758219, 0.7295558279, 0.2704441690),\n        (2, 0, 0.0656099405, 0.7295558279, 0.2704441690),\n        (3, 0, 0.1082686491, 0.7295558279, 0.2704441690),\n        (4, 0, 0.1294309856, 0.7295558279, 0.2704441690),\n        (5, 0, 0.1283258874, 0.7295558279, 0.2704441690),\n        (6, 0, 0.1230138225, 0.7295558279, 0.2704441690),\n        (7, 0, 0.0917943963, 0.7295558279, 0.2704441690),\n        (8, 0, 0.0513363246, 0.7295558279, 0.2704441690)\n)\n</code></pre>"},{"location":"challenging-sql-problems/solutions/gold/risk-invasions/#solution","title":"Solution","text":"<p>Some SQL solutions per database are provided below.</p> <p>DuckDB</p> <pre><code>with recursive\n\ndie(face) as (from generate_series(1, 6)),\n\nrolls(n, outcome) as (\n        select 1, [face]\n        from die\n    union all\n        select\n            rolls.n + 1,\n            list_reverse_sort(list_append(rolls.outcome, die.face)),\n        from rolls\n            cross join die\n        where rolls.n &lt; 3\n),\n\nbattle_scenarios(attackers, defenders) as (\n    select *\n    from\n        generate_series(1, 3) as attackers,\n        generate_series(1, 2) as defenders\n),\n\nbattle_outcomes as (\n    select\n        battle_scenarios.attackers,\n        battle_scenarios.defenders,\n\n        coalesce(attacker_rolls.outcome[1], 0) as attacker_roll_1,\n        coalesce(attacker_rolls.outcome[2], 0) as attacker_roll_2,\n        coalesce(defender_rolls.outcome[1], 0) as defender_roll_1,\n        coalesce(defender_rolls.outcome[2], 0) as defender_roll_2,\n\n        (0\n            + (attacker_roll_1 &gt; defender_roll_1)::int\n            + (attacker_roll_2 &gt; defender_roll_2 and battle_scenarios.defenders = 2)::int\n        ) as attacks_won\n    from battle_scenarios\n        left join rolls as attacker_rolls\n            on battle_scenarios.attackers = attacker_rolls.n\n        left join rolls as defender_rolls\n            on battle_scenarios.defenders = defender_rolls.n\n),\n\nbattle_likelihoods as (\n    select\n        attackers,\n        defenders,\n        attacks_won,\n        max(attacks_won) over scenario - attacks_won as attacks_lost,\n        count(*) / sum(count(*)) over scenario as likelihood\n    from battle_outcomes\n    group by attackers, defenders, attacks_won\n    window scenario as (partition by attackers, defenders)\n),\n\ninvasions as (\n        select\n            8 as attackers_remaining,\n            6 as defenders_remaining,\n            1::numeric(12, 10) as likelihood,\n    union all\n        select\n            invasions.attackers_remaining - battle_likelihoods.attacks_lost,\n            invasions.defenders_remaining - battle_likelihoods.attacks_won,\n            invasions.likelihood * coalesce(battle_likelihoods.likelihood, 0),\n        from invasions\n            inner join battle_likelihoods\n                on  least(invasions.attackers_remaining, 3) = battle_likelihoods.attackers\n                and least(invasions.defenders_remaining, 2) = battle_likelihoods.defenders\n                and battle_likelihoods.attacks_won &lt;= least(invasions.attackers_remaining, invasions.defenders_remaining)\n)\n\nselect\n    attackers_remaining,\n    defenders_remaining,\n    sum(likelihood) as likelihood,\n    sum(sum(likelihood) filter (where attackers_remaining != 0)) over () as attackers_win_likelihood,\n    sum(sum(likelihood) filter (where defenders_remaining != 0)) over () as defenders_win_likelihood,\nfrom invasions\nwhere attackers_remaining = 0 or defenders_remaining = 0\ngroup by attackers_remaining, defenders_remaining\norder by attackers_remaining, defenders_remaining\n</code></pre>"},{"location":"challenging-sql-problems/solutions/gold/supply-chain-network/","title":"Supply chain network \ud83d\ude9b","text":"<p>Tip</p> <p>Solution to the following problem:</p> <ul> <li>supply-chain-network.md</li> </ul>"},{"location":"challenging-sql-problems/solutions/gold/supply-chain-network/#result-set","title":"Result Set","text":"<p>Regardless of the database, the result set should look like:</p> stock_date store_id supplier_id stock_volume stock_proportion 2024-01-01 6 1 0 0.00 2024-01-01 6 2 10 100.00 2024-01-01 7 1 0 0.00 2024-01-01 7 2 0 0.00 2024-01-01 8 1 0 0.00 2024-01-01 8 2 0 0.00 2024-01-02 6 1 0 0.00 2024-01-02 6 2 9 100.00 2024-01-02 7 1 6 54.55 2024-01-02 7 2 5 45.45 2024-01-02 8 1 0 0.00 2024-01-02 8 2 1 100.00 2024-01-03 6 1 14 41.18 2024-01-03 6 2 20 58.82 2024-01-03 7 1 0 0.00 2024-01-03 7 2 2 100.00 2024-01-03 8 1 1 6.25 2024-01-03 8 2 15 93.75 2024-01-04 6 1 34 57.63 2024-01-04 6 2 25 42.37 2024-01-04 7 1 3 100.00 2024-01-04 7 2 0 0.00 2024-01-04 8 1 15 100.00 2024-01-04 8 2 0 0.00 Expand for the DDL <pre><code>solution(stock_date, store_id, supplier_id, stock_volume, stock_proportion) as (\n    values\n        ('2024-01-01'::date, 6, 1,  0,   0.00),\n        ('2024-01-01'::date, 6, 2, 10, 100.00),\n        ('2024-01-01'::date, 7, 1,  0,   0.00),\n        ('2024-01-01'::date, 7, 2,  0,   0.00),\n        ('2024-01-01'::date, 8, 1,  0,   0.00),\n        ('2024-01-01'::date, 8, 2,  0,   0.00),\n        ('2024-01-02'::date, 6, 1,  0,   0.00),\n        ('2024-01-02'::date, 6, 2,  9, 100.00),\n        ('2024-01-02'::date, 7, 1,  6,  54.55),\n        ('2024-01-02'::date, 7, 2,  5,  45.45),\n        ('2024-01-02'::date, 8, 1,  0,   0.00),\n        ('2024-01-02'::date, 8, 2,  1, 100.00),\n        ('2024-01-03'::date, 6, 1, 14,  41.18),\n        ('2024-01-03'::date, 6, 2, 20,  58.82),\n        ('2024-01-03'::date, 7, 1,  0,   0.00),\n        ('2024-01-03'::date, 7, 2,  2, 100.00),\n        ('2024-01-03'::date, 8, 1,  1,   6.25),\n        ('2024-01-03'::date, 8, 2, 15,  93.75),\n        ('2024-01-04'::date, 6, 1, 34,  57.63),\n        ('2024-01-04'::date, 6, 2, 25,  42.37),\n        ('2024-01-04'::date, 7, 1,  3, 100.00),\n        ('2024-01-04'::date, 7, 2,  0,   0.00),\n        ('2024-01-04'::date, 8, 1, 15, 100.00),\n        ('2024-01-04'::date, 8, 2,  0,   0.00)\n)\n</code></pre>"},{"location":"challenging-sql-problems/solutions/gold/supply-chain-network/#solution","title":"Solution","text":"<p>Some SQL solutions per database are provided below.</p> <p>DuckDB</p> <p> </p>"},{"location":"challenging-sql-problems/solutions/gold/travel-plans/","title":"Travel plans \ud83d\ude82","text":"<p>Tip</p> <p>Solution to the following problem:</p> <ul> <li>travel-plans.md</li> </ul>"},{"location":"challenging-sql-problems/solutions/gold/travel-plans/#result-set","title":"Result Set","text":"<p>Regardless of the database, the result set should look like:</p> route departure_datetime_utc arrival_datetime_utc duration cost New York - London Gatwick - London St Pancras - Paris 2024-01-01 18:00:00 2024-01-02 14:30:00 20:30:00 212.00 New York - Paris 2024-01-01 23:00:00 2024-01-02 16:45:00 17:45:00 279.00 Expand for the DDL <pre><code>solution(route, departure_datetime_utc, arrival_datetime_utc, duration, cost) as (\n    values\n        ('New York - London Gatwick - London St Pancras - Paris', '2024-01-01 18:00:00'::timestamp, '2024-01-02 14:30:00'::timestamp, '20:30:00', 212.00),\n        ('New York - Paris',                                      '2024-01-01 23:00:00'::timestamp, '2024-01-02 16:45:00'::timestamp, '17:45:00', 279.00)\n)\n</code></pre>"},{"location":"challenging-sql-problems/solutions/gold/travel-plans/#solution","title":"Solution","text":"<p>Some SQL solutions per database are provided below.</p> <p>DuckDB</p> <p>There's probably a better way to do this. Please let me know if you find it!</p> <pre><code>/*\n    In DuckDB, we can convert a timestamp with a timezone to a timestamp\n    in UTC by casting it to a varchar and then to a timestamp.\n*/\nwith recursive\n\ndate_axis as (\n        select\n            /* give us a day either side to be safe */\n            min(departure_datetime::date) - 1 as date_,\n            max(arrival_datetime::date) + 1 as max_date,\n        from routes_timetable\n    union all\n        select\n            date_ + interval '1 day',\n            max_date,\n        from date_axis\n        where date_ &lt; max_date - 1  /* account for final loop */\n),\n\nscheduled_timetable as (\n        select\n            schedule_id,\n            from_location,\n            to_location,\n            cost,\n            duration::varchar::interval as duration,\n            earliest_departure::varchar::time as departure_time_utc,\n            departure_time_utc + duration::varchar::interval as arrival_time_utc,\n            latest_departure::varchar::time as limit_,\n            frequency::varchar::interval as frequency,\n        from routes_schedule\n    union all\n        select\n            schedule_id,\n            from_location,\n            to_location,\n            cost,\n            duration,\n            departure_time_utc + frequency as departure_time_utc,\n            arrival_time_utc + frequency as arrival_time_utc,\n            limit_,\n            frequency,\n        from scheduled_timetable\n        where departure_time_utc &lt;= limit_ - frequency  /* account for the final loop */\n          and frequency is not null  /* daily schedules don't need to be expanded */\n),\n\ntimetable as (\n        select\n            from_location,\n            to_location,\n            departure_datetime::varchar::timestamp as departure_datetime_utc,\n            arrival_datetime::varchar::timestamp as arrival_datetime_utc,\n            arrival_datetime_utc - departure_datetime_utc as duration,\n            cost,\n        from routes_timetable\n    union all\n        select\n            scheduled_timetable.from_location,\n            scheduled_timetable.to_location,\n            scheduled_timetable.departure_time_utc + date_axis.date_,\n            scheduled_timetable.arrival_time_utc + date_axis.date_,\n            scheduled_timetable.duration,\n            scheduled_timetable.cost,\n        from scheduled_timetable\n            cross join date_axis\n),\n\nroutes as (\n        select\n            from_location as starting_location,\n            departure_datetime_utc as starting_departure_datetime_utc,\n\n            from_location,\n            to_location,\n            departure_datetime_utc,\n            arrival_datetime_utc,\n            cost,\n            from_location || ' - ' || to_location as route,\n        from timetable\n        where 1=1\n            and from_location = 'New York'\n            and departure_datetime_utc &gt;= '2024-01-01 12:00:00-05:00'\n    union all\n        select\n            routes.starting_location,\n            routes.starting_departure_datetime_utc,\n\n            timetable.from_location,\n            timetable.to_location,\n            timetable.departure_datetime_utc,\n            timetable.arrival_datetime_utc,\n            routes.cost + timetable.cost,\n            routes.route || ' - ' || timetable.to_location,\n        from routes\n            inner join timetable\n                on  routes.to_location = timetable.from_location\n                and timetable.departure_datetime_utc between routes.arrival_datetime_utc + interval '30 minutes'\n                                                         and routes.arrival_datetime_utc + interval '6 hours'\n)\n\nselect distinct\n    route,\n    starting_departure_datetime_utc as departure_datetime_utc,\n    arrival_datetime_utc,\n    arrival_datetime_utc - starting_departure_datetime_utc as duration,\n    cost\nfrom routes\nwhere to_location = 'Paris'\nqualify 0=1\n    /* fastest route */\n    or 1 = row_number() over (order by duration, cost, route)\n    /* cheapest route */\n    or 1 = row_number() over (order by cost, duration, route)\norder by arrival_datetime_utc\n</code></pre>"},{"location":"challenging-sql-problems/solutions/silver/bannable-login-activity/","title":"Bannable login activity \u274c","text":"<p>Tip</p> <p>Solution to the following problem:</p> <ul> <li>bannable-login-activity.md</li> </ul>"},{"location":"challenging-sql-problems/solutions/silver/bannable-login-activity/#result-set","title":"Result Set","text":"<p>Regardless of the database, the result set should look like:</p> user_id ban_date 3 2024-02-01 Expand for the DDL <pre><code>solution(user_id, ban_date) as (\n    values\n        (3, '2024-02-01'::date)\n)\n</code></pre>"},{"location":"challenging-sql-problems/solutions/silver/bannable-login-activity/#solution","title":"Solution","text":"<p>Some SQL solutions per database are provided below.</p> <p>DuckDB</p> <pre><code>with event_groups as (\n    select\n        *,\n        (0\n            + row_number() over (partition by user_id             order by event_id)\n            - row_number() over (partition by user_id, event_type order by event_id)\n        ) as event_group,\n    from events\n)\n\nselect\n    user_id,\n    event_datetime::date as ban_date\nfrom event_groups\nwhere event_type = 'login failed'\ngroup by\n    user_id,\n    ban_date,\n    event_group\nqualify 3 = sum((count(*) &gt;= 3)::int) over (\n    partition by user_id\n    order by ban_date range interval '2 days' preceding\n)\norder by user_id\n</code></pre> <p>SQL Server</p> <pre><code>with\n\nevent_groups as (\n    select\n        *,\n        cast(event_datetime as date) as event_date,\n        (0\n            + row_number() over (partition by user_id             order by event_id)\n            - row_number() over (partition by user_id, event_type order by event_id)\n        ) as event_group\n    from events\n),\n\nlogin_failed_events as (\n    select\n        user_id,\n        event_date,\n        event_group,\n        iif(count(*) &gt;= 3, 1, 0) as three_login_failures_flag\n    from event_groups\n    where event_type = 'login failed'\n    group by\n        user_id,\n        event_date,\n        event_group\n)\n\nselect\n    user_id,\n    event_date as ban_date\nfrom login_failed_events\nwhere 3 = (\n    select sum(last_three_days.three_login_failures_flag)\n    from login_failed_events as last_three_days\n    where login_failed_events.user_id = last_three_days.user_id\n      and last_three_days.event_date between dateadd(day, -2, login_failed_events.event_date)\n                                         and login_failed_events.event_date\n)\norder by user_id\n</code></pre>"},{"location":"challenging-sql-problems/solutions/silver/bus-routes/","title":"Bus routes \ud83d\ude8c","text":"<p>Tip</p> <p>Solution to the following problem:</p> <ul> <li>bus-routes.md</li> </ul>"},{"location":"challenging-sql-problems/solutions/silver/bus-routes/#result-set","title":"Result Set","text":"<p>Regardless of the database, the result set should look like:</p> bus_id route 1 Old Street - Cavendish Road - Bakers March - West Quay Stop - Goose Green - Crown Street - Leather Lane 2 Hillside - Fellows Road - Riverside - Laddersmith - Furlong Reach 3 Birch Park - Farfair - Golden Lane - Goose Green - Sailors Rest - Cavendish Road Expand for the DDL <pre><code>solution(bus_id, route) as (\n    values\n        (1, 'Old Street - Cavendish Road - Bakers March - West Quay Stop - Goose Green - Crown Street - Leather Lane'),\n        (2, 'Hillside - Fellows Road - Riverside - Laddersmith - Furlong Reach'),\n        (3, 'Birch Park - Farfair - Golden Lane - Goose Green - Sailors Rest - Cavendish Road')\n)\n</code></pre>"},{"location":"challenging-sql-problems/solutions/silver/bus-routes/#solution","title":"Solution","text":"<p>Some SQL solutions per database are provided below.</p> <p>DuckDB</p> <pre><code>with recursive routes as (\n        select\n            bus_id,\n            from_stop as starting_stop,\n            from_stop as current_stop,\n            from_stop as route\n        from bus_stops\n        where (bus_id, from_stop) in (\n            (1, 'Old Street'),\n            (2, 'Hillside'),\n            (3, 'Birch Park')\n        )\n    union all\n        select\n            routes.bus_id,\n            routes.starting_stop,\n            bus_stops.to_stop,\n            routes.route || ' - ' || bus_stops.to_stop\n        from routes\n            inner join bus_stops\n                on  routes.bus_id = bus_stops.bus_id\n                and routes.current_stop = bus_stops.from_stop\n                and routes.starting_stop != bus_stops.to_stop\n)\n\nselect\n    bus_id,\n    max(route) as route\nfrom routes\ngroup by bus_id\norder by bus_id\n</code></pre> <p>SQL Server</p> <pre><code>with routes as (\n        select\n            bus_id,\n            from_stop as starting_stop,\n            from_stop as current_stop,\n            cast(from_stop as varchar(max)) as route\n        from bus_stops\n        where 0=1\n            or (bus_id = 1 and from_stop = 'Old Street')\n            or (bus_id = 2 and from_stop = 'Hillside')\n            or (bus_id = 3 and from_stop = 'Birch Park')\n    union all\n        select\n            routes.bus_id,\n            routes.starting_stop,\n            bus_stops.to_stop,\n            concat_ws(' - ', routes.route, bus_stops.to_stop)\n        from routes\n            inner join bus_stops as bus_stops\n                on  routes.bus_id = bus_stops.bus_id\n                and routes.current_stop = bus_stops.from_stop\n                and routes.starting_stop != bus_stops.to_stop\n)\n\nselect\n    bus_id,\n    max(route) as route\nfrom routes\ngroup by bus_id\norder by bus_id\n</code></pre>"},{"location":"challenging-sql-problems/solutions/silver/customer-sales-running-totals/","title":"Customer sales running totals \ud83d\udcc8","text":"<p>Tip</p> <p>Solution to the following problem:</p> <ul> <li>customer-sales-running-totals.md</li> </ul>"},{"location":"challenging-sql-problems/solutions/silver/customer-sales-running-totals/#result-set","title":"Result Set","text":"<p>The result set should look like:</p> BalanceDate CustomerID RunningTotal 2014-06-01 11091 1243.5234 2014-06-01 11176 1222.8820 2014-06-01 11287 1115.2109 2014-06-02 11091 1243.5234 2014-06-02 11176 1222.8820 2014-06-02 11287 1115.2109 2014-06-03 11091 1243.5234 2014-06-03 11176 1222.8820 2014-06-03 11287 1115.2109 2014-06-04 11091 1243.5234 2014-06-04 11176 1222.8820 2014-06-04 11287 1115.2109 2014-06-05 11091 1243.5234 2014-06-05 11176 1222.8820 2014-06-05 11287 1158.1733 2014-06-06 11091 1243.5234 2014-06-06 11176 1222.8820 2014-06-06 11287 1158.1733 2014-06-07 11091 1282.1874 2014-06-07 11176 1222.8820 2014-06-07 11287 1158.1733 2014-06-08 11091 1282.1874 2014-06-08 11176 1222.8820 2014-06-08 11287 1158.1733 2014-06-09 11091 1282.1874 2014-06-09 11176 1222.8820 2014-06-09 11287 1158.1733 2014-06-10 11091 1314.2103 2014-06-10 11176 1222.8820 2014-06-10 11287 1158.1733 2014-06-11 11091 1314.2103 2014-06-11 11176 1222.8820 2014-06-11 11287 1158.1733 2014-06-12 11091 1314.2103 2014-06-12 11176 1222.8820 2014-06-12 11287 1158.1733 2014-06-13 11091 1314.2103 2014-06-13 11176 1222.8820 2014-06-13 11287 1158.1733 2014-06-14 11091 1314.2103 2014-06-14 11176 1228.3960 2014-06-14 11287 1191.3012 2014-06-15 11091 1314.2103 2014-06-15 11176 1388.0354 2014-06-15 11287 1191.3012 2014-06-16 11091 1314.2103 2014-06-16 11176 1388.0354 2014-06-16 11287 1191.3012 2014-06-17 11091 1314.2103 2014-06-17 11176 1430.9978 2014-06-17 11287 1191.3012 2014-06-18 11091 1314.2103 2014-06-18 11176 1430.9978 2014-06-18 11287 1191.3012 2014-06-19 11091 1314.2103 2014-06-19 11176 1430.9978 2014-06-19 11287 1191.3012 2014-06-20 11091 1314.2103 2014-06-20 11176 1430.9978 2014-06-20 11287 1191.3012 2014-06-21 11091 1314.2103 2014-06-21 11176 1430.9978 2014-06-21 11287 1191.3012 2014-06-22 11091 1314.2103 2014-06-22 11176 1430.9978 2014-06-22 11287 1191.3012 2014-06-23 11091 1314.2103 2014-06-23 11176 1430.9978 2014-06-23 11287 1191.3012 2014-06-24 11091 1314.2103 2014-06-24 11176 1430.9978 2014-06-24 11287 1191.3012 2014-06-25 11091 1314.2103 2014-06-25 11176 1430.9978 2014-06-25 11287 1191.3012 2014-06-26 11091 1314.2103 2014-06-26 11176 1430.9978 2014-06-26 11287 1191.3012 2014-06-27 11091 1314.2103 2014-06-27 11176 1430.9978 2014-06-27 11287 1191.3012 2014-06-28 11091 1314.2103 2014-06-28 11176 1430.9978 2014-06-28 11287 1191.3012 2014-06-29 11091 1314.2103 2014-06-29 11176 1458.6118 2014-06-29 11287 1191.3012 2014-06-30 11091 1314.2103 2014-06-30 11176 1458.6118 2014-06-30 11287 1289.6131 Expand for the DDL <pre><code>solution(BalanceDate, CustomerID, RunningTotal) as (\n    values\n        ('2014-06-01', 11091, 1243.5234),\n        ('2014-06-01', 11176, 1222.8820),\n        ('2014-06-01', 11287, 1115.2109),\n        ('2014-06-02', 11091, 1243.5234),\n        ('2014-06-02', 11176, 1222.8820),\n        ('2014-06-02', 11287, 1115.2109),\n        ('2014-06-03', 11091, 1243.5234),\n        ('2014-06-03', 11176, 1222.8820),\n        ('2014-06-03', 11287, 1115.2109),\n        ('2014-06-04', 11091, 1243.5234),\n        ('2014-06-04', 11176, 1222.8820),\n        ('2014-06-04', 11287, 1115.2109),\n        ('2014-06-05', 11091, 1243.5234),\n        ('2014-06-05', 11176, 1222.8820),\n        ('2014-06-05', 11287, 1158.1733),\n        ('2014-06-06', 11091, 1243.5234),\n        ('2014-06-06', 11176, 1222.8820),\n        ('2014-06-06', 11287, 1158.1733),\n        ('2014-06-07', 11091, 1282.1874),\n        ('2014-06-07', 11176, 1222.8820),\n        ('2014-06-07', 11287, 1158.1733),\n        ('2014-06-08', 11091, 1282.1874),\n        ('2014-06-08', 11176, 1222.8820),\n        ('2014-06-08', 11287, 1158.1733),\n        ('2014-06-09', 11091, 1282.1874),\n        ('2014-06-09', 11176, 1222.8820),\n        ('2014-06-09', 11287, 1158.1733),\n        ('2014-06-10', 11091, 1314.2103),\n        ('2014-06-10', 11176, 1222.8820),\n        ('2014-06-10', 11287, 1158.1733),\n        ('2014-06-11', 11091, 1314.2103),\n        ('2014-06-11', 11176, 1222.8820),\n        ('2014-06-11', 11287, 1158.1733),\n        ('2014-06-12', 11091, 1314.2103),\n        ('2014-06-12', 11176, 1222.8820),\n        ('2014-06-12', 11287, 1158.1733),\n        ('2014-06-13', 11091, 1314.2103),\n        ('2014-06-13', 11176, 1222.8820),\n        ('2014-06-13', 11287, 1158.1733),\n        ('2014-06-14', 11091, 1314.2103),\n        ('2014-06-14', 11176, 1228.3960),\n        ('2014-06-14', 11287, 1191.3012),\n        ('2014-06-15', 11091, 1314.2103),\n        ('2014-06-15', 11176, 1388.0354),\n        ('2014-06-15', 11287, 1191.3012),\n        ('2014-06-16', 11091, 1314.2103),\n        ('2014-06-16', 11176, 1388.0354),\n        ('2014-06-16', 11287, 1191.3012),\n        ('2014-06-17', 11091, 1314.2103),\n        ('2014-06-17', 11176, 1430.9978),\n        ('2014-06-17', 11287, 1191.3012),\n        ('2014-06-18', 11091, 1314.2103),\n        ('2014-06-18', 11176, 1430.9978),\n        ('2014-06-18', 11287, 1191.3012),\n        ('2014-06-19', 11091, 1314.2103),\n        ('2014-06-19', 11176, 1430.9978),\n        ('2014-06-19', 11287, 1191.3012),\n        ('2014-06-20', 11091, 1314.2103),\n        ('2014-06-20', 11176, 1430.9978),\n        ('2014-06-20', 11287, 1191.3012),\n        ('2014-06-21', 11091, 1314.2103),\n        ('2014-06-21', 11176, 1430.9978),\n        ('2014-06-21', 11287, 1191.3012),\n        ('2014-06-22', 11091, 1314.2103),\n        ('2014-06-22', 11176, 1430.9978),\n        ('2014-06-22', 11287, 1191.3012),\n        ('2014-06-23', 11091, 1314.2103),\n        ('2014-06-23', 11176, 1430.9978),\n        ('2014-06-23', 11287, 1191.3012),\n        ('2014-06-24', 11091, 1314.2103),\n        ('2014-06-24', 11176, 1430.9978),\n        ('2014-06-24', 11287, 1191.3012),\n        ('2014-06-25', 11091, 1314.2103),\n        ('2014-06-25', 11176, 1430.9978),\n        ('2014-06-25', 11287, 1191.3012),\n        ('2014-06-26', 11091, 1314.2103),\n        ('2014-06-26', 11176, 1430.9978),\n        ('2014-06-26', 11287, 1191.3012),\n        ('2014-06-27', 11091, 1314.2103),\n        ('2014-06-27', 11176, 1430.9978),\n        ('2014-06-27', 11287, 1191.3012),\n        ('2014-06-28', 11091, 1314.2103),\n        ('2014-06-28', 11176, 1430.9978),\n        ('2014-06-28', 11287, 1191.3012),\n        ('2014-06-29', 11091, 1314.2103),\n        ('2014-06-29', 11176, 1458.6118),\n        ('2014-06-29', 11287, 1191.3012),\n        ('2014-06-30', 11091, 1314.2103),\n        ('2014-06-30', 11176, 1458.6118),\n        ('2014-06-30', 11287, 1289.6131)\n)\n</code></pre>"},{"location":"challenging-sql-problems/solutions/silver/customer-sales-running-totals/#solution","title":"Solution","text":"<p>The solution for SQL Server is provided below.</p> <p>SQL Server</p> <pre><code>with\n\nDates as (\n        select cast('2014-06-01' as date) as BalanceDate\n    union all\n        select dateadd(day, 1, BalanceDate)\n        from Dates\n        where BalanceDate &lt; '2014-06-30'\n),\n\nCustomerSalesUnioned as (\n        select\n            CustomerID,\n            '2014-05-31' as OrderDate,\n            sum(TotalDue) as TotalDue\n        from Sales.SalesOrderHeader\n        where 1=1\n            and CustomerID in (11176, 11091, 11287)\n            and OrderDate &lt; '2014-06-01'\n        group by CustomerID\n    union all\n        select\n            CustomerID,\n            OrderDate,\n            sum(TotalDue) as TotalDue\n        from Sales.SalesOrderHeader\n        where 1=1\n            and CustomerID in (11176, 11091, 11287)\n            and OrderDate between '2014-06-01' and '2014-06-30'\n        group by\n            CustomerID,\n            OrderDate\n),\n\nCustomerSales as (\n    select\n        CustomerID,\n        OrderDate,\n        -1 + lead(OrderDate, 1, '2014-07-01') over CustomerByOrderDate as NextOrderDate,\n        sum(TotalDue) over CustomerByOrderDate as RunningTotal\n    from CustomerSalesUnioned\n    window CustomerByOrderDate as (\n        partition by CustomerID\n        order by OrderDate\n    )\n)\n\nselect\n    Dates.BalanceDate,\n    CustomerSales.CustomerID,\n    CustomerSales.RunningTotal\nfrom Dates\n    left join CustomerSales\n        on Dates.BalanceDate between CustomerSales.OrderDate and CustomerSales.NextOrderDate\norder by\n    Dates.BalanceDate,\n    CustomerSales.CustomerID\n</code></pre>"},{"location":"challenging-sql-problems/solutions/silver/decoding-datelist-ints/","title":"Decoding datelist ints \ud83d\udd13","text":"<p>Tip</p> <p>Solution to the following problem:</p> <ul> <li>decoding-datelist-ints.md</li> </ul>"},{"location":"challenging-sql-problems/solutions/silver/decoding-datelist-ints/#result-set","title":"Result Set","text":"<p>Regardless of the database, the result set should look like:</p> active_date user_1 user_2 user_3 user_4 user_5 user_6 2024-05-03 0 1 0 0 0 0 2024-05-04 0 1 0 0 0 1 2024-05-05 0 0 1 0 1 1 2024-05-06 0 1 1 0 1 1 2024-05-07 0 1 0 0 1 0 2024-05-08 0 0 0 0 0 1 2024-05-09 0 0 0 1 1 0 2024-05-10 0 0 0 0 0 1 2024-05-11 0 0 0 0 1 0 2024-05-12 1 1 0 1 0 1 2024-05-13 0 0 0 0 1 1 2024-05-14 0 1 0 1 1 1 2024-05-15 0 0 0 0 0 0 2024-05-16 0 0 0 1 1 1 2024-05-17 0 0 0 0 0 0 2024-05-18 0 0 0 0 1 1 2024-05-19 0 1 1 0 1 1 2024-05-20 1 0 0 1 1 0 2024-05-21 1 0 0 0 0 1 2024-05-22 1 0 0 0 0 0 2024-05-23 1 1 0 0 0 0 2024-05-24 0 1 0 1 0 0 2024-05-25 0 0 1 0 0 1 2024-05-26 0 0 1 1 1 1 2024-05-27 0 0 1 1 0 1 2024-05-28 0 1 1 0 1 1 2024-05-29 0 1 1 0 0 0 2024-05-30 0 0 0 0 1 1 2024-05-31 0 0 0 0 1 1 2024-06-01 0 0 0 0 0 1 Expand for the DDL <pre><code>solution(active_date, user_1, user_2, user_3, user_4, user_5, user_6) as (\n    values\n        ('2024-05-03'::date, 0, 1, 0, 0, 0, 0),\n        ('2024-05-04'::date, 0, 1, 0, 0, 0, 1),\n        ('2024-05-05'::date, 0, 0, 1, 0, 1, 1),\n        ('2024-05-06'::date, 0, 1, 1, 0, 1, 1),\n        ('2024-05-07'::date, 0, 1, 0, 0, 1, 0),\n        ('2024-05-08'::date, 0, 0, 0, 0, 0, 1),\n        ('2024-05-09'::date, 0, 0, 0, 1, 1, 0),\n        ('2024-05-10'::date, 0, 0, 0, 0, 0, 1),\n        ('2024-05-11'::date, 0, 0, 0, 0, 1, 0),\n        ('2024-05-12'::date, 1, 1, 0, 1, 0, 1),\n        ('2024-05-13'::date, 0, 0, 0, 0, 1, 1),\n        ('2024-05-14'::date, 0, 1, 0, 1, 1, 1),\n        ('2024-05-15'::date, 0, 0, 0, 0, 0, 0),\n        ('2024-05-16'::date, 0, 0, 0, 1, 1, 1),\n        ('2024-05-17'::date, 0, 0, 0, 0, 0, 0),\n        ('2024-05-18'::date, 0, 0, 0, 0, 1, 1),\n        ('2024-05-19'::date, 0, 1, 1, 0, 1, 1),\n        ('2024-05-20'::date, 1, 0, 0, 1, 1, 0),\n        ('2024-05-21'::date, 1, 0, 0, 0, 0, 1),\n        ('2024-05-22'::date, 1, 0, 0, 0, 0, 0),\n        ('2024-05-23'::date, 1, 1, 0, 0, 0, 0),\n        ('2024-05-24'::date, 0, 1, 0, 1, 0, 0),\n        ('2024-05-25'::date, 0, 0, 1, 0, 0, 1),\n        ('2024-05-26'::date, 0, 0, 1, 1, 1, 1),\n        ('2024-05-27'::date, 0, 0, 1, 1, 0, 1),\n        ('2024-05-28'::date, 0, 1, 1, 0, 1, 1),\n        ('2024-05-29'::date, 0, 1, 1, 0, 0, 0),\n        ('2024-05-30'::date, 0, 0, 0, 0, 1, 1),\n        ('2024-05-31'::date, 0, 0, 0, 0, 1, 1),\n        ('2024-06-01'::date, 0, 0, 0, 0, 0, 1)\n)\n</code></pre>"},{"location":"challenging-sql-problems/solutions/silver/decoding-datelist-ints/#solution","title":"Solution","text":"<p>Some SQL solutions per database are provided below.</p> <p>DuckDB</p> <pre><code>with recursive\n\naxis(active_date, max_, step) as (\n        select\n            (select last_update from user_history limit 1),\n            (select max(log2(activity_history)) from user_history),\n            1\n    union all\n        select active_date - 1, max_, step + 1\n        from axis\n        where step &lt; max_\n),\n\ndecoded as (\n    select\n        user_history.user_id,\n        axis.active_date,\n        (activity_history &amp; power(2, axis.step - 1)::int &gt; 0)::int as active_flag\n    from axis\n        cross join user_history\n)\n\npivot decoded\non ('user_' || user_id)\nusing any_value(active_flag)\norder by active_date\n</code></pre>"},{"location":"challenging-sql-problems/solutions/silver/funnel-analytics/","title":"Funnel analytics \u23ec","text":"<p>Tip</p> <p>Solution to the following problem:</p> <ul> <li>funnel-analytics.md</li> </ul>"},{"location":"challenging-sql-problems/solutions/silver/funnel-analytics/#result-set","title":"Result Set","text":"<p>Regardless of the database, the result set should look like:</p> cohort stage mortgages step_rate total_rate 2024-01 full application 4 100.00 100.00 2024-01 decision 4 100.00 100.00 2024-01 documentation 3 75.00 75.00 2024-01 valuation inspection 3 100.00 75.00 2024-01 valuation made 3 100.00 75.00 2024-01 valuation submitted 3 100.00 75.00 2024-01 solicitation 1 33.33 25.00 2024-01 funds released 1 100.00 25.00 2024-02 full application 6 100.00 100.00 2024-02 decision 6 100.00 100.00 2024-02 documentation 4 66.67 66.67 2024-02 valuation inspection 4 100.00 66.67 2024-02 valuation made 4 100.00 66.67 2024-02 valuation submitted 4 100.00 66.67 2024-02 solicitation 3 75.00 50.00 2024-02 funds released 3 100.00 50.00 2024-03 full application 3 100.00 100.00 2024-03 decision 3 100.00 100.00 2024-03 documentation 1 33.33 33.33 2024-03 valuation inspection 1 100.00 33.33 2024-03 valuation made 1 100.00 33.33 2024-03 valuation submitted 1 100.00 33.33 2024-03 solicitation 0 0.00 0.00 2024-03 funds released 0 0.00 0.00 Expand for the DDL <pre><code>solution(cohort, stage, mortgages, step_rate, total_rate) as (\n    values\n        ('2024-01', 'full application',     4, 100.00, 100.00),\n        ('2024-01', 'decision',             4, 100.00, 100.00),\n        ('2024-01', 'documentation',        3,  75.00,  75.00),\n        ('2024-01', 'valuation inspection', 3, 100.00,  75.00),\n        ('2024-01', 'valuation made',       3, 100.00,  75.00),\n        ('2024-01', 'valuation submitted',  3, 100.00,  75.00),\n        ('2024-01', 'solicitation',         1,  33.33,  25.00),\n        ('2024-01', 'funds released',       1, 100.00,  25.00),\n        ('2024-02', 'full application',     6, 100.00, 100.00),\n        ('2024-02', 'decision',             6, 100.00, 100.00),\n        ('2024-02', 'documentation',        4,  66.67,  66.67),\n        ('2024-02', 'valuation inspection', 4, 100.00,  66.67),\n        ('2024-02', 'valuation made',       4, 100.00,  66.67),\n        ('2024-02', 'valuation submitted',  4, 100.00,  66.67),\n        ('2024-02', 'solicitation',         3,  75.00,  50.00),\n        ('2024-02', 'funds released',       3, 100.00,  50.00),\n        ('2024-03', 'full application',     3, 100.00, 100.00),\n        ('2024-03', 'decision',             3, 100.00, 100.00),\n        ('2024-03', 'documentation',        1,  33.33,  33.33),\n        ('2024-03', 'valuation inspection', 1, 100.00,  33.33),\n        ('2024-03', 'valuation made',       1, 100.00,  33.33),\n        ('2024-03', 'valuation submitted',  1, 100.00,  33.33),\n        ('2024-03', 'solicitation',         0,   0.00,   0.00),\n        ('2024-03', 'funds released',       0,   0.00,   0.00)\n)\n</code></pre>"},{"location":"challenging-sql-problems/solutions/silver/funnel-analytics/#solution","title":"Solution","text":"<p>Some SQL solutions per database are provided below.</p> <p>DuckDB</p> <pre><code>with\n\nstages(stage, step) as (\n    values\n        ('full application',     1),\n        ('decision',             2),\n        ('documentation',        3),\n        ('valuation inspection', 4),\n        ('valuation made',       5),\n        ('valuation submitted',  6),\n        ('solicitation',         7),\n        ('funds released',       8),\n),\n\ncohorts as (\n    select\n        applications.event_id,\n        applications.event_date,\n        applications.mortgage_id,\n        applications.stage,\n        stages.step,\n        first_value(applications.event_date) over (\n            partition by applications.mortgage_id\n            order by stages.step\n        ).strftime('%Y-%m') as cohort,\n    from applications\n        inner join stages\n            using (stage)\n),\n\ncohorts_by_stage as (\n    select\n        cohort,\n        stage,\n        any_value(step) as step,\n        count(*) as cohort_mortgages,\n    from cohorts\n    group by\n        cohort,\n        stage\n),\n\naxis as (\n    select\n        cohort,\n        stages.stage,\n        stages.step\n    from (select distinct cohort from cohorts_by_stage)\n        cross join stages\n),\n\nfunnel as (\n    select\n        axis.cohort,\n        axis.stage,\n        axis.step,\n        coalesce(cohorts_by_stage.cohort_mortgages, 0) as mortgages,\n        lag(mortgages, 1, mortgages) over cohort_by_step as prev_mortgages,\n        first_value(mortgages) over cohort_by_step as first_mortgages,\n    from axis\n        left join cohorts_by_stage\n            using (cohort, step)\n    window cohort_by_step as (\n        partition by axis.cohort\n        order by axis.step\n    )\n)\n\nselect\n    cohort,\n    stage,\n    mortgages,\n    round(100.0 * if(prev_mortgages = 0, 0, mortgages / prev_mortgages), 2) as step_rate,\n    round(100.0 * if(first_mortgages = 0, 0, mortgages / first_mortgages), 2) as total_rate,\nfrom funnel\norder by\n    cohort,\n    step\n</code></pre>"},{"location":"challenging-sql-problems/solutions/silver/mandelbrot-set/","title":"Mandelbrot set \ud83c\udf00","text":"<p>Tip</p> <p>Solution to the following problem:</p> <ul> <li>mandelbrot-set.md</li> </ul>"},{"location":"challenging-sql-problems/solutions/silver/mandelbrot-set/#result-set","title":"Result Set","text":"<p>Regardless of the database, the result set should be a single cell with a value looking something like:</p> <pre><code>                       \u2022\u2022\n                       \u2022\u2022\n                      \u2022\u2022\u2022\n                   \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\n                  \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n                  \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n                 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n           \u2022\u2022\u2022  \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n           \u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n          \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n          \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n           \u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n           \u2022\u2022\u2022  \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n                 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n                  \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n                  \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\n                   \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022\n                      \u2022\u2022\u2022\n                       \u2022\u2022\n                       \u2022\u2022\n</code></pre> Expand for the DDL <pre><code>solution(mandelbrot_set) as (\n    select concat_ws(e'\\n',\n        '                                                   ',\n        '                                                   ',\n        '                                                   ',\n        '                                                   ',\n        '                                                   ',\n        '                                                   ',\n        '                                                   ',\n        '                                                   ',\n        '                                                   ',\n        '                                                   ',\n        '                                                   ',\n        '                                                   ',\n        '                                                   ',\n        '                                                   ',\n        '                                                   ',\n        '                       \u2022\u2022                          ',\n        '                       \u2022\u2022                          ',\n        '                      \u2022\u2022\u2022                          ',\n        '                   \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022                     ',\n        '                  \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022                      ',\n        '                  \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022                     ',\n        '                 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022                    ',\n        '           \u2022\u2022\u2022  \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022                     ',\n        '           \u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022                     ',\n        '          \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022                     ',\n        '\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022                      ',\n        '          \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022                     ',\n        '           \u2022\u2022\u2022\u2022 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022                     ',\n        '           \u2022\u2022\u2022  \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022                     ',\n        '                 \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022                    ',\n        '                  \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022                     ',\n        '                  \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022                      ',\n        '                   \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 \u2022                     ',\n        '                      \u2022\u2022\u2022                          ',\n        '                       \u2022\u2022                          ',\n        '                       \u2022\u2022                          ',\n        '                                                   ',\n        '                                                   ',\n        '                                                   ',\n        '                                                   ',\n        '                                                   ',\n        '                                                   ',\n        '                                                   ',\n        '                                                   ',\n        '                                                   ',\n        '                                                   ',\n        '                                                   ',\n        '                                                   ',\n        '                                                   ',\n        '                                                   ',\n        '                                                   '\n    )\n)\n</code></pre> <p>Success</p> <p>If you want to see a more interesting solution which \"colours\" the points based on how quickly they diverge, you can find it at:</p> <ul> <li>https://thedailywtf.com/articles/stupid-coding-tricks-the-tsql-madlebrot</li> </ul>"},{"location":"challenging-sql-problems/solutions/silver/mandelbrot-set/#solution","title":"Solution","text":"<p>Some SQL solutions per database are provided below.</p> <p>DuckDB</p> <pre><code>with recursive\n\naxis as (\n    select generate_series / 100 as i\n    from generate_series(-200, 200, 8)\n),\n\ngrid as (\n    select x.i as x, y.i as y,\n    from axis as x, axis as y\n),\n\napply_calculation as (\n        /* define the set of complex points (c = (x, y), z = (a, b)) */\n        select\n            x,\n            y,\n            000.0000 as a,\n            000.0000 as b,\n            0 as i,\n        from grid\n    union all\n        /* apply the mandelbrot set formula */\n        select\n            x,\n            y,\n            a^2 - b^2 + x,\n            2 * a * b + y,\n            i + 1,\n        from apply_calculation\n        where 1=1\n            and (abs(a), abs(b)) &lt;= (2, 2)\n            and i &lt; 100\n),\n\nmandelbrot_set as (\n    select\n        grid.x,\n        grid.y,\n        if(apply_calculation.i is not null, '\u2022', ' ') as in_set\n    from grid\n        left join apply_calculation\n            on  grid.x = apply_calculation.x\n            and grid.y = apply_calculation.y\n            and apply_calculation.i = 100\n)\n\nselect string_agg(x_rows, e'\\n' order by y) as mandelbrot_set\nfrom (\n    select\n        y,\n        string_agg(in_set, '' order by x) as x_rows\n    from mandelbrot_set\n    group by y\n)\n</code></pre>"},{"location":"challenging-sql-problems/solutions/silver/metric-correlation/","title":"Metric correlation \ud83d\udd00","text":"<p>Tip</p> <p>Solution to the following problem:</p> <ul> <li>metric-correlation.md</li> </ul>"},{"location":"challenging-sql-problems/solutions/silver/metric-correlation/#result-set","title":"Result Set","text":"<p>Regardless of the database (since they should all use a similar collation), the result set should look like:</p> segment metric_pair correlation 0 metric_1, metric_4 0.8195 1 metric_2, metric_4 0.6741 2 metric_2, metric_4 0.8539 3 metric_3, metric_5 0.9975 4 metric_1, metric_4 0.9122 5 metric_3, metric_5 0.9985 6 metric_3, metric_5 0.9961 7 metric_2, metric_4 0.5686 8 metric_2, metric_4 0.8405 9 metric_1, metric_5 0.9989 10 metric_2, metric_5 0.8042 Expand for the DDL <pre><code>solution(segment, metric_pair, correlation) as (\n    values\n        ( 0, 'metric_1, metric_4', 0.8195),\n        ( 1, 'metric_2, metric_4', 0.6741),\n        ( 2, 'metric_2, metric_4', 0.8539),\n        ( 3, 'metric_3, metric_5', 0.9975),\n        ( 4, 'metric_1, metric_4', 0.9122),\n        ( 5, 'metric_3, metric_5', 0.9985),\n        ( 6, 'metric_3, metric_5', 0.9961),\n        ( 7, 'metric_2, metric_4', 0.5686),\n        ( 8, 'metric_2, metric_4', 0.8405),\n        ( 9, 'metric_1, metric_5', 0.9989),\n        (10, 'metric_2, metric_5', 0.8042)\n)\n</code></pre>"},{"location":"challenging-sql-problems/solutions/silver/metric-correlation/#solution","title":"Solution","text":"<p>Some SQL solutions per database are provided below.</p> <p>DuckDB</p> <pre><code>with\n\nunpivoted as (\n    unpivot metrics\n    on columns(* exclude (segment, customer_id))\n    into\n        name metric_name\n        value metric_value\n),\n\njoined as (\n    select\n        l.customer_id,\n        l.segment,\n        l.metric_name as l_metric_name,\n        r.metric_name as r_metric_name,\n        l.metric_value as l_metric_value,\n        r.metric_value as r_metric_value\n    from unpivoted as l\n        inner join unpivoted as r\n            on  l.customer_id = r.customer_id\n            and l.segment = r.segment\n            and l.metric_name &lt; r.metric_name  /* No point in pairing metrics twice */\n)\n\nselect\n    segment,\n    l_metric_name || ', ' || r_metric_name as metric_pair,\n    round(corr(l_metric_value, r_metric_value), 4) as correlation\nfrom joined\ngroup by segment, l_metric_name, r_metric_name\nqualify correlation = max(correlation) over (partition by segment)\norder by segment, metric_pair\n</code></pre>"},{"location":"challenging-sql-problems/solutions/silver/predicting-values/","title":"Predicting values \ud83c\udfb1","text":"<p>Tip</p> <p>Solution to the following problem:</p> <ul> <li>predicting-values.md</li> </ul>"},{"location":"challenging-sql-problems/solutions/silver/predicting-values/#result-set","title":"Result Set","text":"<p>Regardless of the database, the result set should look like:</p> x dataset_1 dataset_2 dataset_3 dataset_4 16 11.0 11.0 11.0 11.0 17 11.5 11.5 11.5 11.5 18 12.0 12.0 12.0 12.0 <p>This is one of the interesting things about Anscombe's quartet (and is the reason Anscombe created it): the four datasets have the same line of best fit, but look very different when plotted!</p> Expand for the DDL <pre><code>solution(x, dataset_1, dataset_2, dataset_3, dataset_4) as (\n    values\n        (16, 11.0, 11.0, 11.0, 11.0),\n        (17, 11.5, 11.5, 11.5, 11.5),\n        (18, 12.0, 12.0, 12.0, 12.0)\n)\n</code></pre>"},{"location":"challenging-sql-problems/solutions/silver/predicting-values/#solution","title":"Solution","text":"<p>Some SQL solutions per database are provided below.</p> <p>DuckDB (using <code>regr_slope</code> and <code>regr_intercept</code>)</p> <pre><code>with\n\nunpivoted as (\n    unpivot anscombes_quartet\n    on\n        (dataset_1__x, dataset_1__y) as dataset_1,\n        (dataset_2__x, dataset_2__y) as dataset_2,\n        (dataset_3__x, dataset_3__y) as dataset_3,\n        (dataset_4__x, dataset_4__y) as dataset_4,\n    into\n        name dataset\n        value x, y\n),\n\ncoefficients as (\n    select\n        dataset,\n        regr_slope(y, x) as m,\n        regr_intercept(y, x) as c,\n    from unpivoted\n    group by dataset\n),\n\npredictions as (\n    select\n        dataset,\n        x,\n        round(m * x + c, 1) as y\n    from coefficients\n        cross join (values (16), (17), (18)) as v(x)\n)\n\npivot predictions\non dataset\nusing any_value(y)\norder by x\n</code></pre> <p>DuckDB (manual calculations)</p> <pre><code>with\n\nunpivoted as (\n    unpivot anscombes_quartet\n    on\n        (dataset_1__x, dataset_1__y) as dataset_1,\n        (dataset_2__x, dataset_2__y) as dataset_2,\n        (dataset_3__x, dataset_3__y) as dataset_3,\n        (dataset_4__x, dataset_4__y) as dataset_4,\n    into\n        name dataset\n        value x, y\n),\n\ncoefficients as (\n    select\n        dataset,\n        avg(x) as avg_x,\n        avg(y) as avg_y,\n        avg(x * x) as avg_xx,\n        avg(x * y) as avg_xy,\n        (avg_x * avg_y - avg_xy) / (avg_x * avg_x - avg_xx) as m,\n        avg_y - m * avg_x as c,\n    from unpivoted\n    group by dataset\n),\n\npredictions as (\n    select\n        dataset,\n        x,\n        round(m * x + c, 1) as y\n    from coefficients\n        cross join (values (16), (17), (18)) as v(x)\n)\n\npivot predictions\non dataset\nusing any_value(y)\norder by x\n</code></pre>"},{"location":"challenging-sql-problems/solutions/silver/region-precipitation/","title":"Region precipitation \u2614","text":"<p>Tip</p> <p>Solution to the following problem:</p> <ul> <li>region-precipitation.md</li> </ul>"},{"location":"challenging-sql-problems/solutions/silver/region-precipitation/#result-set","title":"Result Set","text":"<p>Regardless of the database, the result set should look like:</p> grid_id average_precipitation AC-27 131.147673 AQ-54 165.693967 AQ-55 142.360421 AQ-56 111.379457 BK-45 58.619428 BK-77 51.46849 BR-18 99.378632 X-17 102.888115 Expand for the DDL <pre><code>solution(grid_id, average_precipitation) as (\n    values\n        ('AC-27', 131.147673),\n        ('AQ-54', 165.693967),\n        ('AQ-55', 142.360421),\n        ('AQ-56', 111.379457),\n        ('BK-45',  58.619428),\n        ('BK-77',  51.468490),\n        ('BR-18',  99.378632),\n        ('X-17',  102.888115)\n)\n</code></pre>"},{"location":"challenging-sql-problems/solutions/silver/region-precipitation/#solution","title":"Solution","text":"<p>Some SQL solutions per database are provided below.</p> <p>DuckDB</p> <pre><code>with\n\nlocations as (\n    select\n        grid_id,\n        split_part(grid_id, '-', 1) as region,\n        split_part(grid_id, '-', 2) as location,\n    from (values\n        ('AC-27'),\n        ('AQ-54'),\n        ('AQ-55'),\n        ('AQ-56'),\n        ('BK-45'),\n        ('BK-77'),\n        ('BR-18'),\n        ('X-17'),\n    ) as v(grid_id)\n),\n\nregion_by_month as (\n    unpivot precipitation\n    on columns(* exclude (grid_id))\n    into\n        name month_name\n        value precipitation\n),\n\naverage_precipitation as (\n    select\n        split_part(grid_id, '-', 1) as region,\n        split_part(grid_id, '-', 2) as location,\n        grouping(region, location) as group_id,  /* 0 - region &amp; location;  1 - region;  3 - total */\n        avg(precipitation) as average_precipitation\n    from region_by_month\n    group by rollup (region, location)\n)\n\nselect\n    locations.grid_id,\n    round(coalesce(\n        region_location.average_precipitation,\n        region_only.average_precipitation,\n        overall.average_precipitation,\n    ), 6) as average_precipitation\nfrom locations\n    left join average_precipitation as region_location\n        on  locations.region = region_location.region\n        and locations.location = region_location.location\n        and region_location.group_id = 0\n    left join average_precipitation as region_only\n        on  locations.region = region_only.region\n        and region_only.group_id = 1\n    left join average_precipitation as overall\n        on overall.group_id = 3\norder by locations.grid_id\n</code></pre> <p>SQL Server</p> <pre><code>with\n\nlocations as (\n    select\n        grid_id,\n        substring(grid_id, 1, -1 + charindex('-', grid_id)) as region,\n        substring(grid_id, 1 + charindex('-', grid_id), 99) as location\n    from (values\n        ('AC-27'),\n        ('AQ-54'),\n        ('AQ-55'),\n        ('AQ-56'),\n        ('BK-45'),\n        ('BK-77'),\n        ('BR-18'),\n        ('X-17')\n    ) as v(grid_id)\n),\n\nregion_by_month as (\n    select\n        grid_id,\n        month_name,\n        precipitation,\n        substring(grid_id, 1, -1 + charindex('-', grid_id)) as region,\n        substring(grid_id, 1 + charindex('-', grid_id), 99) as location\n    from precipitation\n    unpivot (\n        precipitation\n        for month_name in (\n            pr_january,\n            pr_february,\n            pr_march,\n            pr_april,\n            pr_may,\n            pr_june,\n            pr_july,\n            pr_august,\n            pr_september,\n            pr_october,\n            pr_november,\n            pr_december\n        )\n    ) as unpivoted\n),\n\naverage_precipitation as (\n    select\n        region,\n        location,\n        grouping_id(region, location) as group_id,  /* 0 - region &amp; location;  1 - region;  3 - total */\n        avg(precipitation) as average_precipitation\n    from region_by_month\n    group by rollup (region, location)\n)\n\nselect\n    locations.grid_id,\n    round(coalesce(\n        region_location.average_precipitation,\n        region_only.average_precipitation,\n        overall.average_precipitation\n    ), 6) as average_precipitation\nfrom locations\n    left join average_precipitation as region_location\n        on  locations.region = region_location.region\n        and locations.location = region_location.location\n        and region_location.group_id = 0\n    left join average_precipitation as region_only\n        on  locations.region = region_only.region\n        and region_only.group_id = 1\n    left join average_precipitation as overall\n        on overall.group_id = 3\norder by locations.grid_id\n</code></pre>"},{"location":"everything-about-joins/everything-about-joins/","title":"Everything about joins","text":""},{"location":"everything-about-joins/everything-about-joins/#about-this-course","title":"About this course","text":"<p>Success</p> <p>Everything you need to know about SQL joins \ud83c\udf89</p>"},{"location":"everything-about-joins/everything-about-joins/#the-toolsdata-in-this-course","title":"The tools/data in this course","text":"<p>This course will primarily use DuckDB as the database engine, but will also mention SQLite, PostgreSQL, SQL Server, and Snowflake.</p> <p>The data will be made up for the examples.</p>"},{"location":"everything-about-joins/everything-about-joins/#outline","title":"Outline","text":"<ol> <li>Syntax<ol> <li>Join fundamentals: <code>INNER</code>, <code>LEFT</code> (<code>RIGHT</code>), <code>FULL</code>, <code>CROSS</code></li> <li>More fundamentals: <code>USING</code>, <code>NATURAL</code></li> <li>The \"timestamp\" join: <code>ASOF</code></li> <li>The \"filtering\" joins: <code>SEMI</code>, <code>ANTI</code></li> <li>The \"glue\" join: <code>POSITIONAL</code></li> <li>The \"explosive\" join: <code>LATERAL</code></li> <li>SQL-92 rant (ANSI-SQL join syntax)</li> </ol> </li> <li>Under the hood<ol> <li>Joins with an index</li> <li>Does join order matter?</li> <li>Loop joins</li> <li>Hash joins</li> <li>Merge joins</li> <li>Range intersection joins</li> </ol> </li> </ol>"},{"location":"everything-about-joins/syntax/explosive-joins/","title":"The \"explosive\" joins (<code>LATERAL</code>) \ud83d\udca5","text":"<p>Success</p> <p>There are a few implementations of the <code>LATERAL</code> join in SQL databases (and not all are called \"lateral\"!), but they all have the same basic idea: generalising correlated subqueries to explode rows into more rows.</p>"},{"location":"everything-about-joins/syntax/explosive-joins/#syntax","title":"Syntax","text":"<p>The syntax for lateral \"joins\" is different across databases, and some databases don't even call it \"lateral\".</p> <p>In all cases, the lateral join is a generalisation of a correlated subquery, so make sure you have a grasp of those before trying to wrap your head around these!</p> <p>Tip</p> <p>Check out the following page for information on correlated subqueries:</p> <ul> <li>from-excel-to-sql/advanced-concepts/correlated-subqueries.md</li> </ul> <p>In particular, the lateral join \"runs\" the subquery for each row in the left table. The advantage of lateral joins is that we can get multiple columns and multiple rows from the subquery -- hence the \"explosive\" moniker!</p> <p>We'll note the availability of the <code>LATERAL</code> keyword (and equivalents), then jump into the database-specific syntax, and finally provide some examples.</p>"},{"location":"everything-about-joins/syntax/explosive-joins/#availability","title":"Availability","text":"<p>At the time of writing (2024-04-07), the <code>LATERAL</code> join has the following availability:</p> <ul> <li>DuckDB: \u2705 (&gt;=0.9)</li> <li>SQLite: \u274c (but does support normal correlated subqueries)</li> <li>PostgreSQL: \u2705 (&gt;=9.3)</li> <li>SQL Server: \u2705 (but as <code>CROSS APPLY</code>)</li> <li>Snowflake: \u2705</li> </ul> <p>Info</p> <p>For more information on lateral joins, particularly in other databases, see the following slides:</p> <ul> <li>https://www.slideshare.net/slideshow/modern-sql/44086611</li> </ul>"},{"location":"everything-about-joins/syntax/explosive-joins/#duckdb-postgresql-snowflake","title":"DuckDB, PostgreSQL, Snowflake","text":"<p>Each of DuckDB, PostgreSQL, and Snowflake use the same syntax for the lateral join.</p> <p>Although we call it a \"join\", the <code>LATERAL</code> keyword is not used instead of <code>LEFT</code>, <code>INNER</code>, <code>CROSS</code>, etc. Instead, we specify it in front of a correlated subquery.</p> <p>Since it's technically not a join, it's common to see people write lateral joins using non-ANSI join syntax:</p> <pre><code>SELECT *\nFROM left_table, LATERAL (&lt;expr&gt;)\n</code></pre> <p>However, the ANSI syntax is also available:</p> <pre><code>SELECT *\nFROM left_table\n    [CROSS | LEFT] JOIN LATERAL (&lt;expr&gt;)\n</code></pre> <p>Note that, contrary to normal joins, using a lateral join with the non-ANSI syntax or in a <code>CROSS JOIN</code> will drop rows that don't have a result from the subquery! This is why you'll often see <code>LEFT JOIN LATERAL</code> in practice.</p> <p>In particular, since the \"join condition\" is written inside the correlated subquery (it wouldn't be correlated otherwise!), the left lateral joins usually have a dummy true condition in the <code>ON</code> clause. Common choices are <code>1=1</code> or <code>true</code>:</p> <pre><code>SELECT *\nFROM left_table\n    LEFT JOIN LATERAL (&lt;expr&gt;) ON TRUE\n</code></pre> <p>Warning</p> <p>At the time of writing (2024-04-13), Snowflake does not support <code>LEFT JOIN LATERAL</code> and will throw an error if you try to use it.</p>"},{"location":"everything-about-joins/syntax/explosive-joins/#sql-server","title":"SQL Server","text":"<p>Instead of <code>LATERAL</code>, SQL Server provides <code>APPLY</code> as an alternative to <code>JOIN</code> to implement the same concept:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/t-sql/queries/from-transact-sql#using-apply</li> </ul> <p>The syntax is similar to the (ANSI) syntax for normal joins:</p> <pre><code>SELECT *\nFROM left_table\n    [CROSS | OUTER] APPLY (&lt;expr&gt;)\n</code></pre> <p>The <code>CROSS APPLY</code> corresponds to the \"normal\" lateral join which drops rows without a result from the subquery, while the <code>OUTER APPLY</code> corresponds to the <code>LEFT JOIN LATERAL</code>.</p> <p>Tip</p> <p>In SQL Server, <code>APPLY</code> is typically used to \"apply\" a table-valued function to each row of the left table (hence the name of the keyword <code>APPLY</code>).</p>"},{"location":"everything-about-joins/syntax/explosive-joins/#examples","title":"Examples","text":"<p>It's fairly rare to need lateral joins, but they can be a lifesaver when you do!</p> <p>For the examples below, we'll use a <code>customers</code> table and an <code>events</code> table to demonstrate the lateral join.</p> <p>Info</p> <p>This example is inspired by the following:</p> <ul> <li>https://stackoverflow.com/a/28733316/8213085</li> </ul>"},{"location":"everything-about-joins/syntax/explosive-joins/#sample-data","title":"Sample data","text":"<p>Suppose we have two tables, <code>customers</code> and <code>events</code>, with the following data:</p>"},{"location":"everything-about-joins/syntax/explosive-joins/#customers","title":"<code>customers</code>","text":"customer_id name last_review_datetime 1 alice 2023-12-15 03:20:00 2 bob 2024-01-24 03:30:00 3 charlie 2024-02-01 04:10:00 Expand for the object DDL <pre><code>create table customers (\n    customer_id integer primary key,\n    name varchar,\n    last_review_datetime datetime\n);\ninsert into customers (customer_id, name, last_review_datetime)\n    values\n        (1, 'alice', '2023-12-15 03:20:00'),\n        (2, 'bob', '2024-01-24 03:30:00'),\n        (3, 'charlie', '2024-02-01 04:10:00')\n;\n</code></pre>"},{"location":"everything-about-joins/syntax/explosive-joins/#events","title":"<code>events</code>","text":"event_id customer_id event_datetime event_type 1 1 2024-01-01 11:00:00 login 2 1 2024-01-01 12:00:00 logout 3 1 2024-01-03 03:00:00 login failed 4 1 2024-01-03 03:05:00 login 5 2 2024-01-03 10:00:00 login 6 2 2024-01-03 15:00:00 logout 7 1 2024-01-03 23:00:00 logout 8 2 2024-01-04 22:00:00 login failed 9 2 2024-01-04 22:05:00 login 10 3 2024-01-05 20:00:00 login 11 3 2024-01-06 04:00:00 logout 12 2 2024-01-09 15:00:00 logout 13 3 2024-01-11 21:00:00 login 14 1 2024-01-12 12:00:00 login 15 1 2024-01-12 13:00:00 logout 16 1 2024-01-13 03:00:00 login 17 2 2024-01-13 10:00:00 login failed 18 2 2024-01-13 10:05:00 login 19 2 2024-01-13 15:00:00 logout 20 1 2024-01-13 23:00:00 logout 21 2 2024-01-14 22:00:00 login 22 3 2024-01-15 20:00:00 login 23 3 2024-01-16 04:00:00 logout 24 2 2024-01-19 15:00:00 logout 25 3 2024-01-21 21:00:00 login 26 1 2024-01-22 12:00:00 login failed 27 1 2024-01-22 12:05:00 password reset 28 1 2024-01-22 12:10:00 login 29 1 2024-01-22 13:00:00 logout 30 1 2024-01-23 03:00:00 login 31 2 2024-01-23 10:00:00 login 32 2 2024-01-23 15:00:00 logout 33 1 2024-01-23 23:00:00 logout 34 2 2024-01-24 22:00:00 login 35 3 2024-01-25 20:00:00 login 36 3 2024-01-26 04:00:00 logout 37 2 2024-01-29 15:00:00 logout 38 3 2024-01-31 21:00:00 login Expand for the object DDL <pre><code>create table events (\n    event_id integer primary key,\n    customer_id integer,\n    event_datetime datetime,\n    event_type varchar\n);\ninsert into events (event_id, customer_id, event_datetime, event_type)\n    values\n        (1, 1, '2024-01-01 11:00:00', 'login'),\n        (2, 1, '2024-01-01 12:00:00', 'logout'),\n        (3, 1, '2024-01-03 03:00:00', 'login failed'),\n        (4, 1, '2024-01-03 03:05:00', 'login'),\n        (5, 2, '2024-01-03 10:00:00', 'login'),\n        (6, 2, '2024-01-03 15:00:00', 'logout'),\n        (7, 1, '2024-01-03 23:00:00', 'logout'),\n        (8, 2, '2024-01-04 22:00:00', 'login failed'),\n        (9, 2, '2024-01-04 22:05:00', 'login'),\n        (10, 3, '2024-01-05 20:00:00', 'login'),\n        (11, 3, '2024-01-06 04:00:00', 'logout'),\n        (12, 2, '2024-01-09 15:00:00', 'logout'),\n        (13, 3, '2024-01-11 21:00:00', 'login'),\n        (14, 1, '2024-01-12 12:00:00', 'login'),\n        (15, 1, '2024-01-12 13:00:00', 'logout'),\n        (16, 1, '2024-01-13 03:00:00', 'login'),\n        (17, 2, '2024-01-13 10:00:00', 'login failed'),\n        (18, 2, '2024-01-13 10:05:00', 'login'),\n        (19, 2, '2024-01-13 15:00:00', 'logout'),\n        (20, 1, '2024-01-13 23:00:00', 'logout'),\n        (21, 2, '2024-01-14 22:00:00', 'login'),\n        (22, 3, '2024-01-15 20:00:00', 'login'),\n        (23, 3, '2024-01-16 04:00:00', 'logout'),\n        (24, 2, '2024-01-19 15:00:00', 'logout'),\n        (25, 3, '2024-01-21 21:00:00', 'login'),\n        (26, 1, '2024-01-22 12:00:00', 'login failed'),\n        (27, 1, '2024-01-22 12:05:00', 'password reset'),\n        (28, 1, '2024-01-22 12:10:00', 'login'),\n        (29, 1, '2024-01-22 13:00:00', 'logout'),\n        (30, 1, '2024-01-23 03:00:00', 'login'),\n        (31, 2, '2024-01-23 10:00:00', 'login'),\n        (32, 2, '2024-01-23 15:00:00', 'logout'),\n        (33, 1, '2024-01-23 23:00:00', 'logout'),\n        (34, 2, '2024-01-24 22:00:00', 'login'),\n        (35, 3, '2024-01-25 20:00:00', 'login'),\n        (36, 3, '2024-01-26 04:00:00', 'logout'),\n        (37, 2, '2024-01-29 15:00:00', 'logout'),\n        (38, 3, '2024-01-31 21:00:00', 'login')\n;\n</code></pre>"},{"location":"everything-about-joins/syntax/explosive-joins/#the-following-three-events-after-each-customers-last-review","title":"The following three events after each customer's last review","text":"<p>To illustrate the lateral join, we'll find the next three events after each customer's last review. If the customer has no events after their last review, we'll have <code>NULL</code> values for the event columns.</p> <p>Using lateral joins makes sense for this problem because we need to:</p> <ul> <li>Compare information between the <code>customers</code> and <code>events</code> tables</li> <li>Limit the number of events kept per customer</li> <li>\"Explode\" the customer table for each matching row in the <code>events</code> table</li> </ul>"},{"location":"everything-about-joins/syntax/explosive-joins/#sqlite","title":"SQLite","text":"<p>Since SQLite doesn't support lateral joins, we'll show how to solve this problem without using a lateral join:</p> <pre><code>with events_since_review as (\n    select\n        events.event_id,\n        events.event_datetime,\n        events.event_type,\n        events.customer_id,\n        row_number() over (\n            partition by customer_id\n            order by event_datetime\n        ) as event_number\n    from events\n        inner join customers\n            using (customer_id)\n    where events.event_datetime &gt; customers.last_review_datetime\n)\n\nselect\n    customers.customer_id,\n    customers.name as customer_name,\n    events_since_review.event_id,\n    events_since_review.event_datetime,\n    events_since_review.event_type\nfrom customers\n    left join events_since_review\n        on  customers.customer_id = events_since_review.customer_id\n        and events_since_review.event_number &lt;= 3\norder by\n    customers.customer_id,\n    events_since_review.event_datetime\n</code></pre> customer_id customer_name last_review_datetime event_id event_datetime event_type 1 alice 2023-12-15 03:20:00 1 2024-01-01 11:00:00 login 1 alice 2023-12-15 03:20:00 2 2024-01-01 12:00:00 logout 1 alice 2023-12-15 03:20:00 3 2024-01-03 03:00:00 login failed 2 bob 2024-01-24 03:30:00 34 2024-01-24 22:00:00 login 2 bob 2024-01-24 03:30:00 37 2024-01-29 15:00:00 logout 3 charlie 2024-02-01 04:10:00 null null null <p>To write a query that meets the requirements, we need to:</p> <ol> <li>Join the <code>customers</code> table to the <code>events</code> table so that we can compare the last review date with the event date</li> <li>Use the <code>ROW_NUMBER()</code> window function to number the events for each customer so that we can limit the number of events to three</li> <li>Join the results of the previous two steps back onto the <code>customers</code> table to account for customers who have no events after their last review</li> </ol> <p>We'll see that the lateral join makes this query a bit simpler.</p>"},{"location":"everything-about-joins/syntax/explosive-joins/#duckdb-and-postgresql","title":"DuckDB and PostgreSQL","text":"<p>For DuckDB and PostgreSQL, we can use the lateral join to achieve the same result:</p> <pre><code>select\n    customers.customer_id,\n    customers.name as customer_name,\n    customers.last_review_datetime,\n    events_since_review.event_id,\n    events_since_review.event_datetime,\n    events_since_review.event_type\nfrom customers\n    left join lateral (\n        select *\n        from events\n        where 1=1\n            and customers.customer_id = events.customer_id\n            and customers.last_review_datetime &lt; events.event_datetime\n        order by events.event_datetime\n        limit 3\n    ) as events_since_review on 1=1\norder by\n    customers.customer_id,\n    events_since_review.event_datetime\n</code></pre> <p>We do the relevant filtering in the subquery, which is run for each row in the <code>customers</code> table. The <code>left join</code> ensures that we don't drop any customers, and the <code>limit 3</code> is applied per customer to keep at most three events per customer.</p> <p>Question</p> <p>Given the Snowflake doesn't (currently) support <code>LEFT JOIN LATERAL</code>, how would you solve this for Snowflake?</p>"},{"location":"everything-about-joins/syntax/explosive-joins/#sql-server_1","title":"SQL Server","text":"<p>The SQL Server version would be very similar to the DuckDB and PostgreSQL versions, but with <code>OUTER APPLY</code> instead of <code>LEFT JOIN LATERAL</code> (and other minor tweaks for SQL Server syntax):</p> <pre><code>select\n    customers.customer_id,\n    customers.name as customer_name,\n    customers.last_review_datetime,\n    events_since_review.event_id,\n    events_since_review.event_datetime,\n    events_since_review.event_type\nfrom customers\n    outer apply (\n        select top 3 *\n        from events\n        where 1=1\n            and customers.customer_id = events.customer_id\n            and customers.last_review_datetime &lt; events.event_datetime\n        order by events.event_datetime\n    ) as events_since_review\norder by\n    customers.customer_id,\n    events_since_review.event_datetime\n</code></pre>"},{"location":"everything-about-joins/syntax/explosive-joins/#wrap-up","title":"Wrap up","text":"<p>Given that subqueries are one of SQL's most powerful features, lateral joins are a natural extension of that power. They're not often needed, but when you do need them, they can make your life much easier!</p>"},{"location":"everything-about-joins/syntax/filtering-joins/","title":"The \"filtering\" joins (<code>SEMI</code>, <code>ANTI</code>) \ud83d\udea6","text":"<p>Success</p> <p>The <code>SEMI</code> and <code>ANTI</code> joins are slightly different from typical joins because they're used to filter the rows from the left table based on the presence or absence of a match in the right table.</p>"},{"location":"everything-about-joins/syntax/filtering-joins/#syntax","title":"Syntax","text":"<p>The terms \"semi-join\" and \"anti-join\" have, historically, been used to describe the <code>IN</code> and <code>NOT IN</code> (or <code>EXISTS</code> and <code>NOT EXISTS</code>) operators, respectively.</p> <p>For example, it was common to refer to a query like the following as one that implements a \"semi-join\":</p> <pre><code>select *\nfrom left_table\nwhere exists (\n    select *\n    from right_table\n    where right_table.id_column = left_table.id_column\n)\n</code></pre> <p>At the time of writing (2024-04-07), DuckDB is one of the few databases to have introduced the <code>SEMI</code> and <code>ANTI</code> join types to make these types of queries more explicit and easier to read.</p> <pre><code>SELECT *\nFROM left_table\n    [SEMI | ANTI] JOIN right_table\n        ON left_table.id_column = right_table.id_column\n</code></pre>"},{"location":"everything-about-joins/syntax/filtering-joins/#availability","title":"Availability","text":"<p>At the time of writing (2024-04-07), the <code>SEMI</code> and <code>ANTI</code> joins (as explicit join types) have the following availability:</p> <ul> <li>DuckDB: \u2705 (&gt;=0.8)</li> <li>SQLite: \u274c</li> <li>PostgreSQL: \u274c</li> <li>SQL Server: \u274c</li> <li>Snowflake: \u274c</li> </ul> <p>Are you aware of any other databases that support the <code>SEMI</code> and <code>ANTI</code> joins (as explicit join types)?</p>"},{"location":"everything-about-joins/syntax/filtering-joins/#examples","title":"Examples","text":""},{"location":"everything-about-joins/syntax/filtering-joins/#sample-data","title":"Sample data","text":"<p>Suppose we have two tables, <code>loans</code> and <code>credit_cards</code>, with the following data:</p>"},{"location":"everything-about-joins/syntax/filtering-joins/#loans","title":"<code>loans</code>","text":"loan_id loan_value customer_id 1 1000.00 1 2 2000.00 1 3 3000.00 3 4 4000.00 6 5 5000.00 7 Expand for the object DDL <pre><code>create or replace table loans (\n    loan_id int,\n    loan_value decimal(10, 2),\n    customer_id int,\n    constraint pk__loans primary key (loan_id),\n);\ninsert into loans\n    values\n        (1, 1000.0, 1),\n        (2, 2000.0, 1),\n        (3, 3000.0, 3),\n        (4, 4000.0, 6),\n        (5, 5000.0, 7),\n;\n</code></pre>"},{"location":"everything-about-joins/syntax/filtering-joins/#credit_cards","title":"<code>credit_cards</code>","text":"credit_card_id credit_limit customer_id 1 1000.00 1 2 2000.00 2 3 2000.00 4 4 3000.00 5 5 4000.00 6 Expand for the object DDL <pre><code>create or replace table credit_cards (\n    credit_card_id int,\n    credit_limit decimal(10, 2),\n    customer_id int,\n    constraint pk__credit_cards primary key (credit_card_id),\n);\ninsert into credit_cards\n    values\n        (1, 1000.0, 1),\n        (2, 2000.0, 2),\n        (3, 2000.0, 4),\n        (4, 3000.0, 5),\n        (5, 4000.0, 6),\n;\n</code></pre>"},{"location":"everything-about-joins/syntax/filtering-joins/#loan-details-for-customers-who-have-a-loan-and-a-credit-card","title":"Loan details for customers who have a loan and a credit card","text":""},{"location":"everything-about-joins/syntax/filtering-joins/#solution","title":"Solution","text":"<p>To get the loan details for customers who have a loan and a credit card, we can use a <code>SEMI</code> join with the <code>loans</code> table on the left and the <code>credit_cards</code> table on the right:</p> <pre><code>select *\nfrom loans\n    semi join credit_cards\n        using (customer_id)\n</code></pre> loan_id loan_value customer_id 1 1000.00 1 2 2000.00 1 4 4000.00 6 <p>Observe that we only get the <code>loans</code> columns in the result set despite using an unqualified <code>*</code>. This is because the <code>SEMI</code> (and <code>ANTI</code>) join is purely for filtering the rows: it doesn't add any columns to the result set!</p> <p>Exercise</p> <p>Can you write a query to get the credit card details for customers who have a loan and a credit card?</p>"},{"location":"everything-about-joins/syntax/filtering-joins/#traditional-solutions","title":"\"Traditional\" solutions","text":"<p>For comparison, here are a few solutions that illustrate how you might solve this problem without the <code>SEMI</code> join.</p>"},{"location":"everything-about-joins/syntax/filtering-joins/#using-exists","title":"Using <code>EXISTS</code>","text":"<p>Historically, the <code>EXISTS</code> operator has been used to implement a \"semi-join\" in SQL:</p> <pre><code>select *\nfrom loans\nwhere exists (\n    select *\n    from credit_cards\n    where credit_cards.customer_id = loans.customer_id\n)\n</code></pre>"},{"location":"everything-about-joins/syntax/filtering-joins/#using-in","title":"Using <code>IN</code>","text":"<p>Using <code>IN</code> is the most common way to solve this problem, mainly because more people know about <code>IN</code> than <code>EXISTS</code>:</p> <pre><code>select *\nfrom loans\nwhere customer_id in (\n    select customer_id\n    from credit_cards\n)\n</code></pre> <p>However, it's worth noting that <code>EXISTS</code> is typically more performant than <code>IN</code> when the subquery returns a large number of rows.</p>"},{"location":"everything-about-joins/syntax/filtering-joins/#loan-details-for-customers-who-have-a-loan-but-no-credit-card","title":"Loan details for customers who have a loan but no credit card","text":""},{"location":"everything-about-joins/syntax/filtering-joins/#solution_1","title":"Solution","text":"<p>To get the loan details for customers who have a loan but no credit card, we can use an <code>ANTI</code> join with the <code>loans</code> table on the left and the <code>credit_cards</code> table on the right:</p> <pre><code>select *\nfrom loans\n    anti join credit_cards\n        using (customer_id)\n</code></pre> loan_id loan_value customer_id 3 3000.00 3 5 5000.00 7 <p>Simple!</p> <p>Exercise</p> <p>Can you write a query to get the credit card details for customers who have a loan but no credit card?</p>"},{"location":"everything-about-joins/syntax/filtering-joins/#traditional-solutions_1","title":"\"Traditional\" solutions","text":"<p>For comparison, here are a few solutions that illustrate how you might solve this problem without the <code>ANTI</code> join.</p>"},{"location":"everything-about-joins/syntax/filtering-joins/#using-not-exists","title":"Using <code>NOT EXISTS</code>","text":"<p>Similar to <code>EXISTS</code>, the <code>NOT EXISTS</code> operator has historically been used to implement an \"anti-join\" in SQL:</p> <pre><code>select *\nfrom loans\nwhere not exists (\n    select *\n    from credit_cards\n    where credit_cards.customer_id = loans.customer_id\n)\n</code></pre>"},{"location":"everything-about-joins/syntax/filtering-joins/#using-not-in","title":"Using <code>NOT IN</code>","text":"<p>Similarly, <code>NOT IN</code> is the most common way to solve this problem but would also suffer from the same performance issues as <code>IN</code> when the data is sufficiently large:</p> <pre><code>select *\nfrom loans\nwhere customer_id not in (\n    select customer_id\n    from credit_cards\n)\n</code></pre>"},{"location":"everything-about-joins/syntax/filtering-joins/#wrap-up","title":"Wrap up","text":"<p>The <code>SEMI</code> and <code>ANTI</code> joins are a nice addition to the SQL language, making it easier to write and read queries that filter rows based on the presence or absence of a match in another table. However, they're only for filtering and not for adding columns to the result set, which could be surprising if you haven't used them before.</p>"},{"location":"everything-about-joins/syntax/glue-joins/","title":"The \"glue\" join (<code>POSITIONAL</code>) \ud83d\udcce","text":"<p>Success</p> <p>The <code>POSITIONAL</code> join just \"glues\" the rows from the left and right tables together, without any matching condition!</p>"},{"location":"everything-about-joins/syntax/glue-joins/#syntax","title":"Syntax","text":"<p>The <code>POSITIONAL</code> join is most similar to the <code>CROSS</code> join as it doesn't require a matching condition. The difference is that the <code>CROSS</code> join will join every row from the left table with every row from the right table, whereas the <code>POSITIONAL</code> join will join the rows based on their position in the tables.</p> <pre><code>SELECT *\nFROM left_table\n    POSITIONAL JOIN right_table\n</code></pre> <p>At the time of writing (2024-04-07), DuckDB is one of the few databases to have introduced the <code>POSITIONAL</code> join type. This is because it's very uncommon to join tables based on their positions in relational tables, mostly because the order of rows in a table is not guaranteed!</p> <p>However, DuckDB supports this join type because DuckDB often uses imported tables such as data frames or CSV files where the order of rows is guaranteed.</p> <p>In particular, the <code>POSITIONAL</code> join is the SQL equivalent of Pandas' <code>DataFrame.join</code> method:</p> <ul> <li>https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.join.html</li> </ul>"},{"location":"everything-about-joins/syntax/glue-joins/#availability","title":"Availability","text":"<p>At the time of writing (2024-04-07), the <code>POSITIONAL</code> join has the following availability:</p> <ul> <li>DuckDB: \u2705 (&gt;=0.6)</li> <li>SQLite: \u274c</li> <li>PostgreSQL: \u274c</li> <li>SQL Server: \u274c</li> <li>Snowflake: \u274c</li> </ul> <p>Are you aware of any other databases that support the <code>POSITIONAL</code> join?</p>"},{"location":"everything-about-joins/syntax/glue-joins/#examples","title":"Examples","text":"<p>The <code>POSITIONAL</code> join has very specific use cases that we won't cover here, so we'll just show a simple example to illustrate the syntax:</p> <pre><code>with\n\nwords(word) as (values ('hello'), ('world')),\nnumbers(number) as (values (1), (2), (3))\n\nselect *\nfrom words\n    positional join numbers\n</code></pre> word number hello 1 world 2 null 3 <p>Note that the third row has a <code>NULL</code> value for the <code>word</code> column because there was no third row in the <code>words</code> table to join with the third row in the <code>numbers</code> table.</p>"},{"location":"everything-about-joins/syntax/join-fundamentals/","title":"Join fundamentals \ud83e\udde9","text":"<p>Success</p> <p>The bits that everyone knows \ud83d\ude0b</p>"},{"location":"everything-about-joins/syntax/join-fundamentals/#the-fundamental-joins","title":"The \"fundamental\" joins","text":"<p>If you're reading this, you probably know what these are. The fundamental SQL joins are:</p> <ul> <li><code>CROSS JOIN</code></li> <li><code>INNER JOIN</code></li> <li><code>LEFT JOIN</code></li> <li><code>RIGHT JOIN</code></li> <li><code>FULL JOIN</code></li> </ul> <p>All SQL databases support the first four, and most support the <code>FULL JOIN</code> as well.</p>"},{"location":"everything-about-joins/syntax/join-fundamentals/#cross-join","title":"<code>CROSS JOIN</code>","text":"<p>It's not common to see <code>CROSS JOIN</code>, but it can be useful in some scenarios.</p> <p>The <code>CROSS JOIN</code> has the simplest syntax:</p> <pre><code>SELECT *\nFROM left_table\n    CROSS JOIN right_table\n</code></pre> <p>There are no join conditions for a <code>CROSS JOIN</code> as it just joins every row from the left table with every row from the right table.</p> <p>These joins are equivalent to the following non-ANSI SQL syntax (more on this in the SQL-92 rant):</p> <pre><code>SELECT *\nFROM left_table, right_table\n</code></pre>"},{"location":"everything-about-joins/syntax/join-fundamentals/#inner-join","title":"<code>INNER JOIN</code>","text":"<p>The <code>INNER JOIN</code> is one of the most common join types.</p> <p>Unlike the <code>CROSS JOIN</code>, the <code>INNER JOIN</code> requires a join condition to match rows from the left table with rows from the right table:</p> <pre><code>SELECT *\nFROM left_table\n    INNER JOIN right_table\n        ON left_table.id_column = right_table.id_column\n</code></pre> <p>The \"feature\" of the <code>INNER JOIN</code> is that it only returns rows where there is a match between the left and right tables.</p> <p>Note that the <code>INNER</code> keyword is optional in most databases. If you don't specify it, the database will assume you mean an <code>INNER JOIN</code>:</p> <pre><code>SELECT *\nFROM left_table\n    JOIN right_table  /* Still an inner join */\n        ON left_table.id_column = right_table.id_column\n</code></pre>"},{"location":"everything-about-joins/syntax/join-fundamentals/#left-join-and-right-join","title":"<code>LEFT JOIN</code> (and <code>RIGHT JOIN</code>)","text":"<p>The <code>LEFT JOIN</code> is the other most common join type.</p> <p>Like the <code>INNER JOIN</code>, the <code>LEFT JOIN</code> also requires a join condition:</p> <pre><code>SELECT *\nFROM left_table\n    LEFT JOIN right_table\n        ON left_table.id_column = right_table.id_column\n</code></pre> <p>The \"feature\" of the <code>LEFT JOIN</code> is that it keeps all rows from the left table, even if there is no match in the right table.</p> <p>Although it's extremely uncommon, the <code>RIGHT JOIN</code> is the same as the <code>LEFT JOIN</code> except it keeps all rows from the right table:</p> <pre><code>SELECT *\nFROM left_table\n    RIGHT JOIN right_table\n        ON left_table.id_column = right_table.id_column\n</code></pre> <p>These two joins can optionally specify the <code>OUTER</code> keyword, but it's not necessary:</p> <pre><code>SELECT *\nFROM left_table\n    [LEFT | RIGHT] OUTER JOIN right_table\n        ON left_table.id_column = right_table.id_column\n</code></pre>"},{"location":"everything-about-joins/syntax/join-fundamentals/#left-or-right","title":"Left or right?","text":"<p>Any <code>RIGHT JOIN</code> can be rewritten as a <code>LEFT JOIN</code> by swapping the tables around which is why <code>RIGHT JOIN</code> is very uncommon.</p> <p>In older versions of some databases, the join order had performance implications which meant that using <code>RIGHT JOIN</code> could offer performance benefits, but this is rarely the case now.</p> <p>If in doubt, use <code>LEFT JOIN</code>!</p>"},{"location":"everything-about-joins/syntax/join-fundamentals/#full-join","title":"<code>FULL JOIN</code>","text":"<p>The <code>FULL JOIN</code> isn't supported by all databases, but it's a combination of the <code>LEFT JOIN</code> and <code>RIGHT JOIN</code>.</p> <p>Like <code>INNER JOIN</code> and <code>LEFT JOIN</code>, the <code>FULL JOIN</code> requires a join condition:</p> <pre><code>SELECT *\nFROM left_table\n    FULL JOIN right_table\n        ON left_table.id_column = right_table.id_column\n</code></pre> <p>The \"feature\" of the <code>FULL JOIN</code> is that it keeps all rows from both tables, even if there is no match in the other table.</p>"},{"location":"everything-about-joins/syntax/join-fundamentals/#availability","title":"Availability","text":"<p>At the time of writing (2024-04-07), the <code>FULL JOIN</code> has the following availability:</p> <ul> <li>DuckDB: \u2705</li> <li>SQLite: \u2705 (&gt;=3.39.0)</li> <li>PostgreSQL: \u2705</li> <li>SQL Server: \u2705</li> <li>Snowflake: \u2705</li> </ul>"},{"location":"everything-about-joins/syntax/join-fundamentals/#join-syntax","title":"Join syntax","text":""},{"location":"everything-about-joins/syntax/join-fundamentals/#multiple-joins","title":"Multiple joins","text":"<p>This is another thing you probably know, but you can join more than two tables in a single query:</p> <pre><code>SELECT *\nFROM table_1\n    INNER JOIN table_2\n        ON table_1.id_column = table_2.id_column\n    LEFT JOIN table_3\n        ON table_2.id_column = table_3.id_column\n</code></pre> <p>You don't have to use the same join type for all the joins, and you can mix and match as you like.</p> <p>You choose the order of the joins which sometimes has an impact on performance, and you have to put one join after another if the second join depends on the first join.</p>"},{"location":"everything-about-joins/syntax/join-fundamentals/#joins-with-subqueries","title":"Joins with subqueries","text":"<p>You can also join tables with subqueries:</p> <pre><code>SELECT *\nFROM table_1\n    INNER JOIN (\n        SELECT *\n        FROM table_2\n        WHERE column_2 &gt; 100\n    ) AS subquery\n        ON table_1.id_column = subquery.id_column\n</code></pre> <p>Unless the subquery is very basic (or you're a software developer using SQL in a transactional environment), it's typically a good idea to use a CTE instead of a subquery:</p> <pre><code>WITH subquery AS (\n    SELECT *\n    FROM table_2\n    WHERE column_2 &gt; 100\n)\n\nSELECT *\nFROM table_1\n    INNER JOIN subquery\n        ON table_1.id_column = subquery.id_column\n</code></pre>"},{"location":"everything-about-joins/syntax/join-fundamentals/#joins-to-the-same-tables","title":"Joins to the same table(s)","text":"<p>There's no limit to the number of times you can join a table in a query. You just need to alias the table each time you join it so that you can refer to each instance explicitly:</p> <pre><code>SELECT *\nFROM base_table AS base1\n    INNER JOIN base_table AS base2\n        ON base1.id_column = base2.id_column\n    LEFT JOIN other_table AS other1\n        ON base1.id_column = other1.id_column_1\n    LEFT JOIN other_table AS other2\n        ON base2.id_column = other2.id_column_2\n</code></pre>"},{"location":"everything-about-joins/syntax/join-fundamentals/#join-condition-syntax","title":"Join condition syntax","text":"<p>Although the examples above only use a single join condition, you can have multiple join conditions in a join:</p> <pre><code>SELECT *\nFROM left_table\n    LEFT JOIN right_table\n        ON  left_table.column_1 = right_table.column_1\n        AND left_table.column_2 = right_table.column_2\n        AND left_table.column_3 = right_table.column_3\n</code></pre> <p>The conditions don't have to be equality conditions either. You can use any condition you like:</p> <pre><code>SELECT *\nFROM left_table\n    LEFT JOIN right_table\n        ON 0=1\n        OR (1=1\n            AND left_table.column_1 = right_table.column_1\n            AND left_table.column_2 &lt; right_table.column_2\n        )\n        OR left_table.column_3 BETWEEN right_table.column_3\n                                   AND right_table.column_4\n</code></pre> <p>Note that these are all conditions for the join: they're not filters on the result set.</p> <p>These conditions just determine which rows from the left table are matched with which rows from the right table. This is why equality is usually how you join tables, but it's not a requirement.</p> <p>Anything that is valid inside a <code>WHERE</code> clause is valid inside a join's <code>ON</code> condition!</p>"},{"location":"everything-about-joins/syntax/more-fundamentals/","title":"More fundamentals \ud83d\udcda","text":"<p>Success</p> <p>The \"fundamental\" joins can be extended with a few more concepts that are common in most SQL databases.</p>"},{"location":"everything-about-joins/syntax/more-fundamentals/#more-fundamental-stuff","title":"More \"fundamental\" stuff","text":"<p>There are two ways to adjust the \"fundamental\" joins, which impacts how you write the join conditions:</p> <ul> <li>Modifying the join type with <code>NATURAL</code></li> <li>Writing <code>USING</code> instead of <code>ON</code></li> </ul> <p>In both cases, the goal is to simplify the join conditions and make the SQL easier to read.</p>"},{"location":"everything-about-joins/syntax/more-fundamentals/#natural","title":"<code>NATURAL</code>","text":"<p>Warning</p> <p>Although most databases support the <code>NATURAL</code> keyword, it's generally considered a bad idea to use it.</p> <p>The <code>NATURAL</code> keyword is a join modifier; it can optionally be written before the inner and outer join types:</p> <ul> <li><code>NATURAL INNER JOIN</code></li> <li><code>NATURAL LEFT JOIN</code></li> <li><code>NATURAL RIGHT JOIN</code></li> <li><code>NATURAL FULL JOIN</code></li> </ul> <p>The \"feature\" of the <code>NATURAL</code> join modifier is that you don't then need to specify the join condition. Instead, the database will automatically match columns with the same name in both tables.</p> <p>This is why it's generally considered a bad idea to use <code>NATURAL</code> joins:</p> <ol> <li>It's not always clear which columns are being matched</li> <li>It can lead to unexpected results if the column names change, or if new columns are added to the tables</li> </ol> <p>To illustrate an example anyway, consider the following simple (and silly) tables, <code>forenames</code> and <code>surnames</code>:</p>"},{"location":"everything-about-joins/syntax/more-fundamentals/#forenames","title":"<code>forenames</code>","text":"id forename 1 alice 2 bob 3 charlie"},{"location":"everything-about-joins/syntax/more-fundamentals/#surnames","title":"<code>surnames</code>","text":"id surname 1 jones 2 smith 4 lee <p>We could write a natural inner join query like this:</p> <pre><code>select *\nfrom forenames\n    natural inner join surnames\n</code></pre> id forename surname 1 alice jones 2 bob smith <p>This has automatically matched the <code>id</code> columns in both tables, and returned the rows where the <code>id</code> values are the same.</p> <p>Similarly, we could write a natural full join query like this:</p> <pre><code>select *\nfrom forenames\n    natural full join surnames\n</code></pre> id forename surname 1 alice jones 2 bob smith 3 charlie null 4 null lee <p>Notice that, since the <code>id</code> column was matched, the output only has one <code>id</code> column which has automatically taken the non-null values from both tables. This saves us from having to write <code>COALESCE</code> to handle the <code>NULL</code> values!</p>"},{"location":"everything-about-joins/syntax/more-fundamentals/#availability","title":"Availability","text":"<p>At the time of writing (2024-04-07), the <code>NATURAL</code> join modifier has the following availability:</p> <ul> <li>DuckDB: \u2705</li> <li>SQLite: \u2705</li> <li>PostgreSQL: \u2705</li> <li>SQL Server: \u274c</li> <li>Snowflake: \u2705</li> </ul>"},{"location":"everything-about-joins/syntax/more-fundamentals/#using","title":"<code>USING</code>","text":"<p>The <code>USING</code> keyword is another way to simplify the join condition, and is far more common than <code>NATURAL</code>.</p> <p>Rather than specifying the join condition with <code>ON</code>, you can specify the column(s) to join on in the <code>USING</code> clause:</p> <pre><code>select *\nfrom forenames\n    inner join surnames\n        using (id)\n</code></pre> id forename surname 1 alice jones 2 bob smith <p>This is helpful when the column names are the same in both tables, and you want to avoid repeating the column names in the <code>ON</code> clause. You can include as many columns as you want in the <code>USING</code> clause, separated by commas.</p> <p>Like with <code>NATURAL</code>, the columns specified in the <code>USING</code> clause will only be returned once in the output if you use a <code>SELECT *</code> with the non-null values from the tables!</p> <p>Even better, if you want to specify columns explicitly in the output, by not prefixing the columns that are in the <code>USING</code> clause with the table name, the database will automatically keep the non-null values from both tables:</p> <pre><code>select\n    id,\n    forenames.forename,\n    surnames.surname\nfrom forenames\n    full join surnames\n        using (id)\n</code></pre> id forename surname 1 alice jones 2 bob smith 3 charlie null 4 null lee"},{"location":"everything-about-joins/syntax/more-fundamentals/#availability_1","title":"Availability","text":"<p>At the time of writing (2024-04-07), the <code>USING</code> keyword has the following availability:</p> <ul> <li>DuckDB: \u2705</li> <li>SQLite: \u2705</li> <li>PostgreSQL: \u2705</li> <li>SQL Server: \u274c</li> <li>Snowflake: \u2705</li> </ul>"},{"location":"everything-about-joins/syntax/sql-92-rant/","title":"SQL-92 (ANSI-SQL join syntax) \ud83d\udcdd","text":"<p>Success</p> <p>The ANSI SQL-92 standard, introduced in 1992, defined the standard syntax for joins in SQL.</p> <p>Success</p> <p>ANSI is the American National Standards Institute, and they are responsible for defining the SQL standard. Note that they aren't responsible for implementing the standard since they don't build SQL software (as part of the ANSI role) -- that's up to the database vendors.</p>"},{"location":"everything-about-joins/syntax/sql-92-rant/#joins-before-sql-92","title":"Joins before SQL-92","text":""},{"location":"everything-about-joins/syntax/sql-92-rant/#non-ansi-join","title":"Non-ANSI join","text":"<p>In the early days of SQL, the <code>JOIN</code> clause didn't exist. Instead, we had to list the tables we wanted to join in the <code>FROM</code> clause and then specify the join condition in the <code>WHERE</code> clause.</p> <p>For example, to join an <code>employees</code> table with a <code>departments</code> tables, we might have written:</p> <pre><code>select *\nfrom employees, departments\nwhere employees.department_id = departments.department_id\n</code></pre> <p>When tables are listed in the <code>FROM</code> clause, they're combined using an equivalent of the <code>CROSS JOIN</code> that we're familiar with today. This is why we specify the join condition in the <code>WHERE</code> clause: we start with all rows paired with all rows and then filter down to only the rows that we want associated.</p> <p>This also has an important implication: this join is specifically an inner join.</p> <p>So, how do we do an outer join?</p>"},{"location":"everything-about-joins/syntax/sql-92-rant/#non-ansi-outer-joins","title":"Non-ANSI outer joins","text":"<p>Performing an outer join prior to SQL-92 syntax was a bit more complicated, and it depended on the database you were using.</p> <p>Note that the \"outer\" joins are the <code>LEFT</code>, <code>RIGHT</code>, and <code>FULL</code> joins that we're familiar with today.</p>"},{"location":"everything-about-joins/syntax/sql-92-rant/#using","title":"Using <code>(+)</code>","text":"<p>Prior to SQL-92 syntax, you could use the <code>(+)</code> operator to indicate that you wanted to perform an outer join. The <code>(+)</code> operator was placed on the side of the join that you wanted to be the \"outer\" side.</p> <p>For example, to perform a left join, you might have written:</p> <pre><code>select *\nfrom employees, departments\nwhere employees.department_id = departments.department_id (+)\n</code></pre> <p>Since the <code>(+)</code> operator was placed on the <code>departments</code> side, this query would return all rows from the <code>employees</code> table, even if there was no matching row in the <code>departments</code> table.</p> <p>This would also work for a right join: just place the <code>(+)</code> operator on the <code>employees</code> side.</p> <p>However, to perform a full join, we can't just use the <code>(+)</code> operator on both sides of the condition. Instead, we have to use the <code>(+)</code> operator in both directions:</p> <pre><code>    select\n        employees.employee_id,\n        employees.department_id,\n        departments.department_name\n    from employees, departments\n    where employees.department_id = departments.department_id (+)\nunion\n    select\n        employees.employee_id,\n        departments.department_id,\n        departments.department_name\n    from employees, departments\n    where departments.department_id = employees.department_id (+)\n</code></pre>"},{"location":"everything-about-joins/syntax/sql-92-rant/#using-and","title":"Using <code>*=</code> and <code>=*</code>","text":"<p>Another syntax used before SQL-92 syntax was to use the <code>*=</code> or <code>=*</code> operators instead of <code>=</code> to indicate an outer join.</p> <p>For these, <code>=*</code> implements a left join and <code>*=</code> implements a right join. Therefore, to perform a left join with these operators, you might have written:</p> <pre><code>select *\nfrom employees, departments\nwhere employees.department_id =* departments.department_id\n</code></pre> <p>The right join and full join would be similar to the <code>(*)</code> versions, but with the <code>*=</code> operator instead.</p>"},{"location":"everything-about-joins/syntax/sql-92-rant/#non-ansi-concepts-influenced-sql-92","title":"Non-ANSI concepts influenced SQL-92","text":"<p>Since SQL joins started off as inner joins, it was convenient at the time to describe what we now understand as <code>LEFT</code>, <code>RIGHT</code>, and <code>FULL</code> joins as \"outer\" joins.</p> <p>This translated into the SQL-92 standard. Each of these joins can optionally include the <code>OUTER</code> keyword:</p> <ul> <li><code>LEFT OUTER JOIN</code></li> <li><code>RIGHT OUTER JOIN</code></li> <li><code>FULL OUTER JOIN</code></li> </ul> <p>This is to acknowledge that these join types are different to the default <code>INNER JOIN</code>.</p> <p>However, thirty years later, the <code>OUTER</code> keyword is redundant. It's still supported in most databases, but there is no value in including it in your queries (in my opinion) because we have more explicit ways of defining the join type.</p> <p>Instead, I recommend that you:</p> <ul> <li>Always write <code>INNER JOIN</code> instead of <code>JOIN</code></li> <li>Always write <code>LEFT JOIN</code> instead of <code>LEFT OUTER JOIN</code></li> <li>Always write <code>RIGHT JOIN</code> instead of <code>RIGHT OUTER JOIN</code></li> <li>Always write <code>FULL JOIN</code> instead of <code>FULL OUTER JOIN</code></li> <li>Prefer <code>LEFT JOIN</code> over <code>RIGHT JOIN</code> unless you have a specific reason to use a right join</li> </ul> <p>This will make your queries more readable and maintainable because the text you write for joins is always two words unless you add the <code>NATURAL</code> or <code>ASOF</code> modifiers, so it's easy to scan for the join type:</p> <ul> <li><code>LEFT JOIN</code></li> <li><code>RIGHT JOIN</code></li> <li><code>FULL JOIN</code></li> <li><code>INNER JOIN</code></li> <li><code>CROSS JOIN</code></li> </ul> <p>When you see three words, you know that the join has a modifier:</p> <ul> <li><code>ASOF LEFT JOIN</code></li> <li><code>NATURAL INNER JOIN</code></li> </ul>"},{"location":"everything-about-joins/syntax/timestamp-joins/","title":"The \"timestamp\" join (<code>ASOF</code>) \u23f1","text":"<p>Success</p> <p>The <code>ASOF</code> join is fantastic for joining two tables with mismatched timestamps, where you want to join on the closest timestamp in the right table to the timestamp in the left table.</p>"},{"location":"everything-about-joins/syntax/timestamp-joins/#syntax","title":"Syntax","text":"<p>At the time of writing (2024-04-07), the syntax for the <code>ASOF</code> join changes depending on the database engine you're using. However, the general idea is the same across all engines: you're joining two tables based on the closest timestamp in the right table to the timestamp in the left table.</p>"},{"location":"everything-about-joins/syntax/timestamp-joins/#duckdb","title":"DuckDB","text":"<p>Note</p> <p>The explanation below is adjusted from the DuckDB documentation.</p> <p>In DuckDB, the <code>ASOF</code> keyword is a join modifier just like <code>NATURAL</code>, which means that you can use it with any join type (<code>left</code>, <code>inner</code>, etc.).</p> <p>The <code>ASOF</code> join requires at least one inequality condition on the ordering field. The inequality can be any inequality condition (<code>&gt;=</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&lt;</code>) on any data type, but the most common form is <code>&gt;=</code> on a temporal type. Any other conditions must be equalities (or <code>NOT DISTINCT</code>). This means that the left/right order of the tables is significant.</p> <pre><code>SELECT *\nFROM left_table\n    ASOF [LEFT | INNER | ...] JOIN right_table\n        ON  left_table.id_column = right_table.id_column\n        AND left_table.timestamp_column &gt;= right_table.timestamp_column\n</code></pre> <p><code>ASOF</code> joins can also specify join conditions on matching column names with the <code>USING</code> syntax, but the last attribute in the list must be the inequality, which will be greater than or equal to (<code>&gt;=</code>):</p> <pre><code>SELECT *\nFROM left_table\n    ASOF [LEFT | INNER | ...] JOIN right_table\n        USING (id_column, timestamp_column)\n</code></pre>"},{"location":"everything-about-joins/syntax/timestamp-joins/#snowflake","title":"Snowflake","text":"<p>Snowflake has a more unique syntax for the <code>ASOF</code> join. Rather than be a join modifier, Snowflake treats it as a new join type and introduces the <code>MATCH_CONDITION</code> clause:</p> <pre><code>SELECT *\nFROM left_table\n    ASOF JOIN right_table\n        MATCH_CONDITION (left_table.timestamp_column &gt;= right_table.timestamp_column)\n        ON left_table.id_column = right_table.id_column\n</code></pre> <p>It has an added restriction that the <code>MATCH_CONDITION</code> clause must reference the left column first, then the right column. This isn't a typical for join conditions, hence calling it out here.</p>"},{"location":"everything-about-joins/syntax/timestamp-joins/#availability","title":"Availability","text":"<p>At the time of writing (2024-04-07), the <code>ASOF</code> join has the following availability:</p> <ul> <li>DuckDB: \u2705 (&gt;=0.6)</li> <li>SQLite: \u274c</li> <li>PostgreSQL: \u274c</li> <li>SQL Server: \u274c</li> <li>Snowflake: \u2705 (February 28, 2024)</li> </ul> <p>Are you aware of any other databases that support the <code>ASOF</code> join?</p>"},{"location":"everything-about-joins/syntax/timestamp-joins/#examples","title":"Examples","text":""},{"location":"everything-about-joins/syntax/timestamp-joins/#sample-data","title":"Sample data","text":"<p>Suppose we have two tables, <code>transactions</code> and <code>exchange_rates</code>, with the following data:</p>"},{"location":"everything-about-joins/syntax/timestamp-joins/#transactions","title":"<code>transactions</code>","text":"date account amount currency 2023-12-31 A 100.00 GBP 2024-01-01 B 200.00 GBP 2024-01-02 C 300.00 GBP 2024-01-04 A 400.00 GBP 2024-01-07 B 500.00 GBP 2024-01-07 C 600.00 GBP 2024-01-08 A 700.00 GBP 2024-01-10 B 800.00 GBP 2024-01-13 C 900.00 GBP 2024-01-13 A 1000.00 GBP 2024-01-17 B 1100.00 GBP 2024-01-17 C 1200.00 GBP 2024-01-18 A 1300.00 GBP 2024-01-20 B 1400.00 GBP 2024-01-23 C 1500.00 GBP 2024-01-23 A 1600.00 GBP 2024-01-27 B 1700.00 GBP 2024-01-27 C 1800.00 GBP 2024-01-28 A 1900.00 GBP 2024-01-30 B 2000.00 GBP 2024-02-02 C 2100.00 GBP 2024-02-02 A 2200.00 GBP 2024-02-06 B 2300.00 GBP 2024-02-06 C 2400.00 GBP Expand for the object DDL <pre><code>create or replace table transactions(\n    \"date\" date,\n    account varchar(8),\n    amount decimal(10, 2),\n    currency varchar(3),\n    constraint pk__transactions primary key (\"date\", account),\n);\ninsert into transactions\n    values\n        ('2023-12-31', 'A', 100, 'GBP'),\n        ('2024-01-01', 'B', 200, 'GBP'),\n        ('2024-01-02', 'C', 300, 'GBP'),\n        ('2024-01-04', 'A', 400, 'GBP'),\n        ('2024-01-07', 'B', 500, 'GBP'),\n        ('2024-01-07', 'C', 600, 'GBP'),\n        ('2024-01-08', 'A', 700, 'GBP'),\n        ('2024-01-10', 'B', 800, 'GBP'),\n        ('2024-01-13', 'C', 900, 'GBP'),\n        ('2024-01-13', 'A', 1000, 'GBP'),\n        ('2024-01-17', 'B', 1100, 'GBP'),\n        ('2024-01-17', 'C', 1200, 'GBP'),\n        ('2024-01-18', 'A', 1300, 'GBP'),\n        ('2024-01-20', 'B', 1400, 'GBP'),\n        ('2024-01-23', 'C', 1500, 'GBP'),\n        ('2024-01-23', 'A', 1600, 'GBP'),\n        ('2024-01-27', 'B', 1700, 'GBP'),\n        ('2024-01-27', 'C', 1800, 'GBP'),\n        ('2024-01-28', 'A', 1900, 'GBP'),\n        ('2024-01-30', 'B', 2000, 'GBP'),\n        ('2024-02-02', 'C', 2100, 'GBP'),\n        ('2024-02-02', 'A', 2200, 'GBP'),\n        ('2024-02-06', 'B', 2300, 'GBP'),\n        ('2024-02-06', 'C', 2400, 'GBP'),\n;\n</code></pre>"},{"location":"everything-about-joins/syntax/timestamp-joins/#exchange_rates","title":"<code>exchange_rates</code>","text":"date from_currency to_currency rate 2024-01-01 GBP INR 110.0000 2024-01-01 GBP JPY 160.0000 2024-01-01 GBP USD 1.3000 2024-02-01 GBP INR 120.0000 2024-02-01 GBP JPY 170.0000 2024-02-01 GBP USD 1.4000 2024-03-01 GBP INR 100.0000 2024-03-01 GBP JPY 150.0000 2024-03-01 GBP USD 1.2000 Expand for the object DDL <pre><code>create or replace table exchange_rates(\n    \"date\" date,\n    from_currency varchar(3),\n    to_currency varchar(3),\n    rate decimal(10, 4),\n    constraint pk__exchange_rates primary key (\"date\", from_currency, to_currency),\n);\ninsert into exchange_rates\n    values\n        ('2024-01-01', 'GBP', 'INR', 110.0),\n        ('2024-01-01', 'GBP', 'JPY', 160.0),\n        ('2024-01-01', 'GBP', 'USD', 1.3),\n        ('2024-02-01', 'GBP', 'INR', 120.0),\n        ('2024-02-01', 'GBP', 'JPY', 170.0),\n        ('2024-02-01', 'GBP', 'USD', 1.4),\n        ('2024-03-01', 'GBP', 'INR', 100.0),\n        ('2024-03-01', 'GBP', 'JPY', 150.0),\n        ('2024-03-01', 'GBP', 'USD', 1.2),\n;\n</code></pre>"},{"location":"everything-about-joins/syntax/timestamp-joins/#finding-the-usd-amount-for-each-transaction-duckdb","title":"Finding the USD amount for each transaction (DuckDB)","text":""},{"location":"everything-about-joins/syntax/timestamp-joins/#solution","title":"Solution","text":"<p>To find the USD amount for each transaction, we can use the <code>ASOF</code> join to find the closest exchange rate to the transaction date:</p> <pre><code>select\n    transactions.date,\n    transactions.account,\n    transactions.amount,\n    exchange_rates.rate,\n    transactions.amount * exchange_rates.rate as amount_usd,\nfrom transactions\n    asof left join exchange_rates\n        on  transactions.currency = exchange_rates.from_currency\n        and exchange_rates.to_currency = 'USD'\n        and transactions.date &gt;= exchange_rates.date\norder by\n    transactions.date,\n    transactions.amount\n</code></pre> date account amount rate amount_usd 2023-12-31 A 100.00 null null 2024-01-01 B 200.00 1.3000 260.000000 2024-01-02 C 300.00 1.3000 390.000000 2024-01-04 A 400.00 1.3000 520.000000 2024-01-07 B 500.00 1.3000 650.000000 2024-01-07 C 600.00 1.3000 780.000000 2024-01-08 A 700.00 1.3000 910.000000 2024-01-10 B 800.00 1.3000 1040.000000 2024-01-13 C 900.00 1.3000 1170.000000 2024-01-13 A 1000.00 1.3000 1300.000000 2024-01-17 B 1100.00 1.3000 1430.000000 2024-01-17 C 1200.00 1.3000 1560.000000 2024-01-18 A 1300.00 1.3000 1690.000000 2024-01-20 B 1400.00 1.3000 1820.000000 2024-01-23 C 1500.00 1.3000 1950.000000 2024-01-23 A 1600.00 1.3000 2080.000000 2024-01-27 B 1700.00 1.3000 2210.000000 2024-01-27 C 1800.00 1.3000 2340.000000 2024-01-28 A 1900.00 1.3000 2470.000000 2024-01-30 B 2000.00 1.3000 2600.000000 2024-02-02 C 2100.00 1.4000 2940.000000 2024-02-02 A 2200.00 1.4000 3080.000000 2024-02-06 B 2300.00 1.4000 3220.000000 2024-02-06 C 2400.00 1.4000 3360.000000 <p>Note the <code>NULL</code> values for the first transaction -- this is because there is no exchange rate for the date of that transaction in our <code>exchange_rates</code> table, and we specified <code>ASOF LEFT JOIN</code> so that values without matches are still kept in the result set (like a normal <code>LEFT JOIN</code>!).</p> <p>Exercise</p> <p>Can you re-write this query using the Snowflake syntax?</p> Expand for the Snowflake equivalent <pre><code>select\n    transactions.date,\n    transactions.account,\n    transactions.amount,\n    exchange_rates.rate,\n    transactions.amount * exchange_rates.rate as amount_usd,\nfrom transactions\n    asof join exchange_rates\n        match_condition (transactions.date &gt;= exchange_rates.date)\n        on transactions.currency = exchange_rates.from_currency\n/* Currently not allowed in `ASOF` join conditions */\nwhere coalesce(exchange_rates.to_currency, 'USD') = 'USD'\norder by\n    transactions.date,\n    transactions.amount\n</code></pre>"},{"location":"everything-about-joins/syntax/timestamp-joins/#traditional-solutions","title":"\"Traditional\" solutions","text":"<p>For comparison, here are a few solutions that illustrate how you might solve this problem without the <code>ASOF</code> join.</p>"},{"location":"everything-about-joins/syntax/timestamp-joins/#using-a-lateral-join","title":"Using a lateral join","text":"<p>If your database supports lateral \"joins\" (like DuckDB), you can use a lateral join to find the closest exchange rate to the transaction date:</p> <pre><code>select\n    transactions.date,\n    transactions.account,\n    transactions.amount,\n    rates.rate,\n    transactions.amount * rates.rate as amount_usd,\nfrom transactions,\n    lateral (\n        select rate\n        from exchange_rates\n        where 1=1\n            and transactions.currency = exchange_rates.from_currency\n            and exchange_rates.to_currency = 'USD'\n            and exchange_rates.date &lt;= transactions.date\n        order by exchange_rates.date desc\n        limit 1\n    ) as rates\norder by\n    transactions.date,\n    transactions.amount\n</code></pre> <p>Note that this approach would drop the first transaction from the result set because there is no exchange rate for the date of that transaction in our <code>exchange_rates</code> table. We'll \"fix\" this in the next example.</p>"},{"location":"everything-about-joins/syntax/timestamp-joins/#using-a-left-lateral-join","title":"Using a left lateral join","text":"<p>To keep the first transaction in the result set when using lateral, we can move the lateral subquery to a left join:</p> <pre><code>select\n    transactions.date,\n    transactions.account,\n    transactions.amount,\n    rates.rate,\n    transactions.amount * rates.rate as amount_usd,\nfrom transactions\n    left join lateral (\n        select rate\n        from exchange_rates\n        where 1=1\n            and transactions.currency = exchange_rates.from_currency\n            and exchange_rates.to_currency = 'USD'\n            and exchange_rates.date &lt;= transactions.date\n        order by exchange_rates.date desc\n        limit 1\n    ) as rates on 1=1\norder by\n    transactions.date,\n    transactions.amount\n</code></pre>"},{"location":"everything-about-joins/syntax/timestamp-joins/#using-left-join-and-qualify","title":"Using left join and qualify","text":"<p>If your database doesn't have <code>ASOF</code> or lateral joins, you can use a left join and the <code>QUALIFY</code> clause to find the closest exchange rate to the transaction date:</p> <pre><code>select\n    transactions.date,\n    transactions.account,\n    transactions.amount,\n    exchange_rates.rate,\n    transactions.amount * exchange_rates.rate as amount_usd,\nfrom transactions\n    left join exchange_rates\n        on  transactions.currency = exchange_rates.from_currency\n        and exchange_rates.to_currency = 'USD'\n        and exchange_rates.date &lt;= transactions.date\nqualify 1 = row_number() over (\n    partition by transactions.date, transactions.account\n    order by exchange_rates.date desc\n)\norder by\n    transactions.date,\n    transactions.amount\n</code></pre> <p>If you're using a database that doesn't support the <code>QUALIFY</code> clause, you can wrap the query in a subquery with the <code>ROW_NUMBER()</code> calculation saved to a column and filter on the column in the outer query.</p>"},{"location":"everything-about-joins/syntax/timestamp-joins/#using-a-left-join-and-correlated-subquery","title":"Using a left join and correlated subquery","text":"<p>If your database doesn't have <code>ASOF</code> joins, lateral joins, or the <code>QUALIFY</code> clause (and you don't want to use <code>ROW_NUMBER()</code> in a subquery), you can use a correlated subquery:</p> <pre><code>select\n    transactions.date,\n    transactions.account,\n    transactions.amount,\n    exchange_rates.rate,\n    transactions.amount * exchange_rates.rate as amount_usd\nfrom transactions\n    left join exchange_rates\n        on  transactions.currency = exchange_rates.from_currency\n        and exchange_rates.to_currency = 'USD'\n        and exchange_rates.date &lt;= transactions.date\nwhere 0=1\n    or exchange_rates.date is null\n    or exchange_rates.date = (\n        select max(\"date\")\n        from exchange_rates as rates_inner\n        where 1=1\n            and exchange_rates.from_currency = rates_inner.from_currency\n            and exchange_rates.to_currency = rates_inner.to_currency\n            and rates_inner.date &lt;= transactions.date\n    )\norder by\n    transactions.date,\n    transactions.amount\n</code></pre> <p>Note that, since this specific example, is only using the <code>rate</code> column from the <code>exchange_rates</code> table, we could have evaluated the subquery in the <code>SELECT</code> clause instead of the <code>FROM</code> clause. However, this would not work if we needed to use more than one column from the <code>exchange_rates</code> table, hence sticking to approaches that cater for multiple columns.</p>"},{"location":"everything-about-joins/syntax/timestamp-joins/#wrap-up","title":"Wrap up","text":"<p>Like with most SQL problems, there are multiple ways to get the output that we want. The <code>ASOF</code> join is just a great way to solve this particular problem, and it significantly reduces the complexity of our SQL code!</p> <p>Note that, also like with most SQL problems, the performance of these queries will depend on the size of the tables and the indexes available. If you're working with large tables, you will need to consider the performance implications of the <code>ASOF</code> approach compared to more \"traditional\" approaches.</p>"},{"location":"everything-about-joins/under-the-hood/hash-joins/","title":"Hash joins \ud83d\udd11","text":"<p>Success</p> <p>Hash joins are...</p>"},{"location":"everything-about-joins/under-the-hood/join-order/","title":"Does join order matter? \ud83e\udd14","text":"<p>Success</p> <p>We can join tables in any order...</p> <p>ANSI vs non-ANSI joins</p>"},{"location":"everything-about-joins/under-the-hood/joins-with-an-index/","title":"Joins with an index \ud83d\udd0d","text":"<p>Success</p> <p>Joins can be very expensive. Knowing how to use indexes in a join can massively speed them up!</p>"},{"location":"everything-about-joins/under-the-hood/loop-joins/","title":"Loop joins \ud83d\udd04","text":"<p>Success</p> <p>Loop joins are...</p>"},{"location":"everything-about-joins/under-the-hood/merge-joins/","title":"Merge joins \ud83d\udd00","text":"<p>Success</p> <p>Merge joins are...</p>"},{"location":"everything-about-joins/under-the-hood/range-intersection-joins/","title":"Range intersection joins \ud83c\udfaf","text":"<p>Success</p> <p>Range intersection joins are...</p>"},{"location":"from-excel-to-sql/from-excel-to-sql/","title":"From Excel to SQL","text":""},{"location":"from-excel-to-sql/from-excel-to-sql/#about-this-course","title":"About this course","text":"<p>Success</p> <p>SQL for the Excel user \ud83c\udf89</p> <p>Prefer videos? Check out the corresponding YouTube playlist:</p> <ul> <li>https://www.youtube.com/playlist?list=PLEiRgvTilK5rhnVPQ_Tj3Q-CI0rGn_uiD</li> </ul> <p>There are loads of resources online for learning SQL, such as:</p> <ul> <li>W3Schools</li> <li>SQLZoo</li> <li>Mode Analytics</li> <li>Khan Academy</li> <li>Codecademy</li> <li>SQLBolt</li> <li>SQLCourse</li> <li>Analyst Builder</li> <li>DataLemur</li> </ul> <p>...and the list goes on.</p> <p>This course is another one to add to the list, but the focus is coming from an Excel background!</p> <p>Note</p> <p>Some of the information provided here is simplified for the sake of learning. In reality, there may be additional caveats depending on the SQL flavour that you use.</p> <p>There is also plenty of information that is not covered here, even in the advanced concepts section. This is just a starting point, and you are encouraged to check out the documentation for the SQL flavour that you're using \ud83d\udcdd</p>"},{"location":"from-excel-to-sql/from-excel-to-sql/#the-toolsdata-in-this-course","title":"The tools/data in this course","text":"<p>Since this course is aimed at Microsoft Excel users, the \"SQL flavour\" that we'll use is the Microsoft SQL Server dialect (Transact-SQL).</p> <p>Any time a comment is made about how SQL does something, it's referring to the Microsoft SQL Server dialect which may not be the same as other SQL dialects.</p> <p>The data will be the AdventureWorks data. You can access this data for free at a few different places; the following site is recommended since it has an interactive query tool:</p> <ul> <li>https://dbfiddle.uk/</li> </ul> <p>The documentation for the AdventureWorks data is available at:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/samples/adventureworks-install-configure</li> </ul> <p>You can download and install applications to run SQL on your own machine. This won't be covered in this course, but there are several alternative guides online such as:</p> <ul> <li>Introduction to SQL Programming for Excel Users - SQL Server Windows Setup</li> </ul> <p>Tip</p> <p>If you're aiming to run SQL at work (assuming there's some SQL at your work to play with \ud83d\ude1d), you'll need to ask around to see what flavour of SQL is being used and what tools/methods for using it are available to you. This is almost entirely at the discretion of your IT department (or similar).</p>"},{"location":"from-excel-to-sql/from-excel-to-sql/#outline","title":"Outline","text":"<ol> <li>Getting started<ol> <li>Setting the context</li> <li>Syntax rules</li> </ol> </li> <li>Main concepts<ol> <li>Gimme data (<code>SELECT</code> and <code>FROM</code>)</li> <li>Filtering (<code>WHERE</code>)</li> <li>Ordering (<code>ORDER BY</code>)</li> <li>Comments (<code>--</code> and <code>/**/</code>)</li> <li><code>TOP</code> and <code>DISTINCT</code></li> <li>Data types</li> <li>Operators</li> <li>Conditional logic (<code>CASE</code> and <code>IIF</code>)</li> <li>Date formatting (<code>FORMAT</code>)</li> <li>Aggregations (<code>GROUP BY</code>)</li> <li>Pivot Tables (<code>ROLLUP</code>)</li> <li>Joins (<code>JOIN</code>)</li> <li>Unions (<code>UNION</code>)</li> <li>Subqueries</li> <li>Window functions (<code>OVER</code>)</li> <li>Logical processing order</li> <li>Style guide</li> </ol> </li> <li>Advanced concepts<ol> <li>Advanced aggregations</li> <li>Correlated subqueries</li> <li>Recursive CTEs</li> </ol> </li> </ol> <p>Warning</p> <p>The advanced concepts are advanced! They are not necessary for most day-to-day SQL usage, but they are good to know about.</p> <p>It is recommended that you practise the main concepts a lot and come back to the advanced concepts later.</p>"},{"location":"from-excel-to-sql/advanced-concepts/advanced-aggregations/","title":"Advanced aggregations \ud83e\udd47","text":"<p>Success</p> <p>We saw in the pivot table section that we can extend the <code>GROUP BY</code> clause with the <code>ROLLUP</code> modifier.</p> <p>There are two additional modifiers that we can use to extend the <code>GROUP BY</code> clause with even more flexibility: <code>GROUPING SETS</code> and <code>CUBE</code>.</p> <p>Warning</p> <p>The <code>GROUPING SETS</code> and <code>CUBE</code> modifiers are advanced, and there aren't Excel equivalents for these (without using advanced Excel features).</p> <p>Make sure that you are comfortable with the main concepts before diving into these advanced concepts.</p>"},{"location":"from-excel-to-sql/advanced-concepts/advanced-aggregations/#lets-start-with-a-pivot-table-recap","title":"Let's start with a pivot table recap","text":"<p>We saw the <code>ROLLUP</code> modifier in the pivot table section which allows us to create subtotals and grand totals in our <code>GROUP BY</code> clause.</p> <pre><code>SELECT\n    FORMAT(OrderDate, 'yyyy-MM') AS OrderMonth,\n    OnlineOrderFlag,\n    SUM(TotalDue) AS TotalSales\nFROM Sales.SalesOrderHeader\nWHERE '2013-01-01' &lt;= OrderDate AND OrderDate &lt; '2013-07-01'\nGROUP BY ROLLUP (\n    FORMAT(OrderDate, 'yyyy-MM'),\n    OnlineOrderFlag\n)\nORDER BY\n    OrderMonth,\n    OnlineOrderFlag\n;\n</code></pre> <p>The output of this query would include subtotals for each combination of <code>OrderMonth</code> and <code>OnlineOrderFlag</code>, as well as a grand total for each <code>OrderMonth</code> and a grand total for the entire dataset:</p> OrderMonth OnlineOrderFlag TotalSales null null 20996947.7407 \u2190 This is the grand total 2013-01 null 2340061.5521 \u2190 This is the 2013-01 subtotal 2013-01 false 1761132.8322 2013-01 true 578928.7199 2013-02 null 2600218.8667 \u2190 This is the 2013-02 subtotal 2013-02 false 2101152.5476 2013-02 true 499066.3191 2013-03 null 3831605.9389 \u2190 This is the 2013-03 subtotal 2013-03 false 3244501.4287 2013-03 true 587104.5102 2013-04 null 2840711.1734 \u2190 This is the 2013-04 subtotal 2013-04 false 2239156.6675 2013-04 true 601554.5059 2013-05 null 3658084.9461 \u2190 This is the 2013-05 subtotal 2013-05 false 3019173.6253 2013-05 true 638911.3208 2013-06 null 5726265.2635 \u2190 This is the 2013-06 subtotal 2013-06 false 4775809.3027 2013-06 true 950455.9608 <p>One thing you might notice is that we only have the subtotals for <code>OrderMonth</code>. Where are the subtotals for <code>OnlineOrderFlag</code>?</p> <p>As mentioned in the pivot tables section, the <code>ROLLUP</code> modifier follows the same rules as Excel's pivot tables and don't create subtotals for every column.</p> <p>This is where the <code>GROUPING SETS</code> modifier comes in.</p>"},{"location":"from-excel-to-sql/advanced-concepts/advanced-aggregations/#grouping-sets-allow-us-to-specify-the-subtotals-we-want","title":"<code>GROUPING SETS</code> allow us to specify the subtotals we want","text":"<p>We use the <code>GROUPING SETS</code> modifier in a very similar way to the <code>ROLLUP</code> modifier. We write <code>GROUPING SETS</code> after the <code>GROUP BY</code>, but rather than specify a list of columns, we specify a list of lists of columns.</p> <p>I'll say that again: we specify a list of lists of columns.</p> <p>This is a bit funky \ud83d\ude1d</p> <p>Each list of columns in the <code>GROUPING SETS</code> clause will create a subtotal for the combination of columns in that list. To specify a grand total, we use an empty list.</p> <p>To see this, let's rewrite the previous query using <code>GROUPING SETS</code> instead of <code>ROLLUP</code>:</p> <pre><code>SELECT\n    FORMAT(OrderDate, 'yyyy-MM') AS OrderMonth,\n    OnlineOrderFlag,\n    SUM(TotalDue) AS TotalSales\nFROM Sales.SalesOrderHeader\nWHERE '2013-01-01' &lt;= OrderDate AND OrderDate &lt; '2013-07-01'\nGROUP BY GROUPING SETS (\n    (),\n    (FORMAT(OrderDate, 'yyyy-MM')),\n    (FORMAT(OrderDate, 'yyyy-MM'), OnlineOrderFlag)\n)\nORDER BY\n    OrderMonth,\n    OnlineOrderFlag\n;\n</code></pre> <p>Specifically:</p> <ul> <li><code>()</code> corresponds to the grand total.</li> <li><code>(FORMAT(OrderDate, 'yyyy-MM'))</code> corresponds to the subtotals for <code>OrderMonth</code>.</li> <li><code>(FORMAT(OrderDate, 'yyyy-MM'), OnlineOrderFlag)</code> corresponds to the totals for each combination of <code>OrderMonth</code> and <code>OnlineOrderFlag</code>.</li> </ul> <p>Specifying the subtotals explicitly like this gives us more control over the output of the query. For example, it's super easy to add a subtotal for <code>OnlineOrderFlag</code> by adding another list to the <code>GROUPING SETS</code> clause:</p> <pre><code>SELECT\n    FORMAT(OrderDate, 'yyyy-MM') AS OrderMonth,\n    OnlineOrderFlag,\n    SUM(TotalDue) AS TotalSales\nFROM Sales.SalesOrderHeader\nWHERE '2013-01-01' &lt;= OrderDate AND OrderDate &lt; '2013-07-01'\nGROUP BY GROUPING SETS (\n    (),\n    (FORMAT(OrderDate, 'yyyy-MM')),\n    (OnlineOrderFlag),\n    (FORMAT(OrderDate, 'yyyy-MM'), OnlineOrderFlag)\n)\nORDER BY\n    OrderMonth,\n    OnlineOrderFlag\n;\n</code></pre> OrderMonth OnlineOrderFlag TotalSales null null 20996947.7407 null false 17140926.4040 \u2190 This is new null true 3856021.3367 \u2190 This is new 2013-01 null 2340061.5521 2013-01 false 1761132.8322 2013-01 true 578928.7199 2013-02 null 2600218.8667 2013-02 false 2101152.5476 2013-02 true 499066.3191 2013-03 null 3831605.9389 2013-03 false 3244501.4287 2013-03 true 587104.5102 2013-04 null 2840711.1734 2013-04 false 2239156.6675 2013-04 true 601554.5059 2013-05 null 3658084.9461 2013-05 false 3019173.6253 2013-05 true 638911.3208 2013-06 null 5726265.2635 2013-06 false 4775809.3027 2013-06 true 950455.9608 <p>Since we control exactly which subtotals we want, we can create a much more customised output than we could with <code>ROLLUP</code>!</p>"},{"location":"from-excel-to-sql/advanced-concepts/advanced-aggregations/#cube-is-like-rollup-but-for-every-combination-of-columns","title":"<code>CUBE</code> is like <code>ROLLUP</code> but for every combination of columns","text":"<p>In the last example above, we added a subtotal for <code>OnlineOrderFlag</code> by adding another list to the <code>GROUPING SETS</code> clause. This meant that we were creating subtotals for every combination of <code>OrderMonth</code> and <code>OnlineOrderFlag</code>:</p> <ul> <li><code>()</code> for neither (the grand total).</li> <li><code>(FORMAT(OrderDate, 'yyyy-MM'))</code> for just <code>OrderMonth</code>.</li> <li><code>(OnlineOrderFlag)</code> for just <code>OnlineOrderFlag</code>.</li> <li><code>(FORMAT(OrderDate, 'yyyy-MM'), OnlineOrderFlag)</code> for both.</li> </ul> <p>With two columns, we write four lists in the <code>GROUPING SETS</code> clause. With three columns, we'd write eight lists. With four columns, we'd write sixteen lists... So this can easily get out of hand!</p> <p>The <code>CUBE</code> modifier is like <code>ROLLUP</code> but for every combination of columns. It's a shortcut for writing out all the combinations of columns in the <code>GROUPING SETS</code> clause. It's also like <code>ROLLUP</code> because we write a list of columns, not a list of lists of columns.</p> <p>To see this, let's rewrite the previous query using <code>CUBE</code> instead of <code>GROUPING SETS</code>:</p> <pre><code>SELECT\n    FORMAT(OrderDate, 'yyyy-MM') AS OrderMonth,\n    OnlineOrderFlag,\n    SUM(TotalDue) AS TotalSales\nFROM Sales.SalesOrderHeader\nWHERE '2013-01-01' &lt;= OrderDate AND OrderDate &lt; '2013-07-01'\nGROUP BY CUBE (\n    FORMAT(OrderDate, 'yyyy-MM'),\n    OnlineOrderFlag\n)\nORDER BY\n    OrderMonth,\n    OnlineOrderFlag\n;\n</code></pre> <p>This will give us the same output as the previous query, but we didn't have to write out all the combinations of columns in the <code>GROUPING SETS</code> clause.</p>"},{"location":"from-excel-to-sql/advanced-concepts/advanced-aggregations/#we-can-still-use-groupinggrouping_id-to-identify-subtotals","title":"We can still use <code>GROUPING</code>/<code>GROUPING_ID</code> to identify subtotals","text":"<p>Like with <code>ROLLUP</code>, we can use the <code>GROUPING</code> and <code>GROUPING_ID</code> functions to identify which columns are subtotals. For example, here are the same queries as above but with the <code>GROUPING_ID</code> function added to the <code>SELECT</code> clause (they have the same output, so only one is shown):</p> <pre><code>SELECT\n    GROUPING_ID(FORMAT(OrderDate, 'yyyy-MM'), OnlineOrderFlag) AS GroupingId,\n    FORMAT(OrderDate, 'yyyy-MM') AS OrderMonth,\n    OnlineOrderFlag,\n    SUM(TotalDue) AS TotalSales\nFROM Sales.SalesOrderHeader\nWHERE '2013-01-01' &lt;= OrderDate AND OrderDate &lt; '2013-07-01'\nGROUP BY GROUPING SETS (\n    (),\n    (FORMAT(OrderDate, 'yyyy-MM')),\n    (OnlineOrderFlag),\n    (FORMAT(OrderDate, 'yyyy-MM'), OnlineOrderFlag)\n)\nORDER BY\n    OrderMonth,\n    OnlineOrderFlag\n;\n</code></pre> <pre><code>SELECT\n    GROUPING_ID(FORMAT(OrderDate, 'yyyy-MM'), OnlineOrderFlag) AS GroupingId,\n    FORMAT(OrderDate, 'yyyy-MM') AS OrderMonth,\n    OnlineOrderFlag,\n    SUM(TotalDue) AS TotalSales\nFROM Sales.SalesOrderHeader\nWHERE '2013-01-01' &lt;= OrderDate AND OrderDate &lt; '2013-07-01'\nGROUP BY CUBE (\n    FORMAT(OrderDate, 'yyyy-MM'),\n    OnlineOrderFlag\n)\nORDER BY\n    OrderMonth,\n    OnlineOrderFlag\n;\n</code></pre> GroupingId OrderMonth OnlineOrderFlag TotalSales 3 null null 20996947.7407 2 null false 17140926.4040 2 null true 3856021.3367 1 2013-01 null 2340061.5521 0 2013-01 false 1761132.8322 0 2013-01 true 578928.7199 1 2013-02 null 2600218.8667 0 2013-02 false 2101152.5476 0 2013-02 true 499066.3191 1 2013-03 null 3831605.9389 0 2013-03 false 3244501.4287 0 2013-03 true 587104.5102 1 2013-04 null 2840711.1734 0 2013-04 false 2239156.6675 0 2013-04 true 601554.5059 1 2013-05 null 3658084.9461 0 2013-05 false 3019173.6253 0 2013-05 true 638911.3208 1 2013-06 null 5726265.2635 0 2013-06 false 4775809.3027 0 2013-06 true 950455.9608"},{"location":"from-excel-to-sql/advanced-concepts/advanced-aggregations/#cubes-are-actually-a-well-known-concept","title":"\"Cubes\" are actually a well-known concept","text":"<p>Warning</p> <p>This is no longer an SQL concept; this is a general concept in mathematics and computer science.</p> <p>The <code>CUBE</code> modifier creates what's known as an Online Analytical Processing (OLAP) cube.</p> <ul> <li>https://en.wikipedia.org/wiki/OLAP_cube</li> </ul> <p>These are used in data warehousing and business intelligence to enable more performant reporting and analytics. They're a bit more advanced than what we're covering here, but it's good to know that the SQL <code>CUBE</code> modifier is based on a well-known concept.</p> <p>Tip</p> <p>If you've used Excel's Power Pivot, you've already used OLAP cubes!</p> <p>Power Pivot creates OLAP cubes behind the scenes so that using the power functions (like <code>CUBEVALUE</code>) can just \"lookup\" from these OLAP cubes.</p>"},{"location":"from-excel-to-sql/advanced-concepts/advanced-aggregations/#further-reading","title":"Further reading","text":"<p>Check out the official Microsoft documentation for more information on <code>GROUPING SETS</code> and <code>CUBE</code> at:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/t-sql/queries/select-group-by-transact-sql#group-by-grouping-sets--</li> <li>https://learn.microsoft.com/en-us/sql/t-sql/queries/select-group-by-transact-sql#group-by-cube--</li> </ul>"},{"location":"from-excel-to-sql/advanced-concepts/correlated-subqueries/","title":"Correlated subqueries \ud83c\udf10","text":"<p>Success</p> <p>Correlated subqueries are a great way to \"run a subquery\" for each row in a table.</p> <p>This can be slow if used inappropriately, but there are some great places to use this!</p> <p>Warning</p> <p>Correlated subqueries are advanced, and there aren't Excel equivalents for them.</p> <p>Make sure that you are comfortable with the main concepts before diving into these advanced concepts.</p>"},{"location":"from-excel-to-sql/advanced-concepts/correlated-subqueries/#correlated-subqueries-run-a-subquery-for-each-row-in-a-table","title":"Correlated subqueries \"run a subquery\" for each row in a table","text":"<p>We saw subqueries in the subqueries section, so what makes one \"correlated\"?</p> <p>A subquery becomes a \"correlated subquery\" if it references a column from a table in the outer query.</p> <p>The subquery is then \"run\" for each row in the referenced column.</p> <p>This can take a bit of getting used to, but it's a powerful tool when used appropriately.</p>"},{"location":"from-excel-to-sql/advanced-concepts/correlated-subqueries/#converting-joins-into-a-correlated-subquery","title":"Converting joins into a correlated subquery","text":"<p>Warning</p> <p>These are contrived examples to demonstrate the concept. You should stick to using <code>JOIN</code> for this kind of operation! \ud83d\ude1d</p>"},{"location":"from-excel-to-sql/advanced-concepts/correlated-subqueries/#employee-left-join-department","title":"<code>Employee LEFT JOIN Department</code>","text":"<p>To ease into the concept, we'll see how we could convert a <code>JOIN</code> into a correlated subquery.</p> <p>We saw the following query in the <code>Employee LEFT JOIN Department</code> part of the joins section:</p> <pre><code>SELECT\n    Employee.EmployeeID,\n    Employee.EmployeeName,\n    Employee.DepartmentID,\n    Department.DepartmentName\nFROM Employee\n    LEFT JOIN Department\n        ON Employee.DepartmentID = Department.DepartmentID\n;\n</code></pre> EmployeeID EmployeeName DepartmentID DepartmentName 1 Alice 1 Sales 2 Bob 1 Sales 3 Charlie 2 Marketing 4 Dave 2 Marketing 5 Eve 3 null <p>In this example, we're finding the <code>DepartmentName</code> in the <code>Department</code> table that matches the <code>DepartmentID</code> for each employee in the <code>Employee</code> table.</p> <p>We could convert this into a correlated subquery like so:</p> <pre><code>SELECT\n    EmployeeID,\n    EmployeeName,\n    DepartmentID,\n    (\n        SELECT DepartmentName\n        FROM Department\n        WHERE Department.DepartmentID = Employee.DepartmentID\n    ) AS DepartmentName\nFROM Employee\n;\n</code></pre> <p>This isn't just a normal subquery: the <code>WHERE</code> clause references the <code>Employee</code> table which is outside the subquery!</p> <p>Let's break down what's happening with each row in this table.</p>"},{"location":"from-excel-to-sql/advanced-concepts/correlated-subqueries/#employee-1","title":"Employee 1","text":"<p>The first row is employee 1:</p> EmployeeID EmployeeName DepartmentID 1 Alice 1 <p>Their <code>DepartmentID</code> is 1, so SQL will run the following subquery to find the <code>DepartmentName</code>:</p> <pre><code>SELECT DepartmentName\nFROM Department\nWHERE Department.DepartmentID = 1\n;\n</code></pre> DepartmentName Sales <p>Therefore, the value <code>Sales</code> will be used for the <code>DepartmentName</code> for Alice.</p>"},{"location":"from-excel-to-sql/advanced-concepts/correlated-subqueries/#employee-2","title":"Employee 2","text":"<p>The second row is employee 2:</p> EmployeeID EmployeeName DepartmentID 2 Bob 1 <p>Their <code>DepartmentID</code> is also 1, so SQL will run the exact same subquery to find the <code>DepartmentName</code>; this means that Bob will also have <code>Sales</code> as their <code>DepartmentName</code>.</p>"},{"location":"from-excel-to-sql/advanced-concepts/correlated-subqueries/#employee-3","title":"Employee 3","text":"<p>The third row is employee 3:</p> EmployeeID EmployeeName DepartmentID 3 Charlie 2 <p>Their <code>DepartmentID</code> is 2, so SQL will run the following subquery to find the <code>DepartmentName</code>:</p> <pre><code>SELECT DepartmentName\nFROM Department\nWHERE Department.DepartmentID = 2\n;\n</code></pre> DepartmentName Marketing <p>Therefore, the value <code>Marketing</code> will be used for the <code>DepartmentName</code> for Charlie.</p>"},{"location":"from-excel-to-sql/advanced-concepts/correlated-subqueries/#employee-4","title":"Employee 4","text":"<p>The fourth row is employee 4:</p> EmployeeID EmployeeName DepartmentID 4 Dave 2 <p>Their <code>DepartmentID</code> is also 2, so SQL will run the exact same subquery to find the <code>DepartmentName</code>; this means that Dave will also have <code>Marketing</code> as their <code>DepartmentName</code>.</p>"},{"location":"from-excel-to-sql/advanced-concepts/correlated-subqueries/#employee-5","title":"Employee 5","text":"<p>The fifth row is employee 5:</p> EmployeeID EmployeeName DepartmentID 5 Eve 3 <p>Their <code>DepartmentID</code> is 3, so SQL will run the following subquery to find the <code>DepartmentName</code>:</p> <pre><code>SELECT DepartmentName\nFROM Department\nWHERE Department.DepartmentID = 3\n;\n</code></pre> <p>This subquery will return no rows, so the <code>NULL</code> value will be used for the <code>DepartmentName</code> for Eve.</p>"},{"location":"from-excel-to-sql/advanced-concepts/correlated-subqueries/#putting-it-all-together","title":"Putting it all together","text":"<p>When we put all of these rows together, we get the following result:</p> EmployeeID EmployeeName DepartmentID DepartmentName 1 Alice 1 Sales 2 Bob 1 Sales 3 Charlie 2 Marketing 4 Dave 2 Marketing 5 Eve 3 null <p>This is indeed identical to the result we got from the <code>LEFT JOIN</code> query!</p>"},{"location":"from-excel-to-sql/advanced-concepts/correlated-subqueries/#what-about-the-other-join-examples","title":"What about the other join examples?","text":"<p>We saw two other join examples in the joins section:</p> <ul> <li>An inner join: Employee INNER JOIN Department</li> <li>A left join with multiple matches: Employee LEFT JOIN Address</li> </ul> <p>Do you think we could convert these into correlated subqueries, too?</p>"},{"location":"from-excel-to-sql/advanced-concepts/correlated-subqueries/#employee-inner-join-department","title":"Employee INNER JOIN Department","text":"<p>If we try to convert the inner join into a correlated subquery, we get exactly the same result as the <code>LEFT JOIN</code> example unless we explicitly filter out the <code>NULL</code> values after the subquery.</p> <p>This means that correlated subqueries are more like <code>LEFT JOIN</code> than <code>INNER JOIN</code> in this context.</p>"},{"location":"from-excel-to-sql/advanced-concepts/correlated-subqueries/#employee-left-join-address","title":"Employee LEFT JOIN Address","text":"<p>If we try to convert the left join with multiple matches into a correlated subquery, we'd end up with an error.</p> <p>This is because the subquery would return two rows for Eve, and subqueries used to define columns are only allowed to return a single value.</p> <p>This is a good example of why correlated subqueries are less appropriate for joins!</p>"},{"location":"from-excel-to-sql/advanced-concepts/correlated-subqueries/#using-correlated-subqueries-for-filtering","title":"Using correlated subqueries for filtering","text":"<p>The examples above were contrived to demonstrate the concept, but they're not the best use cases for correlated subqueries.</p> <p>The most common place that you'll see correlated subqueries being used is in the <code>WHERE</code> clause; these usually come in two forms:</p> <ul> <li>Relativity tests</li> <li>Existence tests</li> </ul>"},{"location":"from-excel-to-sql/advanced-concepts/correlated-subqueries/#using-correlated-subqueries-for-relativity-tests","title":"Using correlated subqueries for relativity tests","text":"<p>It's typical to see correlated subqueries to be used to get the latest position of a record in a table that holds historical data.</p> <p>For example, the <code>HumanResources.EmployeeDepartmentHistory</code> table holds the history of an employee's department. Let's take a look at the rows corresponding to employees 4, 16, 224, 234, and 250:</p> <pre><code>SELECT\n    BusinessEntityID,\n    DepartmentID,\n    StartDate,\n    EndDate\nFROM HumanResources.EmployeeDepartmentHistory\nWHERE BusinessEntityID IN (4, 16, 224, 234, 250)\nORDER BY BusinessEntityID, StartDate\n;\n</code></pre> BusinessEntityID DepartmentID StartDate EndDate 4 1 2007-12-05 2010-05-30 4 2 2010-05-31 null 16 5 2007-12-20 2009-07-14 16 4 2009-07-15 null 224 7 2009-01-07 2011-08-31 224 8 2011-09-01 null 234 10 2009-01-31 2013-11-13 234 16 2013-11-14 null 250 4 2011-02-25 2011-07-30 250 13 2011-07-31 2012-07-14 250 5 2012-07-15 null <p>This table is nice for finding the latest department for each employee because we can just filter the rows where <code>EndDate</code> is <code>NULL</code>, but what if we didn't have the <code>EndDate</code> column and had to rely on just the <code>StartDate</code>?</p> <p>We could use a correlated subquery to find the latest <code>StartDate</code> for each employee, and then use that to filter the rows:</p> <pre><code>SELECT\n    BusinessEntityID,\n    DepartmentID,\n    StartDate\nFROM HumanResources.EmployeeDepartmentHistory AS History\nWHERE BusinessEntityID IN (4, 16, 224, 234, 250)\n  AND StartDate = (\n    SELECT MAX(StartDate)\n    FROM HumanResources.EmployeeDepartmentHistory AS InnerHistory\n    WHERE InnerHistory.BusinessEntityID = History.BusinessEntityID\n  )\n;\n</code></pre> <p>This is a bit more complex than the previous examples!</p> <p>Not only is the subquery correlated, but it's also using an aggregate function (<code>MAX</code>) to find the latest <code>StartDate</code> for each employee.</p> <p>Let's break down what's happening with each row in this table.</p>"},{"location":"from-excel-to-sql/advanced-concepts/correlated-subqueries/#employee-4_1","title":"Employee 4","text":"<p>The first employee is 4, and they have two rows:</p> BusinessEntityID DepartmentID StartDate 4 1 2007-12-05 4 2 2010-05-31 <p>We know that we want to keep the row with the latest <code>StartDate</code>.</p> <p>A correlated subquery is run for each row in the <code>EmployeeDepartmentHistory</code> table, so the subquery is run twice for employee 4.</p> <p>The subquery that is run for this employee (for both their rows) is:</p> <pre><code>SELECT MAX(StartDate)\nFROM HumanResources.EmployeeDepartmentHistory AS InnerHistory\nWHERE InnerHistory.BusinessEntityID = 4\n;\n</code></pre> MAX(StartDate) 2010-05-31 <p>When the subquery is run for the first row, the <code>StartDate</code> is <code>2007-12-05</code>. This is not the result of the subquery, so the row is discarded.</p> <p>Conversely, when the subquery is run for the second row, the <code>StartDate</code> is <code>2010-05-31</code> so this second row is kept.</p>"},{"location":"from-excel-to-sql/advanced-concepts/correlated-subqueries/#employee-16","title":"Employee 16","text":"<p>The second employee is 16:</p> BusinessEntityID DepartmentID StartDate 16 5 2007-12-20 16 4 2009-07-15 <p>Similar to employee 4, a correlated subquery is run for each row in the <code>EmployeeDepartmentHistory</code> table, so the subquery is also run twice for employee 16.</p> <p>The subquery that is run for this employee (for both their rows) is:</p> <pre><code>SELECT MAX(StartDate)\nFROM HumanResources.EmployeeDepartmentHistory AS InnerHistory\nWHERE InnerHistory.BusinessEntityID = 16\n;\n</code></pre> MAX(StartDate) 2009-07-15 <p>When the subquery is run for the first row, the <code>StartDate</code> is <code>2007-12-20</code>. This is not the result of the subquery, so the row is discarded.</p> <p>Conversely, when the subquery is run for the second row, the <code>StartDate</code> is <code>2010-05-31</code> so this second row is kept.</p>"},{"location":"from-excel-to-sql/advanced-concepts/correlated-subqueries/#employees-224-and-234","title":"Employees 224 and 234","text":"<p>Like the previous employees, employees 224 and 234 each have two rows, so the subquery is run twice for each of them too.</p> <p>In both cases, the second row per employee is kept because it has the latest <code>StartDate</code>.</p>"},{"location":"from-excel-to-sql/advanced-concepts/correlated-subqueries/#employee-250","title":"Employee 250","text":"<p>The fifth employee is 250:</p> BusinessEntityID DepartmentID StartDate 250 4 2011-02-25 250 13 2011-07-31 250 5 2012-07-15 <p>This employee has three rows, so the subquery is run three times for them.</p> <p>The subquery that is run for this employee (for all three of their rows) is:</p> <pre><code>SELECT MAX(StartDate)\nFROM HumanResources.EmployeeDepartmentHistory AS InnerHistory\nWHERE InnerHistory.BusinessEntityID = 250\n;\n</code></pre> MAX(StartDate) 2012-07-15 <p>When the subquery is run for the first row, the <code>StartDate</code> is <code>2011-02-25</code>. This is not the result of the subquery, so the row is discarded.</p> <p>When the subquery is run for the second row, the <code>StartDate</code> is <code>2011-07-31</code>. Again, this is not the result of the subquery, so the row is discarded.</p> <p>Finally, when the subquery is run for the third row, the <code>StartDate</code> is <code>2012-07-15</code> so this third row is kept.</p>"},{"location":"from-excel-to-sql/advanced-concepts/correlated-subqueries/#putting-it-all-together_1","title":"Putting it all together","text":"<p>When we put all of these rows together, we get the following result:</p> BusinessEntityID DepartmentID StartDate 4 2 2010-05-31 16 4 2009-07-15 224 8 2011-09-01 234 16 2013-11-14 250 5 2012-07-15 <p>Tip</p> <p>There are other ways (not using correlated subqueries) to achieve this result, too. Can you think of any?</p>"},{"location":"from-excel-to-sql/advanced-concepts/correlated-subqueries/#using-correlated-subqueries-for-existence-tests","title":"Using correlated subqueries for existence tests","text":"<p>Another common place that you'll see correlated subqueries being used is to check whether a record exists.</p> <p>In this case, the <code>EXISTS</code> keyword is used with the subquery to tell SQL to do the existence test.</p> <p>For example, we could check which people in the <code>Person.Person</code> table have a record in the <code>HumanResources.JobCandidate</code> table:</p> <pre><code>SELECT\n    BusinessEntityID,\n    FirstName,\n    LastName\nFROM Person.Person\nWHERE EXISTS (\n    SELECT *\n    FROM HumanResources.JobCandidate\n    WHERE JobCandidate.BusinessEntityID = Person.BusinessEntityID\n)\n;\n</code></pre> BusinessEntityID FirstName LastName 212 Peng Wu 274 Stephen Jiang <p>As per the Microsoft documentation:</p> <p>Notice that subqueries that are introduced with <code>EXISTS</code> are a bit different from other subqueries in the following ways:</p> <ul> <li>The keyword <code>EXISTS</code> isn't preceded by a column name, constant, or other expression.</li> <li>The select list of a subquery introduced by <code>EXISTS</code> almost always consists of an asterisk (<code>*</code>). There's no reason to list column names because you're just testing whether rows that meet the conditions specified in the subquery exist.</li> </ul> <p>The <code>EXISTS</code> keyword is important because frequently there's no alternative formulation without subqueries. Although some queries that are created with <code>EXISTS</code> can't be expressed any other way, many queries can use <code>IN</code> or a comparison operator modified by <code>ANY</code> or <code>ALL</code> to achieve similar results.</p> <p>Our example is, in fact, an example of a query that can be expressed using <code>IN</code> instead of <code>EXISTS</code>:</p> <pre><code>SELECT\n    BusinessEntityID,\n    FirstName,\n    LastName\nFROM Person.Person\nWHERE BusinessEntityID IN (\n    SELECT BusinessEntityID\n    FROM HumanResources.JobCandidate\n)\n;\n</code></pre> <p>So, what's an example of a query that can't be expressed using <code>IN</code> instead of <code>EXISTS</code>?</p> <p>It's rare, but you'd need <code>EXISTS</code> over <code>IN</code> to evaluate more complex conditions that can't be boiled down to an equality test.</p> <p>For example, the following query can't be expressed using <code>IN</code>:</p> <pre><code>SELECT\n    BusinessEntityID,\n    FirstName,\n    LastName,\n    ModifiedDate\nFROM Person.Person\nWHERE EXISTS (\n    SELECT *\n    FROM HumanResources.JobCandidate\n    WHERE JobCandidate.BusinessEntityID = Person.BusinessEntityID\n      AND JobCandidate.ModifiedDate &gt; Person.ModifiedDate\n)\n;\n</code></pre> <p>This query checks for people who have a record in the <code>HumanResources.JobCandidate</code> table that was modified more recently than their record in the <code>Person.Person</code> table.</p> <p>This can't be expressed using <code>IN</code> because of the <code>JobCandidate.ModifiedDate &gt; Person.ModifiedDate</code> condition.</p> <p>This could, however, be expressed using a <code>LEFT JOIN</code> and a <code>WHERE</code> clause:</p> <pre><code>SELECT\n    Person.BusinessEntityID,\n    Person.FirstName,\n    Person.LastName,\n    Person.ModifiedDate\nFROM Person.Person\n    LEFT JOIN HumanResources.JobCandidate\n        ON JobCandidate.BusinessEntityID = Person.BusinessEntityID\nWHERE JobCandidate.BusinessEntityID IS NOT NULL\n  AND JobCandidate.ModifiedDate &gt; Person.ModifiedDate\n;\n</code></pre> <p>Like with many SQL problems, the approach that you take will depend on the context, the performance of the approaches, and the readability of the code.</p> <p>In this particular case, the <code>LEFT JOIN</code> is more performant than the <code>EXISTS</code> approach (on my personal computer, averaging 100,000 runs):</p> <ul> <li>The <code>LEFT JOIN</code> approach takes approximately 0.88 milliseconds to run (44%)</li> <li>The <code>EXISTS</code> approach takes approximately 1.11 milliseconds to run (56%)</li> </ul> <p>However, the <code>EXISTS</code> approach is slightly more readable (in my opinion), so both approached have pros and cons.</p> <p>Tip</p> <p>You'll encounter several ways to achieve the same output in SQL. There's rarely a \"right\" way to do something, so to figure out which approach you take, you should consider several things like:</p> <ul> <li>The performance of the code</li> <li>The readability of the code</li> <li>The extensibility of the code</li> <li>The preferences of your team/company</li> <li>The tools that you have available to you</li> </ul> <p>This is not a complete list and there are many other things to consider, but it's a good starting point.</p>"},{"location":"from-excel-to-sql/advanced-concepts/correlated-subqueries/#a-quick-note-for-the-programmers","title":"A quick note for the programmers","text":"<p>Warning</p> <p>If you've never done any programming before, skip this bit!</p> <p>If you've used other programming languages, you might be thinking that correlated subqueries are similar to loops. You'd be right!</p> <p>Correlated subqueries can be thought of as for-loops that run for each row in a table.</p> <p>This is why correlated subqueries can be slow if used inappropriately: they can run a lot of times!</p>"},{"location":"from-excel-to-sql/advanced-concepts/correlated-subqueries/#an-extra-example-to-get-the-brain-juices-flowing","title":"An extra example to get the brain juices flowing","text":"<p>This example is taken from the following article:</p> <ul> <li>https://buttondown.email/jaffray/archive/sql-scoping-is-surprisingly-subtle-and-semantic/</li> </ul> <p>Suppose we have the following tables:</p> <p><code>aa</code></p> a 1 2 3 <p><code>xx</code></p> x 10 20 30 <p>What do you think the results of the following queries are?</p> <pre><code>SELECT (SELECT TOP 1 SUM(1)     FROM xx) FROM aa;\nSELECT (SELECT TOP 1 SUM(a)     FROM xx) FROM aa;\nSELECT (SELECT TOP 1 SUM(x)     FROM xx) FROM aa;\nSELECT (SELECT TOP 1 SUM(a + x) FROM xx) FROM aa;\n</code></pre> <p>Here are the results:</p> Expand to show the results <code>SELECT (SELECT TOP 1 SUM(1) FROM xx) FROM aa</code> 3 3 3 <code>SELECT (SELECT TOP 1 SUM(a) FROM xx) FROM aa</code> 6 <code>SELECT (SELECT TOP 1 SUM(x) FROM xx) FROM aa</code> 60 60 60 <code>SELECT (SELECT TOP 1 SUM(a + x) FROM xx) FROM aa</code> 63 66 69 <p>Weird, right? \ud83e\udd2f</p> <p>See if you can wrap your head around what's going on here, and then check out the article linked above for a great explanation of what's happening.</p>"},{"location":"from-excel-to-sql/advanced-concepts/correlated-subqueries/#the-sql-for-running-this-example","title":"The SQL for running this example","text":"<p>This data is not in the AdventureWorks database, so you can run this example using the SQL below (uncomment the line you want to run):</p> <pre><code>WITH\n\naa AS (\n    SELECT *\n    FROM (VALUES (1), (2), (3)) AS t(a)\n),\nxx AS (\n    SELECT *\n    FROM (VALUES (10), (20), (30)) AS t(x)\n)\n\nSELECT (SELECT TOP 1 SUM(1)     FROM xx) FROM aa\n-- SELECT (SELECT TOP 1 SUM(a)     FROM xx) FROM aa\n-- SELECT (SELECT TOP 1 SUM(x)     FROM xx) FROM aa\n-- SELECT (SELECT TOP 1 SUM(a + x) FROM xx) FROM aa\n;\n</code></pre>"},{"location":"from-excel-to-sql/advanced-concepts/correlated-subqueries/#further-reading","title":"Further reading","text":"<p>Check out the official Microsoft documentation for more information on correlated subqueries at:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/relational-databases/performance/subqueries#correlated</li> </ul>"},{"location":"from-excel-to-sql/advanced-concepts/correlated-subqueries/#similarity-to-cross-apply","title":"Similarity to <code>CROSS APPLY</code>","text":"<p>There is a similarity between correlated subqueries and <code>CROSS APPLY</code>. This is outside the scope of this course, but you can read more about it at:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/t-sql/queries/from-transact-sql#using-apply</li> <li>https://learn.microsoft.com/en-us/sql/t-sql/queries/from-transact-sql#k-use-apply</li> <li>https://learn.microsoft.com/en-us/sql/t-sql/queries/from-transact-sqll-use-cross-apply</li> </ul>"},{"location":"from-excel-to-sql/advanced-concepts/recursive-ctes/","title":"Recursive CTEs \ud83d\udd01","text":"<p>Success</p> <p>Recursive CTEs are a great way to generate data and to flatten hierarchical data.</p> <p>Warning</p> <p>Recursive CTEs are advanced, and while there is almost an Excel equivalent for one type of recursive CTE, the Excel equivalent is not as flexible as the SQL version.</p> <p>Make sure that you are comfortable with the main concepts before diving into these advanced concepts.</p>"},{"location":"from-excel-to-sql/advanced-concepts/recursive-ctes/#a-recursive-cte-is-one-that-references-itself","title":"A recursive CTE is one that references itself","text":"<p>Note</p> <p>When you use a recursive CTE, it will keep \"re-running\"/iterating the CTE and adding new rows until the CTE is told to stop.</p> <p>We saw in the subqueries section that subqueries can be defined in the <code>WITH</code> clause at the top of a query:</p> <pre><code>WITH Orders AS (\n    SELECT\n        FORMAT(OrderDate, 'yyyy-MM') AS OrderMonth,\n        TotalDue\n    FROM Sales.SalesOrderHeader\n)\n\nSELECT\n    OrderMonth,\n    SUM(TotalDue) AS TotalSales\nFROM Orders\nWHERE OrderMonth IN ('2013-01', '2013-02', '2013-03')\nGROUP BY OrderMonth\nORDER BY OrderMonth\n;\n</code></pre> <p>The <code>Orders</code> CTE references the <code>Sales.SalesOrderHeader</code> table, which is then referenced by the <code>SELECT</code> statement.</p> <pre><code>flowchart LR\n    A[Sales.SalesOrderHeader] --&gt; B[Orders] --&gt; C[SELECT]</code></pre> <p>A recursive CTE is one that references itself. This sounds a bit weird, but it has two main uses:</p> <ol> <li>To generate rows of data, e.g. a sequence of numbers or a date range</li> <li>To flatten hierarchical data, e.g. an organisation's structure</li> </ol>"},{"location":"from-excel-to-sql/advanced-concepts/recursive-ctes/#the-shape-of-a-recursive-cte","title":"The \"shape\" of a recursive CTE","text":"<p>We'll use the following recursive CTE as an example. For now, we'll just focus on how it's written and will explain what it does later.</p> <pre><code>WITH Numbers AS (\n        SELECT 1 AS Number\n    UNION ALL\n        SELECT Number + 1\n        FROM Numbers\n        WHERE Number &lt; 5\n)\n\nSELECT *\nFROM Numbers\n;\n</code></pre> Number 1 2 3 4 5 <p>There are four parts to a recursive CTE:</p> <ol> <li>The anchor <code>SELECT</code> statement</li> <li>The <code>UNION ALL</code> keywords</li> <li>The recursive <code>SELECT</code> statement</li> <li>The termination condition in the <code>WHERE</code> clause</li> </ol>"},{"location":"from-excel-to-sql/advanced-concepts/recursive-ctes/#the-anchor-select-statement","title":"The anchor <code>SELECT</code> statement","text":"<p>For a CTE to be able to reference itself, it has to have a starting point \ud83d\ude1d</p> <p>In the example above, the starting point/anchor statement is:</p> <pre><code>SELECT 1 AS Number\n</code></pre> <p>This creates a single row with a single column whose value is <code>1</code>.</p>"},{"location":"from-excel-to-sql/advanced-concepts/recursive-ctes/#the-union-all-keywords","title":"The <code>UNION ALL</code> keywords","text":"<p>A recursive CTE will append new rows to the bottom of the existing rows, so it needs a <code>UNION</code>.</p> <p>However, you must use <code>UNION ALL</code> and not just <code>UNION</code>. This is a requirement of SQL, not just a recommendation.</p>"},{"location":"from-excel-to-sql/advanced-concepts/recursive-ctes/#the-recursive-select-statement","title":"The recursive <code>SELECT</code> statement","text":"<p>The recursive <code>SELECT</code> statement is the one that references the CTE itself.</p> <p>In the example above, the recursive statement is:</p> <pre><code>SELECT Number + 1\nFROM Numbers\nWHERE Number &lt; 5\n</code></pre> <p>The <code>SELECT</code> clause in this statement contains the logic that you want to apply with each \"re-run\"/iteration. In this case, we're adding <code>1</code> to the <code>Number</code> column.</p> <p>The <code>FROM</code> clause is explicitly where we reference the CTE itself. If we don't reference the CTE itself within the CTE, it won't be recursive \ud83d\ude04</p>"},{"location":"from-excel-to-sql/advanced-concepts/recursive-ctes/#the-termination-condition-in-the-where-clause","title":"The termination condition in the <code>WHERE</code> clause","text":"<p>The <code>WHERE</code> clause in the recursive <code>SELECT</code> statement is the condition that tells the CTE to stop.</p> <p>This is super important: if you don't have a termination condition, the CTE will not stop running!</p> <p>In the example above, the termination condition is:</p> <pre><code>WHERE Number &lt; 5\n</code></pre> <p>This tells the CTE to stop when the <code>Number</code> column is less than <code>5</code>. Note that, because of the way that SQL processes the <code>WHERE</code> clause, the last loop will be when <code>Number</code> is <code>4</code> but the <code>SELECT</code> clause will still add <code>1</code> to it, so the final number in <code>Number</code> will be <code>5</code>.</p>"},{"location":"from-excel-to-sql/advanced-concepts/recursive-ctes/#putting-it-all-together","title":"Putting it all together","text":"<p>The query below is the same as the one above, but it has comments to show where each part of the recursive CTE is.</p> <pre><code>WITH Numbers AS (\n        /* 1. The anchor SELECT statement */\n        SELECT 1 AS Number\n    /* 2. The UNION ALL keywords */\n    UNION ALL\n        /* 3. The recursive SELECT statement */\n        SELECT Number + 1\n        FROM Numbers\n        /* 4. The termination condition in the WHERE clause */\n        WHERE Number &lt; 5\n)\n\nSELECT *\nFROM Numbers\n;\n</code></pre> <p>Warning</p> <p>In some SQL flavours, you have to explicitly write <code>RECURSIVE</code> after the <code>WITH</code> keyword to tell SQL that the CTE is recursive.</p> <p>This is not necessary (and not even allowed!) in Microsoft SQL Server.</p>"},{"location":"from-excel-to-sql/advanced-concepts/recursive-ctes/#generating-data-with-a-recursive-cte","title":"Generating data with a recursive CTE","text":"<p>The simplest example of a recursive CTE is one that generates a sequence of numbers. This is precisely what the example above does!</p> <p>There's two Excel equivalents to this:</p>"},{"location":"from-excel-to-sql/advanced-concepts/recursive-ctes/#there-are-two-excel-equivalents","title":"There are two Excel equivalents","text":"<ul> <li>The Series feature</li> <li>The <code>SEQUENCE</code> function</li> </ul> <p>The Series feature is in the Home tab, in the Editing group, and is called Series... (under AutoSum). This is not available in the Web version of Excel; Microsoft recommend dragging the fill handle instead.</p> <p>The <code>SEQUENCE</code> function is a function to do the same as the Series feature and the fill handle.</p>"},{"location":"from-excel-to-sql/advanced-concepts/recursive-ctes/#breaking-down-the-data-generation-example","title":"Breaking down the data generation example","text":"<p>We've gone through how the query is written, but we haven't explained what it does.</p>"},{"location":"from-excel-to-sql/advanced-concepts/recursive-ctes/#the-anchor-select-statement_1","title":"The anchor <code>SELECT</code> statement","text":"<p>The anchor <code>SELECT</code> statement is only run once, and it's the starting point for the CTE.</p> <p>In our example, the anchor <code>SELECT</code> statement is:</p> <pre><code>SELECT 1 AS Number\n</code></pre> Number 1 <p>This is the starting point for the recursive CTE and is the first row to be passed to the recursive <code>SELECT</code> statement.</p>"},{"location":"from-excel-to-sql/advanced-concepts/recursive-ctes/#the-recursive-select-statement_1","title":"The recursive <code>SELECT</code> statement","text":"<p>The first time that the recursive <code>SELECT</code> statement is run, it'll use the row from the anchor <code>SELECT</code> statement as its input; the rest of the time, it'll use the rows from the previous iteration (this will be significant in the hierarchy flattening example).</p> <p>As a reminder, the recursive <code>SELECT</code> statement is:</p> <pre><code>SELECT Number + 1\nFROM Numbers\nWHERE Number &lt; 5\n</code></pre> <p>We start with a single row from the anchor statement, so the first iteration of the recursive statement takes the <code>1</code> (which is less than <code>5</code>) and adds <code>1</code> to it to get <code>2</code>, so after this iteration the CTE is:</p> Number 1 \u2190 anchor 2 \u2190 1st iteration <p>Now the recursive <code>SELECT</code> statement is run again, but this time it uses the row from the first iteration as its input. It takes the <code>2</code> (which is less than <code>5</code>) and adds <code>1</code> to it to get <code>3</code>, so after this iteration the CTE is:</p> Number 1 \u2190 anchor 2 \u2190 1st iteration 3 \u2190 2nd iteration <p>This process continues for a third iteration, using only the row from the previous iteration (the <code>3</code>):</p> Number 1 \u2190 anchor 2 \u2190 1st iteration 3 \u2190 2nd iteration 4 \u2190 3rd iteration <p>The process continues for a fourth iteration, using only the row from the previous iteration (the <code>4</code>):</p> Number 1 \u2190 anchor 2 \u2190 1st iteration 3 \u2190 2nd iteration 4 \u2190 3rd iteration 5 \u2190 4th iteration <p>Finally, when the process attempts to run a fifth iteration using <code>5</code>, the <code>WHERE</code> condition <code>Number &lt; 5</code> is no longer met. No rows are generated during this iteration, so the CTE stops running and the final result is:</p> Number 1 2 3 4 5"},{"location":"from-excel-to-sql/advanced-concepts/recursive-ctes/#generating-a-date-range","title":"Generating a date range","text":"<p>The same principle can be applied to generate a date range. The following query generates a date range from <code>2023-01-01</code> to <code>2023-01-31</code>:</p> <pre><code>WITH Dates AS (\n        SELECT CAST('2023-01-01' AS DATE) AS Date\n    UNION ALL\n        SELECT DATEADD(DAY, 1, Date)\n        FROM Dates\n        WHERE Date &lt; '2023-01-31'\n)\n\nSELECT *\nFROM Dates\n;\n</code></pre> Date 2023-01-01 2023-01-02 2023-01-03 2023-01-04 2023-01-05 2023-01-06 2023-01-07 2023-01-08 2023-01-09 2023-01-10 2023-01-11 2023-01-12 2023-01-13 2023-01-14 2023-01-15 2023-01-16 2023-01-17 2023-01-18 2023-01-19 2023-01-20 2023-01-21 2023-01-22 2023-01-23 2023-01-24 2023-01-25 2023-01-26 2023-01-27 2023-01-28 2023-01-29 2023-01-30 2023-01-31 <p>Can you think of any other ranges that you'd want to generate?</p> <p>Tip</p> <p>If you're working with a good data set, you'll hopefully have a table of dates that you can use instead of generating them (often called a \"calendar table\" or a \"date dimension\").</p> <p>However, not all data sets will have this, so generating a date range on the fly can be super useful!</p>"},{"location":"from-excel-to-sql/advanced-concepts/recursive-ctes/#flattening-hierarchical-data-with-a-recursive-cte","title":"Flattening hierarchical data with a recursive CTE","text":"<p>Warning</p> <p>There is no Excel equivalent to this, so wrapping your head around this will probably be a bit more difficult than the data generation example.</p> <p>Flattening hierarchical data is the other main use of a recursive CTE.</p> <p>There are lots of examples of hierarchical data:</p> <ul> <li>An organisation's structure, where each employee has a manager and each manager has a manager</li> <li>A family tree, where each person has parents and each parent has parents</li> <li>A file system, where each file is in a folder and each folder is in a folder</li> </ul> <p>We'll focus on the first example: an organisation's structure.</p> <p>The AdventureWorks database doesn't have a table for an organisation's structure, so we'll use the following CTE (adapted from the Microsoft docs) to create a table of employees and their managers:</p> <pre><code>WITH Organisation AS (\n    SELECT *\n    FROM (VALUES\n        (1,   N'Ken S\u00e1nchez',    N'Chief Executive Officer',      NULL),\n        (273, N'Brian Welcker',  N'Vice President of Sales',      1),\n        (274, N'Stephen Jiang',  N'North American Sales Manager', 273),\n        (275, N'Michael Blythe', N'Sales Representative',         274),\n        (276, N'Linda Mitchell', N'Sales Representative',         274),\n        (285, N'Syed Abbas',     N'Pacific Sales Manager',        273),\n        (286, N'Lynn Tsoflias',  N'Sales Representative',         285),\n        (16,  N'David Bradley',  N'Marketing Manager',            273),\n        (23,  N'Mary Gibson',    N'Marketing Specialist',         16)\n    ) AS V(EmployeeID, Name, Title, ManagerID)\n)\n\nSELECT *\nFROM Organisation\n;\n</code></pre> EmployeeID Name Title ManagerID 1 Ken S\u00e1nchez Chief Executive Officer null 273 Brian Welcker Vice President of Sales 1 274 Stephen Jiang North American Sales Manager 273 275 Michael Blythe Sales Representative 274 276 Linda Mitchell Sales Representative 274 285 Syed Abbas Pacific Sales Manager 273 286 Lynn Tsoflias Sales Representative 285 16 David Bradley Marketing Manager 273 23 Mary Gibson Marketing Specialist 16 <p>This table has the employee ID and their manager's ID, so it would be quite easy to write a query to get the manager's name for each employee -- just do a self-join:</p> <pre><code>WITH Organisation AS (...)\n\nSELECT\n    Employee.EmployeeID,\n    Employee.Name,\n    Employee.ManagerID,\n    Manager.Name AS ManagerName\nFROM Organisation AS Employee\n    LEFT JOIN Organisation AS Manager\n        ON Employee.ManagerID = Manager.EmployeeID\n;\n</code></pre> EmployeeID Name ManagerID ManagerName 1 Ken S\u00e1nchez null null 273 Brian Welcker 1 Ken S\u00e1nchez 274 Stephen Jiang 273 Brian Welcker 275 Michael Blythe 274 Stephen Jiang 276 Linda Mitchell 274 Stephen Jiang 285 Syed Abbas 273 Brian Welcker 286 Lynn Tsoflias 285 Syed Abbas 16 David Bradley 273 Brian Welcker 23 Mary Gibson 16 David Bradley <p>What if we wanted to know the hierarchy level of each employee (with the CEO as <code>1</code>)?</p> <p>Well, we could try some subqueries and unions, but this would be a lot to write:</p> <pre><code>WITH Organisation AS (...),\n\nLevel1 AS (\n    SELECT\n        1 AS EmployeeLevel,\n        EmployeeID,\n        ManagerID,\n        Name\n    FROM Organisation\n    WHERE ManagerID IS NULL  /* The CEO */\n),\n\nLevel2 AS (\n    SELECT\n        2 AS EmployeeLevel,\n        EmployeeID,\n        ManagerID,\n        Name\n    FROM Organisation\n    WHERE ManagerID IN (\n        SELECT EmployeeID\n        FROM Level1\n    )\n),\n\nLevel3 AS (\n    SELECT\n        3 AS EmployeeLevel,\n        EmployeeID,\n        ManagerID,\n        Name\n    FROM Organisation\n    WHERE ManagerID IN (\n        SELECT EmployeeID\n        FROM Level2\n    )\n),\n\nLevel4 AS (\n    SELECT\n        4 AS EmployeeLevel,\n        EmployeeID,\n        ManagerID,\n        Name\n    FROM Organisation\n    WHERE ManagerID IN (\n        SELECT EmployeeID\n        FROM Level3\n    )\n)\n\n    SELECT * FROM Level1\nUNION\n    SELECT * FROM Level2\nUNION\n    SELECT * FROM Level3\nUNION\n    SELECT * FROM Level4\n;\n</code></pre> <p>Not only is this a lot to write, but it's also not very flexible. We needed to know how many levels there were in the hierarchy to write this query, and if the hierarchy changes, we'd need to change the query.</p> <p>This is where a recursive CTE comes in handy!</p> <p>The recursive CTE will keep going until it runs out of rows, so it doesn't need to know how many levels there are in the hierarchy beforehand.</p> <pre><code>WITH Organisation AS (...),\n\nLevels AS (\n        SELECT\n            1 AS EmployeeLevel,\n            EmployeeID,\n            ManagerID,\n            Name\n        FROM Organisation\n        WHERE ManagerID IS NULL  /* The CEO */\n    UNION ALL\n        SELECT\n            Levels.EmployeeLevel + 1,\n            Organisation.EmployeeID,\n            Organisation.ManagerID,\n            Organisation.Name\n        FROM Organisation\n            INNER JOIN Levels\n                ON Organisation.ManagerID = Levels.EmployeeID\n)\n\nSELECT *\nFROM Levels\n;\n</code></pre> EmployeeLevel EmployeeID ManagerID Name 1 1 null Ken S\u00e1nchez 2 273 1 Brian Welcker 3 274 273 Stephen Jiang 3 285 273 Syed Abbas 3 16 273 David Bradley 4 23 16 Mary Gibson 4 286 285 Lynn Tsoflias 4 275 274 Michael Blythe 4 276 274 Linda Mitchell <p>This query is definitely shorter than the subquery/union version, but it's more complex than the data generation example; there are a few differences to call out and explain.</p>"},{"location":"from-excel-to-sql/advanced-concepts/recursive-ctes/#the-anchor-select-statement_2","title":"The anchor <code>SELECT</code> statement","text":"<p>The anchor statement this time is:</p> <pre><code>SELECT\n    1 AS EmployeeLevel,\n    EmployeeID,\n    ManagerID,\n    Name\nFROM Organisation\nWHERE ManagerID IS NULL  /* The CEO */\n</code></pre> EmployeeLevel EmployeeID ManagerID Name 1 1 null Ken S\u00e1nchez <p>In contrast to the data generation example, we want to start with an actual employee. In this case, we want to start at the top of the hierarchy (the CEO) which is why we:</p> <ul> <li>Select from the <code>Organisation</code> table/CTE</li> <li>Filter for the row where <code>ManagerID</code> is <code>NULL</code></li> </ul> <p>This <code>WHERE</code> clause is just to set up the anchor node; this clause is not used in the recursive <code>SELECT</code> statement.</p> <p>We do, however, set the starting point for the <code>EmployeeLevel</code> column to <code>1</code> in the anchor statement.</p>"},{"location":"from-excel-to-sql/advanced-concepts/recursive-ctes/#the-recursive-select-statement_2","title":"The recursive <code>SELECT</code> statement","text":"<p>The recursive statement is:</p> <pre><code>SELECT\n    Levels.EmployeeLevel + 1,\n    Organisation.EmployeeID,\n    Organisation.ManagerID,\n    Organisation.Name\nFROM Organisation\n    INNER JOIN Levels\n        ON Organisation.ManagerID = Levels.EmployeeID\n</code></pre> <p>The <code>Levels.EmployeeLevel + 1</code> line should be fairly intuitive: it's just adding <code>1</code> to the <code>EmployeeLevel</code> column from the previous iteration, just like in the data generation example.</p> <p>The <code>INNER JOIN</code> clause is where the magic happens. This is actually doing two things for us:</p> <ol> <li>It's how we know which rows to use from the <code>Organisation</code> table/CTE for this iteration. This example also shows that the recursive CTE can be referenced in a <code>JOIN</code> clause instead of just in the <code>FROM</code> clause.</li> <li>By being an <code>INNER JOIN</code>, it's also the termination condition for the CTE! When there are no more rows to join, the CTE will stop running. This is why we don't need a <code>WHERE</code> clause in the recursive <code>SELECT</code> statement.</li> </ol>"},{"location":"from-excel-to-sql/advanced-concepts/recursive-ctes/#breaking-down-the-hierarchy-flattening-example","title":"Breaking down the hierarchy flattening example","text":"<p>This is still probably a bit confusing, so let's break it down with the first few iterations.</p>"},{"location":"from-excel-to-sql/advanced-concepts/recursive-ctes/#the-anchor-is-the-ceo","title":"The anchor is the CEO","text":"<p>We've already seen that the anchor statement gives us the CEO:</p> EmployeeLevel EmployeeID ManagerID Name 1 1 null Ken S\u00e1nchez"},{"location":"from-excel-to-sql/advanced-concepts/recursive-ctes/#the-first-iteration-is-the-ceos-direct-reports","title":"The first iteration is the CEO's direct report(s)","text":"<p>The first iteration of the recursive statement will use the CEO's <code>EmployeeID</code> to find the employees who report to the CEO, and it'll increment the <code>EmployeeLevel</code> by <code>1</code>:</p> EmployeeLevel EmployeeID ManagerID Name 2 273 1 Brian Welcker <p>At this point, the CTE will be:</p> EmployeeLevel EmployeeID ManagerID Name 1 1 null Ken S\u00e1nchez \u2190 anchor 2 273 1 Brian Welcker \u2190 1st iteration"},{"location":"from-excel-to-sql/advanced-concepts/recursive-ctes/#the-second-iteration-is-the-first-iterations-direct-reports","title":"The second iteration is the first iteration's direct report(s)","text":"<p>The second iteration will use the output of the first iteration, namely Brian Welcker, and find their direct reports (and increment the <code>EmployeeLevel</code> by <code>1</code>):</p> EmployeeLevel EmployeeID ManagerID Name 3 274 273 Stephen Jiang 3 285 273 Syed Abbas 3 16 273 David Bradley <p>At this point, the CTE will be:</p> EmployeeLevel EmployeeID ManagerID Name 1 1 null Ken S\u00e1nchez \u2190 anchor 2 273 1 Brian Welcker \u2190 1st iteration 3 274 273 Stephen Jiang \u2190 2nd iteration 3 285 273 Syed Abbas \u2190 2nd iteration 3 16 273 David Bradley \u2190 2nd iteration <p>This is a case that we haven't seen yet: this iteration has produced multiple rows. This means that all three of these rows will be used in the next iteration.</p>"},{"location":"from-excel-to-sql/advanced-concepts/recursive-ctes/#the-third-iteration-is-the-second-iterations-direct-reports","title":"The third iteration is the second iteration's direct report(s)","text":"<p>The third iteration will use the output of the second iteration, namely Stephen Jiang, Syed Abbas, and David Bradley, and find their direct reports (and increment the <code>EmployeeLevel</code> by <code>1</code>):</p> EmployeeLevel EmployeeID ManagerID Name 4 23 16 Mary Gibson 4 286 285 Lynn Tsoflias 4 275 274 Michael Blythe 4 276 274 Linda Mitchell <p>At this point, the CTE will be:</p> EmployeeLevel EmployeeID ManagerID Name 1 1 null Ken S\u00e1nchez \u2190 anchor 2 273 1 Brian Welcker \u2190 1st iteration 3 274 273 Stephen Jiang \u2190 2nd iteration 3 285 273 Syed Abbas \u2190 2nd iteration 3 16 273 David Bradley \u2190 2nd iteration 4 23 16 Mary Gibson \u2190 3rd iteration 4 286 285 Lynn Tsoflias \u2190 3rd iteration 4 275 274 Michael Blythe \u2190 3rd iteration 4 276 274 Linda Mitchell \u2190 3rd iteration"},{"location":"from-excel-to-sql/advanced-concepts/recursive-ctes/#the-fourth-iteration-will-terminate-the-loop","title":"The fourth iteration will terminate the loop","text":"<p>When the fourth iteration is run, it will look for the direct reports of Mary Gibson, Lynn Tsoflias, Michael Blythe, and Linda Mitchell by using the <code>INNER JOIN</code>.</p> <p>Since there are no direct reports for these employees, the <code>INNER JOIN</code> will not produce any rows, and the CTE will stop running!</p> <p>Therefore, the final result is:</p> EmployeeLevel EmployeeID ManagerID Name 1 1 null Ken S\u00e1nchez 2 273 1 Brian Welcker 3 274 273 Stephen Jiang 3 285 273 Syed Abbas 3 16 273 David Bradley 4 23 16 Mary Gibson 4 286 285 Lynn Tsoflias 4 275 274 Michael Blythe 4 276 274 Linda Mitchell"},{"location":"from-excel-to-sql/advanced-concepts/recursive-ctes/#other-hierarchy-flattening-examples","title":"Other hierarchy flattening examples","text":"<p>If you want to see some more hierarchy flattening examples, check out the Microsoft documentation's examples (it has some good ones!):</p> <ul> <li>https://learn.microsoft.com/en-us/sql/t-sql/queries/with-common-table-expression-transact-sql#use-a-recursive-common-table-expression-to-display-two-levels-of-recursion</li> <li>https://learn.microsoft.com/en-us/sql/t-sql/queries/with-common-table-expression-transact-sql#use-a-recursive-common-table-expression-to-display-a-hierarchical-list</li> <li>https://learn.microsoft.com/en-us/sql/t-sql/queries/with-common-table-expression-transact-sql#e-use-a-common-table-expression-to-selectively-step-through-a-recursive-relationship-in-a-select-statement</li> <li>https://learn.microsoft.com/en-us/sql/t-sql/queries/with-common-table-expression-transact-sql#h-use-multiple-anchor-and-recursive-members</li> <li>https://learn.microsoft.com/en-us/sql/t-sql/queries/with-common-table-expression-transact-sql#bkmkUsingAnalyticalFunctionsInARecursiveCTE</li> </ul> <p>Tip</p> <p>You might be thinking: what if the recursive CTE never hits the termination condition?</p> <p>Microsoft SQL Server will stop the CTE after 100 iterations, so you won't get an infinite loop.</p> <p>You might then be thinking: what if I want more than 100 iterations?</p> <p>Microsoft SQL Server allows you to change this with the <code>MAXRECURSION</code> option:</p> <pre><code>WITH SomeRecursiveCTE AS (...)\n\nSELECT *\nFROM SomeRecursiveCTE\nOPTION (MAXRECURSION 500)\n;\n</code></pre>"},{"location":"from-excel-to-sql/advanced-concepts/recursive-ctes/#a-quick-note-for-the-programmers","title":"A quick note for the programmers","text":"<p>Warning</p> <p>If you've never done any programming before, skip this bit!</p> <p>If you've used other programming languages, you might be thinking that recursive CTEs are similar to loops. You'd be right!</p> <p>Just like how correlated subqueries can be thought of as for-loops, recursive CTEs can be thought of as while-loops that run while a condition is met.</p> <p>This is why recursive CTEs can be slow if used inappropriately: they can run a lot of times!</p>"},{"location":"from-excel-to-sql/advanced-concepts/recursive-ctes/#some-fun-recursive-cte-examples","title":"Some fun recursive CTE examples","text":"<p>Warning</p> <p>These are just for fun. If you're not comfortable with the main concepts, don't worry about these examples!</p>"},{"location":"from-excel-to-sql/advanced-concepts/recursive-ctes/#generating-a-fibonacci-sequence","title":"Generating a Fibonacci sequence","text":"<p>The Fibonacci sequence is a sequence of numbers where each number is the sum of the two preceding ones. For example, the first few terms are:</p> <ul> <li>1</li> <li>1</li> <li>2 (1 + 1)</li> <li>3 (1 + 2)</li> <li>5 (2 + 3)</li> <li>8 (3 + 5)</li> <li>...</li> </ul> <p>To generate the first <code>10</code> terms of the Fibonacci sequence, you can use the following recursive CTE:</p> <pre><code>WITH Fibonacci AS (\n        SELECT\n            1 AS n,\n            1 AS f_n,  /* f(n) */\n            0 AS f_n_1   /* f(n - 1) */\n    UNION ALL\n        SELECT\n            n + 1 AS n,\n            f_n + f_n_1 AS f_n,\n            f_n AS f_n_1\n        FROM Fibonacci\n        WHERE n &lt; 10\n)\n\nSELECT\n    n,\n    f_n\nFROM Fibonacci\n</code></pre> n f_n 1 1 2 1 3 2 4 3 5 5 6 8 7 13 8 21 9 34 10 55"},{"location":"from-excel-to-sql/advanced-concepts/recursive-ctes/#generating-the-mandelbrot-set","title":"Generating the Mandelbrot set","text":"<p>Info</p> <p>This example is taken from Graeme Job at:</p> <ul> <li>https://thedailywtf.com/articles/Stupid-Coding-Tricks-The-TSQL-Madlebrot</li> </ul> <p>The following beauty produces the Mandelbrot set, a well-known fractal, and the SQL is written by Graeme Job:</p> <pre><code>WITH\n    XGEN(X, IX) AS (\n        /* X DIM GENERATOR */\n            SELECT\n                CAST(-2.2 AS FLOAT) AS X,\n                0 AS IX\n        UNION ALL\n            SELECT\n                CAST(X + 0.031 AS FLOAT) AS X,\n                IX + 1 AS IX\n            FROM XGEN\n            WHERE IX &lt; 100\n    ),\n    YGEN(Y, IY) AS (\n        /* Y DIM GENERATOR */\n            SELECT\n                CAST(-1.5 AS FLOAT) AS Y,\n                0 AS IY\n        UNION ALL\n            SELECT\n                CAST(Y + 0.031 AS FLOAT) AS Y,\n                IY + 1 AS IY\n            FROM YGEN\n            WHERE IY &lt; 100\n    ),\n    Z(IX, IY, CX, CY, X, Y, I) AS (\n        /* Z POINT ITERATOR */\n            SELECT\n                IX,\n                IY,\n                X AS CX,\n                Y AS CY,\n                X,\n                Y,\n                0 AS I\n            FROM XGEN, YGEN\n        UNION ALL\n            SELECT\n                IX,\n                IY,\n                CX,\n                CY,\n                X * X - Y * Y + CX AS X,\n                Y * X * 2 + CY,\n                I + 1\n            FROM Z\n            WHERE X * X + Y * Y &lt; 16\n              AND I &lt; 100\n    )\n\nSELECT\n    TRANSLATE(\n        (\n            X0 +X1 +X2 +X3 +X4 +X5 +X6 +X7 +X8 +X9 +X10+X11+X12+X13+X14+X15+X16+X17+X18+X19+\n            X20+X21+X22+X23+X24+X25+X26+X27+X28+X29+X30+X31+X32+X33+X34+X35+X36+X37+X38+X39+\n            X40+X41+X42+X43+X44+X45+X46+X47+X48+X49+X50+X51+X52+X53+X54+X55+X56+X57+X58+X59+\n            X60+X61+X62+X63+X64+X65+X66+X67+X68+X69+X70+X71+X72+X73+X74+X75+X76+X77+X78+X79+\n            X80+X81+X82+X83+X84+X85+X86+X87+X88+X89+X90+X91+X92+X93+X94+X95+X96+X97+X98+X99\n        ),\n        'ABCDEFGHIJKLMNOPQRSTUVWXYZ',\n        ' .,,,-----++++%%%%@@@@### '\n    )\nFROM (\n    SELECT\n        'X' + CAST(IX AS VARCHAR) AS IX,\n        IY,\n        SUBSTRING('ABCDEFGHIJKLMNOPQRSTUVWXYZ', ISNULL(NULLIF(I, 0), 1), 1) AS I\n    FROM Z\n) AS ZT\nPIVOT (\n    MAX(I) FOR IX IN (\n        X0, X1, X2, X3, X4, X5, X6, X7, X8, X9, X10,X11,X12,X13,X14,X15,X16,X17,X18,X19,\n        X20,X21,X22,X23,X24,X25,X26,X27,X28,X29,X30,X31,X32,X33,X34,X35,X36,X37,X38,X39,\n        X40,X41,X42,X43,X44,X45,X46,X47,X48,X49,X50,X51,X52,X53,X54,X55,X56,X57,X58,X59,\n        X60,X61,X62,X63,X64,X65,X66,X67,X68,X69,X70,X71,X72,X73,X74,X75,X76,X77,X78,X79,\n        X80,X81,X82,X83,X84,X85,X86,X87,X88,X89,X90,X91,X92,X93,X94,X95,X96,X97,X98,X99\n    )\n) AS PZT\n;\n</code></pre> <p>The output doesn't look great here, so give it a run yourself!</p>"},{"location":"from-excel-to-sql/advanced-concepts/recursive-ctes/#further-reading","title":"Further reading","text":"<p>Check out the official Microsoft documentation for more information on recursive CTEs at:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/t-sql/queries/with-common-table-expression-transact-sql#guidelines-for-defining-and-using-recursive-common-table-expressions</li> </ul>"},{"location":"from-excel-to-sql/advanced-concepts/recursive-ctes/#some-links-for-excel-stuff","title":"Some links for Excel stuff","text":"<p>The Series feature and the fill handles are documented at:</p> <ul> <li>https://support.microsoft.com/en-gb/office/project-values-in-a-series-5311f5cf-149e-4d06-81dd-5aaad87e5400</li> <li>https://support.microsoft.com/en-gb/office/fill-data-automatically-in-worksheet-cells-74e31bdd-d993-45da-aa82-35a236c5b5db</li> </ul> <p>The <code>SEQUENCE</code> function can be super handy for generating sequences of numbers in Excel. The official documentation is at:</p> <ul> <li>https://support.microsoft.com/en-gb/office/sequence-function-57467a98-57e0-4817-9f14-2eb78519ca90</li> </ul> <p>However, the following tutorials are also super useful:</p> <ul> <li>https://exceljet.net/functions/sequence-function</li> <li>https://exceljet.net/videos/the-sequence-function</li> <li>https://exceljet.net/formulas/sequence-of-days</li> <li>https://exceljet.net/formulas/sequence-of-times</li> <li>https://exceljet.net/formulas/sequence-of-months</li> <li>https://exceljet.net/formulas/sequence-of-years</li> </ul>"},{"location":"from-excel-to-sql/getting-started/setting-the-context/","title":"Setting the context \ud83e\udd14","text":""},{"location":"from-excel-to-sql/getting-started/setting-the-context/#what-the-fudge-is-sql","title":"What the fudge is SQL?","text":"<p>Success</p> <p>SQL stands for Structured Query Language.</p> <p>SQL is a programming language oriented around data. The confusing part about SQL is that it isn't one language: there are loads of different \"flavours\" of SQL!</p>"},{"location":"from-excel-to-sql/getting-started/setting-the-context/#spreadsheets-and-sql","title":"Spreadsheets and SQL","text":"<p>To help make this a bit more relatable, think about the general concept of a \"spreadsheet\".</p> <p>This is a general concept because there are lots of different \"implementations\" of spreadsheet software, for example:</p> <ul> <li>Microsoft Excel</li> <li>Google Sheets</li> <li>Apple Numbers</li> <li>LibreOffice</li> </ul> <p>SQL is also a general concept; there are loads of different \"implementations\" of SQL, for example:</p> <ul> <li>Microsoft SQL Server</li> <li>Google BigQuery</li> <li>PostgreSQL</li> <li>MySQL</li> </ul> <p>Tip</p> <p>If someone claims that they're a \"SQL wiz\", always ask them \"which flavour?\" \ud83d\ude1d</p>"},{"location":"from-excel-to-sql/getting-started/setting-the-context/#what-does-sql-do","title":"What does SQL do?","text":"<p>The point of SQL is to interact with data that lives in a \"database\".</p> <p>Since SQL is a programming language, you write some code, then you run it. Running SQL code means running it on this \"database\", and we usually call the SQL code a \"query\".</p> <p>The reason that we have so many different flavours of SQL is because each one is specific to the database that it's running on.</p> <p>If you want to write SQL on your own machine, you'll need to install a database. This is just like needing to install a spreadsheet application (e.g. Excel) to work with spreadsheets \ud83d\ude0b</p>"},{"location":"from-excel-to-sql/getting-started/setting-the-context/#what-the-fudge-is-a-database","title":"What the fudge is a database?","text":"<p>Success</p> <p>A database is just some software that stores and manipulates data.</p> <p>There are a few different types of databases, but SQL is designed for databases that store their data as tables. This makes these types of databases very familiar to us: they're like spreadsheets!</p>"},{"location":"from-excel-to-sql/getting-started/setting-the-context/#database-tables-are-like-excel-tables","title":"Database tables are like Excel tables","text":"<p>One of the awesome things about spreadsheets is that the sheets are super flexible, and you can have data pretty much wherever you want.</p> <p>Databases are a bit more rigid. The tables in databases are like the tables in spreadsheets, with a few key differences -- the most important differences to know are:</p> <ul> <li>Database tables cannot have merged cells</li> <li>Columns in database tables have a specific data type (more on this later)</li> </ul> <p>Database tables are very similar to the table structures in Excel that you create with the \"Format as Table\" feature:</p> <ul> <li>https://support.microsoft.com/en-gb/office/bf0ce08b-d012-42ec-8ecf-a2259c9faf3f</li> </ul> <p>It'll be important to understand these Excel \"tables\" as we go through this course, so if you're not familiar with them, it's worth checking out the link above or the one below:</p> <ul> <li>https://exceljet.net/articles/excel-tables</li> </ul>"},{"location":"from-excel-to-sql/getting-started/setting-the-context/#why-the-fudge-should-you-use-sql","title":"Why the fudge should you use SQL?","text":"<p>Success</p> <p>SQL is everywhere!</p> <p>If you're looking to move into a more data-oriented role (e.g. data analyst, data scientist, data engineer, analytics engineer), SQL is a must-have skill.</p> <p>Even if you're just looking to find ways to improve your existing role, knowing SQL can empower you to crunch more data, streamline/automate more processes, and generally make your life easier.</p>"},{"location":"from-excel-to-sql/getting-started/sql-syntax/","title":"SQL syntax \ud83d\udcda","text":"<p>Success</p> <p>Syntax is the set of rules that define how you write in a programming language.</p> <p>SQL has a very flexible syntax which is heavily based on the English language. This makes it easy to read and write, but it also means that there are many ways to write the same thing.</p> <p>We'll go through a lot of the specifics in the following sections, but here are some general rules to get you started.</p>"},{"location":"from-excel-to-sql/getting-started/sql-syntax/#sql-sounds-like-english","title":"SQL \"sounds like\" English","text":"<p>Info</p> <p>At least, for simple queries \ud83d\ude1d</p> <p>Consider the following SQL statement (we'll go through what this means in the next sections):</p> <pre><code>SELECT first_name, last_name\nFROM employees\nWHERE department = 'Sales'\n</code></pre> <p>This reads closely to the English sentence:</p> <p>\"Select the first name and last name from the employees where the department is Sales\"</p> <p>The only words that we've dropped from the English version of the sentence are \"the\" and \"and\"!</p> <p>This is a basic example, but it's a good way to think about writing SQL when you're starting out. If it sounds like English, you're probably on the right track.</p>"},{"location":"from-excel-to-sql/getting-started/sql-syntax/#whitespace-and-capitalisation-are-mostly-ignored","title":"Whitespace and capitalisation are (mostly) ignored","text":"<p>Note</p> <p>Some databases (\"SQL flavours\") are stricter about this than others, but most of the time they won't care about how you add whitespace or capitalise your letters.</p> <p>For the most part, SQL doesn't care about whitespace (spaces, tabs, new lines) or capitalisation. This means that all the following are equivalent:</p> <pre><code>SELECT first_name, last_name\nFROM employees\nWHERE department = 'Sales'\n</code></pre> <pre><code>select first_name, last_name from employees where department = 'Sales'\n</code></pre> <pre><code>SELECT\nFIRST_NAME,\nLAST_NAME\nFROM\nEMPLOYEES\nWHERE\nDEPARTMENT\n=\n'Sales'\n</code></pre> <pre><code>                             SeLeCt FiRsT_NaMe\n,\n                lAsT_nAmE\n    FrOm\n\n         eMpLoyEEs WhErE\n                 dEpaRTmEnT\n\n=\n                                    'Sales'\n</code></pre> <p>Of course, some of these are easier to read than others, so it's generally a good idea to use whitespace and capitalisation to make your SQL easier to read. Some choices are totally down to your opinion, we'll go through some best practices in the style guide section to help you choose a style that you like.</p>"},{"location":"from-excel-to-sql/getting-started/sql-syntax/#semicolons-are-mostly-optional","title":"Semicolons are (mostly) optional","text":"<p>Note</p> <p>Semicolons are generally only required when you have multiple SQL statements in the same file (or when you're using a database/SQL flavour that requires them).</p> <p>Semicolons (<code>;</code>) are used to explicitly end a SQL statement. This is useful when you have multiple SQL statements in the same file, but it's not always required.</p> <p>Ending your SQL with a semicolon is a good habit to get into, but it's unlikely to be mandatory for the SQL that you write.</p> <pre><code>SELECT first_name, last_name\nFROM employees\nWHERE department = 'Sales'\n;\n</code></pre>"},{"location":"from-excel-to-sql/getting-started/sql-syntax/#text-needs-to-be-put-in-single-quotes","title":"Text needs to be put in single quotes","text":"<p>Just like in Excel, any text needs to be enclosed in quotes in SQL. The only difference is that the quotes that you use are single quotes, <code>'</code>, not double quotes, <code>\"</code>.</p> <p>The quotes are used to tell the database that the text is text, and not a column name or a keyword.</p> <p>SQL also treats dates differently to Excel; we \"pretend\" that dates are some text and put them in single quotes. In general, dates need to be specified in the universal date format, <code>YYYY-MM-DD</code>.</p> <ul> <li><code>123</code> is an example of a number in SQL</li> <li><code>'abc'</code> is an example of text in SQL</li> <li><code>'2020-01-01'</code> is an example of a date in SQL</li> </ul>"},{"location":"from-excel-to-sql/getting-started/sql-syntax/#we-use-some-specific-words-to-describe-sql-elements","title":"We use some specific words to describe SQL elements","text":"<p>It's worth mentioning some of the specific words that we use to describe the elements of SQL. We'll use the example above to explain these terms, plus some other bits that we'll see later:</p> <ul> <li>Keywords are the words that are specific to SQL and have a special meaning. For example, the following are all keywords:<ul> <li><code>SELECT</code></li> <li><code>FROM</code></li> <li><code>WHERE</code></li> <li><code>AND</code></li> <li><code>OR</code></li> <li><code>ON</code></li> </ul> </li> <li>Identifiers are the names of the tables, columns, and other objects in your database. For example, the following are all identifiers:<ul> <li><code>employees</code></li> <li><code>first_name</code></li> <li><code>last_name</code></li> <li><code>department</code></li> </ul> </li> <li>Operators are the symbols that you use to compare or manipulate data. For example, the following are all operators:<ul> <li><code>+</code></li> <li><code>-</code></li> <li><code>=</code></li> <li><code>&gt;</code></li> <li><code>&lt;</code></li> </ul> </li> <li>Clauses are the different parts of an SQL statement made up of keywords, identifiers, and any other text. For example, the following are all clauses:<ul> <li><code>SELECT first_name, last_name</code></li> <li><code>FROM employees</code></li> <li><code>WHERE department = 'Sales'</code></li> </ul> </li> <li>Statements or queries are the complete SQL commands that you write. For example, the following is a statement/query:<ul> <li><code>SELECT first_name, last_name FROM employees WHERE department = 'Sales'</code></li> </ul> </li> <li>Result or result set is the data that comes back from the database when you run a query (more on this in the next section). For example, the following is a result set:</li> </ul> first_name last_name department John Smith Sales Jane Doe Sales"},{"location":"from-excel-to-sql/main-concepts/comments/","title":"Comments \ud83d\udcdd","text":"<p>Success</p> <p>Comments are a way to add notes to your SQL code. They are not executed and are only there for the benefit of the reader.</p> <p>Note</p> <p>Comments can go anywhere in your SQL code! \ud83d\ude80</p>"},{"location":"from-excel-to-sql/main-concepts/comments/#comments-can-be-written-with-or","title":"Comments can be written with <code>--</code> or <code>/* */</code>","text":"<p>A \"comment\" in an SQL statement is some text which is ignored when the query is run. It's a way to add notes to your code to help explain what's happening.</p> <p>There are two ways to write comments in SQL:</p> <ol> <li>Single-line comments using <code>--</code> which comment out the rest of the line</li> <li>Multi-line comments using <code>/* */</code> which comment out everything they enclose</li> </ol> <p>For info, the multi-line comment goes by several different names:</p> <ul> <li>\"C-style\" comment</li> <li>\"Block\" comment</li> <li>\"Slash-star\" comment</li> </ul> <p>Writing comments is super easy, for example:</p> <pre><code>-- This is a title\nSELECT *\nFROM HumanResources.Department\n;\n\n/*\nThis query won't run\n\nSELECT *\nFROM HumanResources.Department\n;\n*/\n</code></pre> <p>Warning</p> <p>Comments can be helpful for explaining bits of code, but a comment should never excuse bad code! Use them sparingly and only when necessary.</p> <p>Tip</p> <p>This is my personal preference, but I'd recommend using the slash-star comments for any \"documentation\", and the double-dash comments for any (temporary) commented-out code. This is because the double-dash comments are the ones used by most IDEs to \"comment out\" code, so following this practice makes it easier to distinguish between the two types of comments.</p>"},{"location":"from-excel-to-sql/main-concepts/comments/#further-reading","title":"Further reading","text":"<p>Check out the official Microsoft documentation for more information on comments at:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/t-sql/language-elements/comment-transact-sql</li> <li>https://learn.microsoft.com/en-us/sql/t-sql/language-elements/slash-star-comment-transact-sql</li> </ul> <p>The video version of this content is also available at:</p> <ul> <li>https://youtu.be/H6SfeXFWWmg</li> </ul>"},{"location":"from-excel-to-sql/main-concepts/conditionals/","title":"Conditionals \ud83d\udd00","text":"<p>Success</p> <p>Just like Excel's <code>IF</code> function, SQL has a few ways to handle conditional logic.</p>"},{"location":"from-excel-to-sql/main-concepts/conditionals/#sqls-if-function-is-iif","title":"SQL's \"if\" function is <code>IIF</code>","text":"<p>Info</p> <p>Why is it called <code>IIF</code>? It's short for \"Immediate If\" which is the typical name programming languages give a function that acts like the <code>IF</code> function in Excel. This is to distinguish the <code>IIF</code> function from the alternative <code>IF</code> statement already in the programming language, and SQL is no exception.</p> <p>This is just an FYI. If it doesn't make sense, don't worry about it! \ud83d\ude1d</p> <p>The <code>IIF</code> function is exactly what you'd expect: it's the same as the Excel <code>IF</code> function!</p> <pre><code>SELECT IIF(AGE &lt; 18, 'Child', 'Adult') AS AGE_GROUP\n</code></pre> <p>This function is great for simple conditions, but anyone who's used Excel for a while knows that anything slightly more complex can get a bit unwieldy.</p> <p>To handle more complex conditions, SQL has a <code>CASE</code> statement.</p>"},{"location":"from-excel-to-sql/main-concepts/conditionals/#case-statements-are-great-for-complex-conditions","title":"<code>CASE</code> statements are great for complex conditions","text":"<p>Note</p> <p>The <code>CASE</code> statement is a staple of SQL and is used in many different SQL dialects. It's worth getting to know it well!</p> <p>A <code>CASE</code> statement is really a series of <code>IF</code> statements (logically).</p> <p>Converting the <code>IIF</code> example from above to a <code>CASE</code> statement looks like this:</p> <pre><code>SELECT\n    CASE\n        WHEN AGE &lt; 18\n            THEN 'Child'\n            ELSE 'Adult'\n    END AS AGE_GROUP\n</code></pre> <p>When a <code>CASE</code> statement is used, the <code>WHEN</code> part is checked for each value. For each value, if a condition is met the <code>THEN</code> part is used, otherwise the next condition is checked.</p> <p>To show this off, the example above can be extended to include more conditions:</p> <pre><code>SELECT\n    CASE\n        WHEN AGE &lt; 13\n            THEN 'Child'\n        WHEN AGE &lt; 18\n            THEN 'Teenager'\n            ELSE 'Adult'\n    END AS AGE_GROUP\n</code></pre> <p>Info</p> <p>The corresponding Excel formula for this <code>CASE</code> statement would be (assuming the age is in cell <code>A1</code>):</p> <pre><code>=IF(A1 &lt; 13, \"Child\", IF(A1 &lt; 18, \"Teenager\", \"Adult\"))\n</code></pre> <p>Let's walk through this example to see how it works. Suppose that the underlying data has the following rows:</p> ID AGE 1 10 2 15 3 23 4 18 5 null <ol> <li>The first row has an age of 10, so the first condition, <code>AGE &lt; 13</code>, is met and therefore the result is <code>Child</code>.</li> <li>The second row has an age of 15, so the first condition is not met. This means that the second condition, <code>AGE &lt; 18</code>, is checked. The second condition is met so the result is <code>Teenager</code>.</li> <li>The third row has an age of 23, so neither the first condition nor the second condition are met. There are no more conditions so the <code>ELSE</code> part is used and the result is <code>Adult</code>.</li> <li>The fourth row has an age of 18, so the first condition is not met. The second condition is also not met because 18 is not less than 18, so the result for this row is also <code>Adult</code>.</li> <li>The fifth row is missing an age, so SQL can't say whether the conditions (<code>AGE &lt; 13</code> and <code>AGE &lt; 18</code>) are met. Therefore, it assumes that they're not met and also uses the else part of the <code>CASE</code> statement, also resulting in <code>Adult</code>.</li> </ol> ID AGE AGE_GROUP 1 10 Child 2 15 Teenager 3 23 Adult 4 18 Adult 5 null Adult <p>Warning</p> <p>The last example (with the missing age) might be a bit confusing, so it's important to practice using data with <code>NULL</code> values to get used to handling them.</p> <p>Tip</p> <p>When using case statements, it is usually a good idea to have a condition that checks for <code>NULL</code> values right at the start. Adding this to the example above might look something like:</p> <pre><code>SELECT\n    CASE\n        WHEN AGE IS NULL\n            THEN 'Unknown'\n        WHEN AGE &lt; 13\n            THEN 'Child'\n        WHEN AGE &lt; 18\n            THEN 'Teenager'\n            ELSE 'Adult'\n    END AS AGE_GROUP\n</code></pre> <p>The corresponding Excel formula for this <code>CASE</code> statement would be (assuming the age is in cell <code>A1</code>) something like:</p> <pre><code>=IF(A1 = \"\", \"Unknown\", IF(A1 &lt; 13, \"Child\", IF(A1 &lt; 18, \"Teenager\", \"Adult\")))\n</code></pre>"},{"location":"from-excel-to-sql/main-concepts/conditionals/#keep-redundant-logic-out-of-your-case-statements","title":"Keep redundant logic out of your <code>CASE</code> statements","text":"<p>Notice how the order of the conditions in the <code>CASE</code> statement is important. Since the <code>AGE &lt; 18</code> condition comes after the <code>AGE &lt; 13</code> condition, the <code>AGE &lt; 18</code> condition already knows that the age is at least 13 if a value comes to it! If it wasn't, it would have been caught by the <code>AGE &lt; 13</code> condition.</p> <p>Out in the wild, you might find people who still add these redundant conditions to their <code>CASE</code> statements. It's not wrong to do this, but it's not necessary and it can make the code harder to read.</p> <p>For example, the following <code>CASE</code> statement is equivalent to the one above, but it includes the redundant conditions which make it harder to read:</p> <pre><code>SELECT\n    CASE\n        WHEN AGE IS NULL\n            THEN 'Unknown'\n        WHEN AGE IS NOT NULL AND AGE &lt; 13  /* `AGE IS NOT NULL` is redundant! */\n            THEN 'Child'\n        WHEN AGE IS NOT NULL AND AGE &gt;= 13 AND AGE &lt; 18  /* `AGE IS NOT NULL AND AGE &gt;= 13` is redundant! */\n            THEN 'Teenager'\n            ELSE 'Adult'\n    END AS AGE_GROUP\n</code></pre>"},{"location":"from-excel-to-sql/main-concepts/conditionals/#alternative-case-syntax","title":"Alternative <code>CASE</code> syntax","text":"<p>There are times when you might want to use the <code>CASE</code> statement to convert one set of values into another set of values, for example:</p> <pre><code>SELECT\n    CASE\n        WHEN CODE = 'A'\n            THEN 'Alpha'\n        WHEN CODE = 'B'\n            THEN 'Bravo'\n        WHEN CODE = 'C'\n            THEN 'Charlie'\n            ELSE 'Unknown'\n    END AS PHONETIC\n</code></pre> <p>In this specific case where the conditions are all checking for specific values (using an equals) in a single column, the column can be specified once at the start of the <code>CASE</code> statement and just the values can be written in the <code>WHEN</code> part:</p> <pre><code>SELECT\n    CASE CODE\n        WHEN 'A'\n            THEN 'Alpha'\n        WHEN 'B'\n            THEN 'Bravo'\n        WHEN 'C'\n            THEN 'Charlie'\n            ELSE 'Unknown'\n    END AS PHONETIC\n</code></pre> CODE PHONETIC A Alpha B Bravo C Charlie D Unknown null Unknown <p>Tip</p> <p>Mapping values like this is convenient with the <code>CASE</code> statement, but in most cases it's better to have a lookup table that you can join on (we'll cover joins later).</p> <p>However, you might not always have a lookup table available, so the <code>CASE</code> statement is a good alternative in those cases.</p>"},{"location":"from-excel-to-sql/main-concepts/conditionals/#the-sql-for-running-these-examples","title":"The SQL for running these examples","text":"<p>Error</p> <p>The data for these examples isn't in the AdventureWorks database that we're using, so it has been created for this section. If you want to run these examples yourself, you can use the SQL below but note that this is using some features that we haven't covered yet!</p> <p>For the examples above, the rows are created on the fly. You're not expected to understand this yet, but it's provided so that you can run the SQL yourself if you want to.</p> <pre><code>/* Age Example */\nSELECT\n    ID,\n    AGE,\n    CASE\n        WHEN AGE IS NULL\n            THEN 'Unknown'\n        WHEN AGE &lt; 13\n            THEN 'Child'\n        WHEN AGE &lt; 18\n            THEN 'Teenager'\n            ELSE 'Adult'\n    END AS AGE_GROUP\nFROM (\n    VALUES\n        (1, 10),\n        (2, 15),\n        (3, 23),\n        (4, 18),\n        (5, NULL)\n) AS AGES(ID, AGE)\n;\n\n/* Phonetic Example */\nSELECT\n    CODE,\n    CASE CODE\n        WHEN 'A'\n            THEN 'Alpha'\n        WHEN 'B'\n            THEN 'Bravo'\n        WHEN 'C'\n            THEN 'Charlie'\n            ELSE 'Unknown'\n    END AS PHONETIC\nFROM (\n    VALUES\n        ('A'),\n        ('B'),\n        ('C'),\n        ('D'),\n        (NULL)\n) AS CODES(CODE)\n</code></pre>"},{"location":"from-excel-to-sql/main-concepts/conditionals/#further-reading","title":"Further reading","text":"<p>Check out the official Microsoft documentation for more information on <code>IIF</code> and <code>CASE</code> at:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/t-sql/functions/logical-functions-iif-transact-sql</li> <li>https://learn.microsoft.com/en-us/sql/t-sql/language-elements/case-transact-sql</li> </ul> <p>The video version of this content is also available at:</p> <ul> <li>https://youtu.be/4admV4I3fMU</li> </ul>"},{"location":"from-excel-to-sql/main-concepts/data-types/","title":"Data types \ud83e\uddf1","text":"<p>Success</p> <p>Understanding data types is a fundamental part of working with SQL.</p> <p>Warning</p> <p>This is one of the biggest differences between Excel and SQL. In Excel, you can just type whatever you want into a cell, and it will be interpreted as a number, a date, or some text appropriately.</p> <p>In SQL, each column will have an explicit type of data that it can hold.</p>"},{"location":"from-excel-to-sql/main-concepts/data-types/#there-are-lots-of-data-types","title":"There are lots of data types","text":"<p>When we work with Excel, we really only think about two (maybe three) types of data:</p> <ul> <li>Numbers</li> <li>Text</li> <li>Dates (but these are really numbers, anyway)</li> </ul> <p>In SQL database, there are loads of different data types!</p> <p>Even something like numbers has several different data types, each covering a different range of numbers and whether to care about decimal places.</p> <p>This might feel like a pain, but SQL databases are able to do some things so well precisely because we have to be explicit about the data types. We won't go into why in this course, but just know that it's a good thing!</p>"},{"location":"from-excel-to-sql/main-concepts/data-types/#microsoft-sql-server-data-types","title":"Microsoft SQL Server data types","text":"<p>Warning</p> <p>You are not expected to understand this straight away. This is just to give you an idea of the variety of data types that are available.</p> <p>The data types that Microsoft SQL Server has (be default) are all documented at:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/t-sql/data-types/data-types-transact-sql</li> </ul> <p>We'll just call out a few of the most common ones here so that you can get a feel for what's available and start to recognise them.</p> Data type Description <code>INT</code> Whole numbers (no decimal places) in the range -2,147,483,648 to 2,147,483,647 (-2^31^ to 2^31^ - 1) <code>DECIMAL</code>* Numbers with decimal places. When maximum precision is used, valid values are from -10^38^ + 1 through 10^38^ - 1. <code>VARCHAR</code>* Variable-length string of text. Maximum length is specified. <code>NVARCHAR</code>* Variable-length string of text that allows unicode characters. Maximum length is specified. <code>DATE</code> Date values in the range 0001-01-01 through 9999-12-31. <code>TIME</code> Time values in the range 00:00:00.0000000 through 23:59:59.9999999. <code>DATETIME</code> Date and time values in the range 1753-01-01 through 9999-12-31. <p>Note</p> <p>The data types denoted with the <code>*</code> also require specifying a precision and scale (if they're a type of number) or a length (if they're a type of text).</p> <p>For example:</p> <ul> <li><code>DECIMAL(10, 2)</code> would be a number with 10 total digits and 2 decimal places (so 8 digits before the decimal place).</li> <li><code>VARCHAR(50)</code> would be a string of text with a maximum length of 50 characters.</li> </ul> <p>If you're not sure what data type the column you're working with is, you can check the <code>INFORMATION_SCHEMA.COLUMNS</code> table to see the data types of the columns in a table. For example, the query below shows the data types of the columns in the <code>HumanResources.Department</code> table:</p> <pre><code>SELECT COLUMN_NAME, DATA_TYPE\nFROM INFORMATION_SCHEMA.COLUMNS\nWHERE TABLE_SCHEMA = 'HumanResources'\n  AND TABLE_NAME = 'Department'\n;\n</code></pre> COLUMN_NAME DATA_TYPE DepartmentID smallint Name nvarchar GroupName nvarchar ModifiedDate datetime"},{"location":"from-excel-to-sql/main-concepts/data-types/#use-the-cast-function-to-change-data-types","title":"Use the <code>CAST</code> function to change data types","text":"<p>Now that we know what data types are available, we want to know how to change the data type of a column \ud83d\ude04</p> <p>It's worth noting that SQL will regularly do behind-the-scenes conversions of data types for you, but it's good to be explicit about what you're doing.</p> <p>When you want (or need) to be explicit about the data type of a column, use the <code>CAST</code> function:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/t-sql/functions/cast-and-convert-transact-sql</li> </ul> <p>This function looks a bit different to the functions that we'll have seen so far or in Excel. To use it, we write the column that we want to change the data type of, then the <code>AS</code> keyword, then the new data type that we want to change it to.</p> <p>For example, if we want to change the <code>DepartmentID</code> column to text or to a number with decimal places, we could use the following query:</p> <pre><code>SELECT TOP 5\n    DepartmentID,\n    CAST(DepartmentID AS VARCHAR(10)) AS DepartmentID_VARCHAR,\n    CAST(DepartmentID AS DECIMAL(8, 2)) AS DepartmentID_DECIMAL\nFROM HumanResources.Department\n;\n</code></pre> DepartmentID DepartmentID_VARCHAR DepartmentID_DECIMAL 12 12 12.00 1 1 1.00 16 16 16.00 14 14 14.00 10 10 10.00 <p>The text version looks the same, but I can guarantee that it's not a number any more! \ud83d\ude1d</p> <p>Note</p> <p>After using Excel for so long, this might feel a bit weird -- but Excel will sometimes hold numbers as text, and it's not always obvious when it's doing that.</p> <p>You can store a number as text yourself in Excel in loads of ways, for example using <code>=\"123\"</code>, <code>='123</code>, or just typing <code>123</code> into a cell that's formatted as text.</p>"},{"location":"from-excel-to-sql/main-concepts/data-types/#what-about-casting-something-that-cant-be-converted","title":"What about casting something that can't be converted?","text":"<p>If you try to cast a value to a data type that it can't be converted to, you'll get an error.</p> <p>For example, the following will break:</p> <pre><code>SELECT CAST('abc' AS INT)\n;\n</code></pre> <p>SQL doesn't know how to convert the text <code>'abc'</code> into a number, so it will complain!</p>"},{"location":"from-excel-to-sql/main-concepts/data-types/#null-is-a-special-value-that-can-be-used-in-any-data-type","title":"<code>NULL</code> is a special value that can be used in any data type","text":"<p>Warning</p> <p><code>NULL</code> values can be a pain to work with, but they're a fundamental part of SQL. It's worth getting to grips with them early on.</p> <p><code>NULL</code> is a special value that can be used in any data type. It's used to represent the absence of a value, and it's different to <code>0</code>, <code>''</code>, or any other value that you might use to represent \"nothing\".</p> <p>Excel has a similar concept, but it's not as explicit as it is in SQL. In Excel, your cells can be empty, and this is similar to <code>NULL</code> in SQL. An empty cell is not the same as a cell with a value of <code>0</code> or <code>\"\"</code>!</p>"},{"location":"from-excel-to-sql/main-concepts/data-types/#whats-this-mysterious","title":"What's this mysterious <code>''</code>/<code>\"\"</code>?","text":"<p>This is known as the \"empty string\", and Excel has it too -- <code>''</code> is the SQL version and <code>\"\"</code> is the Excel version.</p> <p>It's a piece of text that has no characters in it. \ud83d\ude04</p> <p>It's not the same as <code>NULL</code>, but it's also not the same as a string of text with a space in it!</p> <p>Warning</p> <p>If you've not seen this before, don't worry too much about it. It's just something to be aware of.</p>"},{"location":"from-excel-to-sql/main-concepts/data-types/#null-vs-vs-0","title":"<code>NULL</code> vs <code>''</code> vs <code>0</code>","text":"<p>The concept of a <code>NULL</code> value doesn't exist only in SQL, so when people start to learn about it for the first time, you'll usually see an image like the one below shown to help explain it:</p> <p></p> <p>Humour aside, it's a good way to think about it. <code>NULL</code> is not the same as <code>''</code> or <code>0</code>; rather than saying that something is \"empty\" or \"zero\", it's saying that there's no value there at all.</p>"},{"location":"from-excel-to-sql/main-concepts/data-types/#further-reading","title":"Further reading","text":"<p>Check out the official Microsoft documentation for more information on data types at:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/t-sql/data-types/data-types-transact-sql</li> </ul> <p>The docs for <code>NULL</code> are at:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/t-sql/language-elements/null-and-unknown-transact-sql</li> </ul> <p>The video version of this content is also available at:</p> <ul> <li>https://youtu.be/Vp40VH4_OX8</li> </ul>"},{"location":"from-excel-to-sql/main-concepts/date-formatting/","title":"Date formatting \ud83d\udcc6","text":"<p>Success</p> <p>Formatting dates in SQL is very similar to formatting dates in Excel -- the <code>FORMAT</code> function is SQL's equivalent of the <code>TEXT</code> function in Excel.</p> <p>Info</p> <p>I'm from the UK so I've used UK date formats in the examples, but you can find US date formats in the Microsoft SQL Server documentation.</p>"},{"location":"from-excel-to-sql/main-concepts/date-formatting/#formatting-dates-in-excel","title":"Formatting dates in Excel","text":"<p>You may have noticed that different systems/tools deal with dates differently. Excel stores dates as numbers where the number corresponds to how many days have passed since 1899-12-31:</p> <ul> <li>The date 1900-01-01 is the number 1</li> <li>The date 1900-02-01 is the number 32</li> <li>The date 2024-01-01 is the number 45292</li> </ul> Date Number Excel Formula 1900-01-01 1 <code>=TEXT(A1, \"#\")</code> 1900-01-02 2 <code>=TEXT(A2, \"#\")</code> 1900-01-03 3 <code>=TEXT(A3, \"#\")</code> ... ... 1900-01-31 31 <code>=TEXT(A5, \"#\")</code> 1900-02-01 32 <code>=TEXT(A6, \"#\")</code> 1900-02-02 33 <code>=TEXT(A7, \"#\")</code> ... ... 2024-01-01 45292 <code>=TEXT(A9, \"#\")</code> 2024-01-02 45293 <code>=TEXT(A10, \"#\")</code> 2024-01-03 45294 <code>=TEXT(A11, \"#\")</code> <p>This has many convenient implications, such as adding 1 to a date increases it by 1 day.</p>"},{"location":"from-excel-to-sql/main-concepts/date-formatting/#sql-dates-are-different-to-excel-dates","title":"SQL dates are different to Excel dates","text":"<p>SQL databases store dates differently, so applying the usual Excel-type logic to dates in SQL, such as adding the number 1 to a date won't work (in Microsoft SQL Server).</p> <p>Manipulating dates in SQL is outside the scope of this course, but it's worth getting up to speed with some of the date manipulation functions such as:</p> <ul> <li><code>DATEADD</code> for adding to or subtracting from dates</li> <li><code>DATEDIFF</code> for finding the difference between two dates</li> <li><code>DATEPART</code> for extracting parts of a date (year, quarter, day of year, etc.)</li> <li><code>YEAR</code>, <code>MONTH</code>, and <code>DAY</code> which are specific versions of <code>DATEPART</code></li> </ul>"},{"location":"from-excel-to-sql/main-concepts/date-formatting/#formatting-dates-in-sql","title":"Formatting dates in SQL","text":"<p>Although this course won't cover date manipulation, it will cover formatting dates. The two main functions to use for this are:</p> <ul> <li><code>CONVERT</code> which is good for formatting dates using pre-defined styles</li> <li><code>FORMAT</code> which is good for formatting dates using custom styles</li> </ul> <p>These functions are used in slightly different ways.</p>"},{"location":"from-excel-to-sql/main-concepts/date-formatting/#convert-has-three-parameters-and-has-some-pre-defined-styles","title":"<code>CONVERT</code> has three parameters and has some pre-defined styles","text":"<p>For <code>CONVERT</code>, you need to specify three parameters:</p> <ol> <li>The data type that you want to convert your data into, e.g. <code>VARCHAR</code></li> <li>The data that you want to convert, e.g. <code>SOME_DATE_COLUMN</code></li> <li>The style that you want to use, e.g. <code>103</code> for <code>dd/MM/yyyy</code></li> </ol> <p>The styles that you can use are all documented in the Microsoft SQL Server documentation:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/t-sql/functions/cast-and-convert-transact-sql#date-and-time-styles</li> </ul> <p>Here's an example of using <code>CONVERT</code> to format some order dates:</p> <pre><code>SELECT DISTINCT TOP 5\n    OrderDate,\n    CONVERT(VARCHAR, OrderDate, 101) AS OrderDateFormatted\nFROM Sales.SalesOrderHeader\n;\n</code></pre> OrderDate OrderDateFormatted 2011-05-31 00:00:00.000 05/31/2011 2011-06-01 00:00:00.000 06/01/2011 2011-06-02 00:00:00.000 06/02/2011 2011-06-03 00:00:00.000 06/03/2011 2011-06-04 00:00:00.000 06/04/2011"},{"location":"from-excel-to-sql/main-concepts/date-formatting/#format-has-two-required-parameters-and-is-good-for-custom-styles","title":"<code>FORMAT</code> has two (required) parameters and is good for custom styles","text":"<p>For <code>FORMAT</code>, you need to specify two parameters:</p> <ol> <li>The data that you want to format, e.g. <code>SOME_DATE_COLUMN</code></li> <li>The style that you want to use, e.g. <code>MM/dd/yyyy</code></li> </ol> <p>Similar to Excel, there are pre-defined characters which correspond to different parts of the date. For example, <code>MM</code> is the month, <code>dd</code> is the day, and <code>yyyy</code> is the year.</p> <p>These pre-defined characters are different to Excel's characters and, unlike Excel's, are case-sensitive -- you can find the SQL characters in the Microsoft SQL Server documentation:</p> <ul> <li>https://learn.microsoft.com/en-us/dotnet/standard/base-types/standard-date-and-time-format-strings#table-of-format-specifiers</li> </ul> <p>Here's an example of using <code>FORMAT</code> to format some order dates:</p> <pre><code>SELECT DISTINCT TOP 5\n    OrderDate,\n    FORMAT(OrderDate, 'MM/dd/yyyy') AS OrderDateFormatted\nFROM Sales.SalesOrderHeader\n;\n</code></pre> OrderDate OrderDateFormatted 2011-05-31 00:00:00.000 05/31/2011 2011-06-01 00:00:00.000 06/01/2011 2011-06-02 00:00:00.000 06/02/2011 2011-06-03 00:00:00.000 06/03/2011 2011-06-04 00:00:00.000 06/04/2011 <p>Note that this is the same result as the <code>CONVERT</code> example above -- there is a lot of overlap between the two functions, so it's up to you which one you use.</p>"},{"location":"from-excel-to-sql/main-concepts/date-formatting/#more-formatting-examples-and-handy-styles","title":"More formatting examples and handy styles","text":"<p>It can be a bit intimidating getting up to speed with these different styles and characters, so some of the most common ones that you'll likely need are below for reference.</p> <p>Note that some of these formats don't have a pre-defined style for <code>CONVERT</code>, so we have to use <code>FORMAT</code> for them. The examples are all how the date <code>2024-12-01</code> with the time <code>11:30:45</code> would be formatted:</p> Style (<code>CONVERT</code>) Format (<code>FORMAT</code>) Example <code>103</code> <code>dd/MM/yyyy</code> 01/12/2024 <code>106</code> <code>dd MMM yyyy</code> 01 Dec 2024 <code>108</code> <code>HH:mm:ss</code> 11:30:45 <code>120</code> <code>yyyy-MM-dd HH:mm:ss</code> 2024-12-01 11:30:45 <code>109</code> <code>MMM d yyyy HH:mm:ss:ffftt</code> Dec 1 2024 11:30:45:000AM <code>yyyy-MM</code> 2024-12 <code>MMM-yy</code> Dec-24 <code>MMMM yyyy</code> December 2024 <code>dddd, dd MMMM yyyy</code> Sunday, 01 December 2024 Expand for some SQL to run yourself  These are more examples created just for this course, so they aren't in the AdventureWorks database. If you want to run these examples yourself, you can use the SQL below -- but note that this is using some features that we haven't covered yet!  <pre><code>SELECT\n    CONVERT(VARCHAR, EXAMPLE_DATE, 103),\n    FORMAT(EXAMPLE_DATE, 'dd/MM/yyyy'),\n    CONVERT(VARCHAR, EXAMPLE_DATE, 106),\n    FORMAT(EXAMPLE_DATE, 'dd MMM yyyy'),\n    CONVERT(VARCHAR, EXAMPLE_DATE, 108),\n    FORMAT(EXAMPLE_DATE, 'HH:mm:ss'),\n    CONVERT(VARCHAR, EXAMPLE_DATE, 120),\n    FORMAT(EXAMPLE_DATE, 'yyyy-MM-dd HH:mm:ss'),\n    CONVERT(VARCHAR, EXAMPLE_DATE, 109),\n    FORMAT(EXAMPLE_DATE, 'MMM d yyyy HH:mm:ss:ffftt'),\n    FORMAT(EXAMPLE_DATE, 'yyyy-MM'),\n    FORMAT(EXAMPLE_DATE, 'MMM-yy'),\n    FORMAT(EXAMPLE_DATE, 'MMMM yyyy'),\n    FORMAT(EXAMPLE_DATE, 'dddd, dd MMMM yyyy')\nFROM (\n    VALUES (CAST('2024-12-01 11:30:45' AS DATETIME))\n) AS V(EXAMPLE_DATE)\n;\n</code></pre>"},{"location":"from-excel-to-sql/main-concepts/date-formatting/#further-reading","title":"Further reading","text":"<p>Check out the official Microsoft documentation for more information on <code>CONVERT</code> and <code>FORMAT</code> at:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/t-sql/functions/cast-and-convert-transact-sql</li> <li>https://learn.microsoft.com/en-us/sql/t-sql/functions/format-transact-sql</li> </ul> <p>The video version of this content is also available at:</p> <ul> <li>https://youtu.be/gYjYLL99jaQ</li> </ul>"},{"location":"from-excel-to-sql/main-concepts/date-formatting/#additional-date-functions","title":"Additional date functions","text":"<p>Manipulating dates in SQL is fairly different to Excel, but it's outside the scope of this course. However, it's recommended that you check some out to see what options are available:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/t-sql/functions/dateadd-transact-sql</li> <li>https://learn.microsoft.com/en-us/sql/t-sql/functions/datediff-transact-sql</li> <li>https://learn.microsoft.com/en-us/sql/t-sql/functions/datepart-transact-sql</li> <li>https://learn.microsoft.com/en-us/sql/t-sql/functions/year-transact-sql</li> <li>https://learn.microsoft.com/en-us/sql/t-sql/functions/month-transact-sql</li> <li>https://learn.microsoft.com/en-us/sql/t-sql/functions/day-transact-sql</li> </ul>"},{"location":"from-excel-to-sql/main-concepts/group-by/","title":"Aggregations \ud83d\udcca","text":"<p>Success</p> <p>One of the most common ways to use SQL is to aggregate data!</p> <p>Note</p> <p>The <code>GROUP BY</code> clause is optional. If you use it, it must come after the <code>WHERE</code> clause and before the <code>ORDER BY</code> clause.</p>"},{"location":"from-excel-to-sql/main-concepts/group-by/#there-are-two-parts-to-aggregating-data","title":"There are two parts to aggregating data","text":"<p>By now, we know how to get some data, how to filter it, how to sort it, and how to do some simple manipulations on it.</p> <p>To aggregate rows, use aggregate functions and the <code>GROUP BY</code> clause.</p> <p>Let's break this down into the two parts: aggregate functions and the <code>GROUP BY</code> clause.</p>"},{"location":"from-excel-to-sql/main-concepts/group-by/#the-aggregate-functions-are-how-we-aggregate-the-data","title":"The aggregate functions are how we aggregate the data","text":"<p>Info</p> <p>\"Aggregating\" data is just a fancy way of saying \"summarising\" data, such as counting the number of rows, finding the average of a column, finding the maximum value in a column, and so on.</p> <p>To tell SQL that we want to aggregate rows, we use some of these \"aggregate functions\". These functions are used after the <code>SELECT</code> clause where we'd normally list the columns.</p> <p>SQL has a bunch of aggregate functions, but the most common ones are:</p> <ul> <li><code>COUNT</code>, which counts the number of rows</li> <li><code>SUM</code>, which adds up the values in a column</li> <li><code>AVG</code>, which finds the average of the values in a column</li> <li><code>MIN</code>, which finds the minimum value in a column</li> <li><code>MAX</code>, which finds the maximum value in a column</li> </ul> <p>If we want to aggregate/summarise a table, we can just use one of these functions in the <code>SELECT</code> clause. For example:</p> <pre><code>SELECT\n    /* We'll explain these `COUNT` contents below */\n    COUNT(*) AS TotalRows,\n    COUNT(DISTINCT SalesOrderID) AS TotalOrders,\n\n    AVG(1.0 * OrderQty) AS AverageOrderQuantity,\n    SUM(LineTotal) AS TotalSalesAmount,\n    MIN(ModifiedDate) AS FirstModifiedDate,\n    MAX(ModifiedDate) AS LastModifiedDate\nFROM Sales.SalesOrderDetail\n;\n</code></pre> TotalRows TotalOrders AverageOrderQuantity TotalSalesAmount FirstModifiedDate LastModifiedDate 121317 31465 2.266079 109846381.399888 2011-05-31 00:00:00.000 2014-06-30 00:00:00.000 <p>Info</p> <p>Do you remember why we have had to use <code>1.0 * OrderQty</code> inside the <code>AVG</code> function?</p> <p>See the Operators section for a refresher.</p> <p>Without the <code>GROUP BY</code> clause, these functions will aggregate the entire table, so we get a single row as a result.</p> <p>However, it's super common to want to aggregate data by some grouping, such as by a category or by a time period. This is where the <code>GROUP BY</code> clause comes in!</p> <p>Success</p> <p>Summarising data like this is like using summary functions in Excel over an entire column, such as <code>=SUM(A:A)</code>!</p>"},{"location":"from-excel-to-sql/main-concepts/group-by/#count-can-be-used-in-a-few-ways","title":"<code>COUNT</code> can be used in a few ways","text":"<p>Before jumping into the <code>GROUP BY</code> clause, let's take a closer look at the <code>COUNT</code> function. For some column called <code>COLUMN_NAME</code>, the main ways to use <code>COUNT</code> are described below:</p> <ul> <li><code>COUNT(*)</code> counts the number of rows in the table/group</li> <li><code>COUNT(COLUMN_NAME)</code> counts the number of non-<code>NULL</code> values in the column/group</li> <li><code>COUNT(DISTINCT COLUMN_NAME)</code> counts the number of unique values in the column/group</li> </ul> <p>Each way of using <code>COUNT</code> has its own use case, and you'll see all of them out in the wild.</p> <p>Let's see these three ways of using <code>COUNT</code> in action on the first 10 people in the <code>Person.Person</code> table:</p> <pre><code>SELECT\n    BusinessEntityID,\n    FirstName,\n    MiddleName,\n    LastName\nFROM Person.Person\nWHERE BusinessEntityID &lt;= 10\n;\n</code></pre> BusinessEntityID FirstName MiddleName LastName 1 Ken J S\u00e1nchez 2 Terri Lee Duffy 3 Roberto null Tamburello 4 Rob null Walters 5 Gail A Erickson 6 Jossef H Goldberg 7 Dylan A Miller 8 Diane L Margheim 9 Gigi N Matthew 10 Michael null Raheem <p>The <code>MiddleName</code> has a variety of values, including <code>NULL</code>. Here are the results of the three ways of using <code>COUNT</code> on this column for these rows:</p> <pre><code>SELECT\n    COUNT(*) AS ROW_COUNT,\n    COUNT(MiddleName) AS NON_NULL_COUNT,\n    COUNT(DISTINCT MiddleName) AS DISTINCT_COUNT\nFROM Person.Person\nWHERE BusinessEntityID &lt;= 10\n;\n</code></pre> ROW_COUNT NON_NULL_COUNT DISTINCT_COUNT 10 7 6 <p>The <code>ROW_COUNT</code> is just the number of rows, which is 10. The <code>NON_NULL_COUNT</code> is the number of non-<code>NULL</code> values, which is 7. The <code>DISTINCT_COUNT</code> is the number of unique values, which is 6. In particular, they are:</p> <ul> <li><code>J</code></li> <li><code>Lee</code></li> <li><code>A</code></li> <li><code>H</code></li> <li><code>L</code></li> <li><code>N</code></li> </ul> <p>Note how the <code>NULL</code> values are not counted in the <code>DISTINCT_COUNT</code> -- this is a common behaviour in SQL.</p> <p>Tip</p> <p>Since each of the ways above has a different use case, make sure you use the right one for your needs!</p>"},{"location":"from-excel-to-sql/main-concepts/group-by/#the-group-by-clause-summarises-data-by-a-category-or-categories","title":"The <code>GROUP BY</code> clause summarises data by a category (or categories)","text":"<p>Note</p> <p>To tell SQL that we want to aggregate rows by a category (or categories), we use the <code>GROUP BY</code> clause. This clause comes after the <code>WHERE</code> clause and before the <code>ORDER BY</code> clause.</p> <p>The examples above are just producing a single row with the \"overall\" aggregate/summary values.</p> <p>Using the <code>GROUP BY</code> clause allows us to group the data by one or more columns.</p> <p>To help illustrate why this is helpful, it might be useful to think about what kind of questions you can answer with this.</p> <p>Without the <code>GROUP BY</code> clause, the kinds of questions you can answer are things like:</p> <ul> <li>\"What are the total sales over all orders?\"</li> <li>\"What is the average order quantity over all orders?\"</li> <li>\"What is the minimum and maximum unit price over all orders?\"</li> </ul> <p>With the <code>GROUP BY</code> clause, you can answer questions like:</p> <ul> <li>\"What are the total sales per order?\"</li> <li>\"What is the average order quantity per month?\"</li> <li>\"What is the minimum and maximum unit price per product?\"</li> </ul> <p>To see this in action, the example above can be adjusted to group by the <code>SalesOrderID</code> column which would give us the summary statistics for each order:</p> <pre><code>SELECT TOP 5\n    SalesOrderID,\n\n    COUNT(*) AS TotalRowsInOrder,\n    AVG(1.0 * OrderQty) AS AverageOrderQuantity,\n    SUM(OrderQty) AS TotalItemsOrdered,\n    SUM(LineTotal) AS TotalSalesAmount,\n    MIN(UnitPrice) AS CheapestItemPrice,\n    MAX(UnitPrice) AS MostExpensiveItemPrice\nFROM Sales.SalesOrderDetail\nGROUP BY SalesOrderID\n;\n</code></pre> SalesOrderID TotalRowsInOrder AverageOrderQuantity TotalItemsOrdered TotalSalesAmount CheapestItemPrice MostExpensiveItemPrice 43659 12 2.166666 26 20565.620600 5.1865 2039.9940 43660 2 1.000000 2 1294.252900 419.4589 874.7940 43661 15 2.533333 38 32726.478600 5.1865 2039.9940 43662 22 2.454545 54 28832.528900 178.5808 2146.9620 43663 1 1.000000 1 419.458900 419.4589 419.4589"},{"location":"from-excel-to-sql/main-concepts/group-by/#calculated-columns-need-to-be-re-calculated-in-the-group-by-clause","title":"Calculated columns need to be \"re-calculated\" in the <code>GROUP BY</code> clause","text":"<p>Info</p> <p>This is not the case for many SQL dialects, but it is for Microsoft SQL Server.</p> <p>If you want to group by a column that is calculated in the <code>SELECT</code> clause, you need to repeat the calculation in the <code>GROUP BY</code> clause.</p> <p>For example, if we wanted to aggregate the sales by the order month, we would need to convert the order date into a month both in the <code>SELECT</code> and the <code>GROUP BY</code> lists:</p> <pre><code>SELECT\n    FORMAT(OrderDate, 'yyyy-MM') AS OrderMonth,\n    SUM(TotalDue) AS TotalSales\nFROM Sales.SalesOrderHeader\nWHERE YEAR(OrderDate) = 2013\nGROUP BY FORMAT(OrderDate, 'yyyy-MM')\nORDER BY OrderMonth\n;\n</code></pre> OrderMonth TotalSales 2013-01 2340061.5521 2013-02 2600218.8667 2013-03 3831605.9389 2013-04 2840711.1734 2013-05 3658084.9461 2013-06 5726265.2635 2013-07 5521840.8445 2013-08 3733973.0032 2013-09 5083505.3374 2013-10 5374375.9418 2013-11 3694667.9998 2013-12 4560577.0958 <p>Remember that we don't need to repeat the calculation in the <code>ORDER BY</code> clause though -- it's one of the few places where we can use the alias.</p>"},{"location":"from-excel-to-sql/main-concepts/group-by/#you-can-group-by-one-or-more-columns","title":"You can group by one or more columns","text":"<p>To group by more than one column, list the columns in the <code>GROUP BY</code> clause!</p> <p>The example below is a more involved query to show how we can calculate the total sales for each month in 2013, split by whether the order was placed online or not:</p> <pre><code>SELECT\n    FORMAT(OrderDate, 'yyyy-MM') AS OrderMonth,\n    OnlineOrderFlag,\n    SUM(TotalDue) AS TotalSales\nFROM Sales.SalesOrderHeader\nWHERE YEAR(OrderDate) = 2013\nGROUP BY\n    FORMAT(OrderDate, 'yyyy-MM'),\n    OnlineOrderFlag\nORDER BY\n    OrderMonth,\n    OnlineOrderFlag\n;\n</code></pre> OrderMonth OnlineOrderFlag TotalSales 2013-01 false 1761132.8322 2013-01 true 578928.7199 2013-02 false 2101152.5476 2013-02 true 499066.3191 2013-03 false 3244501.4287 2013-03 true 587104.5102 2013-04 false 2239156.6675 2013-04 true 601554.5059 2013-05 false 3019173.6253 2013-05 true 638911.3208 2013-06 false 4775809.3027 2013-06 true 950455.9608 2013-07 false 4585752.5978 2013-07 true 936088.2467 2013-08 false 2573825.2983 2013-08 true 1160147.7049 2013-09 false 3927650.1373 2013-09 true 1155855.2001 2013-10 false 3954900.7112 2013-10 true 1419475.2306 2013-11 false 1878956.5046 2013-11 true 1815711.4952 2013-12 false 3044875.2799 2013-12 true 1515701.8159 <p>Although SQL itself doesn't have any graphing capabilities, this would be easy to drop into Excel or a similar tool to create a graph like the one below!</p> <pre><code>xychart-beta\n    title \"Monthly Sales (2013) by Online Order Flag\"\n    x-axis \"Offline Orders (Bar) | Online Orders (Line)\" [Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec]\n    bar \"Offline Orders\" [1761132.8322, 2101152.5476, 3244501.4287, 2239156.6675, 3019173.6253, 4775809.3027, 4585752.5978, 2573825.2983, 3927650.1373, 3954900.7112, 1878956.5046, 3044875.2799]\n    line \"Online Orders\" [578928.7199, 499066.3191, 587104.5102, 601554.5059, 638911.3208, 950455.9608, 936088.2467, 1160147.7049, 1155855.2001, 1419475.2306, 1815711.4952, 1515701.8159]</code></pre>"},{"location":"from-excel-to-sql/main-concepts/group-by/#you-can-filter-the-aggregated-rows-with-having","title":"You can filter the aggregated rows with <code>HAVING</code>","text":"<p>Note</p> <p>The <code>HAVING</code> clause is optional. If you use it, it must come after the <code>GROUP BY</code> clause.</p> <p>A handy way to filter the aggregated rows is to use the <code>HAVING</code> clause. This clause is similar to the <code>WHERE</code> clause, but it filters the aggregated rows -- that is, it does the filtering after the aggregation.</p> <p>To use the <code>HAVING</code> clause, add it after the <code>GROUP BY</code> clause. For example, we could extend the monthly summary example above to keep only the months when the total sales were more than $5m:</p> <pre><code>SELECT\n    FORMAT(OrderDate, 'yyyy-MM') AS OrderMonth,\n    SUM(TotalDue) AS TotalSales\nFROM Sales.SalesOrderHeader\nWHERE YEAR(OrderDate) = 2013\nGROUP BY FORMAT(OrderDate, 'yyyy-MM')\nHAVING SUM(TotalDue) &gt; 5000000\nORDER BY OrderMonth\n;\n</code></pre> OrderMonth TotalSales 2013-06 5726265.2635 2013-07 5521840.8445 2013-09 5083505.3374 2013-10 5374375.9418 <p>Warning</p> <p>There are two things to be careful of with the <code>HAVING</code> clause:</p> <ol> <li>This is similar to the <code>WHERE</code> clause, but it is not the same. Reserve it for filtering by aggregated values and continue to use the <code>WHERE</code> clause for filtering the original/underlying rows.</li> <li>You cannot use column aliases in the <code>HAVING</code> clause, so you need to repeat the calculation.</li> </ol>"},{"location":"from-excel-to-sql/main-concepts/group-by/#further-reading","title":"Further reading","text":"<p>Check out the official Microsoft documentation for more information on the <code>GROUP BY</code> clause at:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/t-sql/queries/select-group-by-transact-sql</li> <li>https://learn.microsoft.com/en-us/sql/t-sql/functions/aggregate-functions-transact-sql</li> </ul> <p>The video version of this content is also available at:</p> <ul> <li>https://youtu.be/Tlckp6UbmQE</li> </ul>"},{"location":"from-excel-to-sql/main-concepts/group-by/#additional-modifiers","title":"Additional modifiers","text":"<p>The <code>GROUP BY</code> clause also has additional modifiers including <code>ROLLUP</code>, <code>GROUPING SETS</code>, and <code>CUBE</code>. We'll see <code>ROLLUP</code> in the next section and both <code>GROUPING SETS</code> and <code>CUBE</code> and the advanced aggregations section.</p>"},{"location":"from-excel-to-sql/main-concepts/join/","title":"Joins \ud83d\udd17","text":"<p>Success</p> <p>Joins are how we combine data from multiple tables. They're like the <code>LOOKUP</code> functions in Excel (but with some important differences)!</p> <p>Note</p> <p>The <code>JOIN</code> clause is optional. If you use it, it must come after the <code>FROM</code> clause and before the <code>WHERE</code> clause.</p>"},{"location":"from-excel-to-sql/main-concepts/join/#the-join-clause-combines-data-from-tables","title":"The <code>JOIN</code> clause combines data from tables","text":"<p>To get data from another table in Excel, you'd likely use one of the <code>LOOKUP</code> functions:</p> <ul> <li><code>VLOOKUP</code></li> <li><code>HLOOKUP</code></li> <li><code>XLOOKUP</code></li> </ul> <p>These functions keep your data in the same shape as the original table, but they pull in data from another table based on a common value.</p> <p>In SQL, we use the <code>JOIN</code> clause to do a similar thing. The <code>JOIN</code> clause is used to combine rows between tables based on a related column between them (usually).</p> <p>In Microsoft SQL Server, there are a few different types of <code>JOIN</code> clauses:</p> <ul> <li><code>INNER JOIN</code></li> <li><code>LEFT JOIN</code></li> <li><code>RIGHT JOIN</code></li> <li><code>FULL JOIN</code></li> <li><code>CROSS JOIN</code></li> </ul> <p>We'll only cover the <code>INNER JOIN</code> and <code>LEFT JOIN</code> in this course, but you can find more information on the others in the official Microsoft documentation.</p>"},{"location":"from-excel-to-sql/main-concepts/join/#the-join-syntax","title":"The <code>JOIN</code> syntax","text":"<p>To add a join to a query, you need to add the <code>JOIN</code> clause after the <code>FROM</code> clause and before the <code>WHERE</code> clause. We also need to tell SQL how these tables are related using the <code>ON</code> clause, for example:</p> <pre><code>SELECT *\nFROM HumanResources.Employee\n    INNER JOIN HumanResources.vEmployeeDepartment\n        ON Employee.BusinessEntityID = vEmployeeDepartment.BusinessEntityID\n;\n</code></pre> <p>This is a fairly intuitive example:</p> <ul> <li>The \"base\" table (in the <code>FROM</code> clause) is <code>HumanResources.Employee</code> which has the employee information.</li> <li>The \"joined\" table (in the <code>INNER JOIN</code> clause) is <code>HumanResources.vEmployeeDepartment</code> which has the employee's current department.</li> <li>The relationship between the two tables is that the <code>BusinessEntityID</code> in the <code>Employee</code> table matches the <code>BusinessEntityID</code> in the <code>vEmployeeDepartment</code> table. This would be similar to the <code>VLOOKUP</code> function in Excel where the <code>Employee.BusinessEntityID</code> is the lookup value and the <code>vEmployeeDepartment.BusinessEntityID</code> is the lookup column.</li> </ul> <p>Notice how we've had to prefix the <code>BusinessEntityID</code> with the table name in the <code>ON</code> clause. This is because both tables have this column, so SQL needs us to prefix them to know which one we're referring to.</p> <p>When we join tables, we don't usually want to <code>SELECT *</code> like we have above. Instead, we explicitly list the columns we want to see from each table:</p> <pre><code>SELECT\n    Employee.BusinessEntityID,\n    Employee.NationalIDNumber,\n    vEmployeeDepartment.FirstName,\n    vEmployeeDepartment.LastName,\n    vEmployeeDepartment.Department,\n    vEmployeeDepartment.JobTitle\nFROM HumanResources.Employee\n    LEFT JOIN HumanResources.vEmployeeDepartment\n        ON Employee.BusinessEntityID = vEmployeeDepartment.BusinessEntityID\n;\n</code></pre> BusinessEntityID NationalIDNumber FirstName LastName Department JobTitle 1 295847284 Ken S\u00e1nchez Executive Chief Executive Officer 2 245797967 Terri Duffy Engineering Vice President of Engineering 3 509647174 Roberto Tamburello Engineering Engineering Manager 4 112457891 Rob Walters Tool Design Senior Tool Designer 5 695256908 Gail Erickson Engineering Design Engineer <p>Tip</p> <p>Although you only need to prefix the columns that are ambiguous (exist in both tables), it's a good idea to prefix all columns in a query when there's a join. This makes it easier to read and understand the query, and it can help avoid errors if the tables change in the future.</p>"},{"location":"from-excel-to-sql/main-concepts/join/#we-can-aliasrename-tables-like-we-can-with-columns","title":"We can alias/rename tables like we can with columns","text":"<p>When the table names are long, it can be helpful to give them a shorter name for the rest of the query. The way that we do this is identical to how we alias columns by using the <code>AS</code> keyword:</p> <pre><code>SELECT\n    Emp.BusinessEntityID,\n    Emp.NationalIDNumber,\n    Dep.FirstName,\n    Dep.LastName,\n    Dep.Department,\n    Dep.JobTitle\nFROM HumanResources.Employee AS Emp\n    LEFT JOIN HumanResources.vEmployeeDepartment AS Dep\n        ON Emp.BusinessEntityID = Dep.BusinessEntityID\n;\n</code></pre> <p>Tip</p> <p>Although you can alias the tables to whatever you want, please try to make the alias meaningful. This makes the query easier to read and understand!</p>"},{"location":"from-excel-to-sql/main-concepts/join/#step-by-step-examples","title":"Step-by-step examples","text":"<p>We've just seen the <code>LEFT JOIN</code> clause in action, but let's break it down a bit more with some examples.</p> <p>For these examples, we'll use the following fake tables:</p> <p><code>Employee</code></p> EmployeeID EmployeeName DepartmentID 1 Alice 1 2 Bob 1 3 Charlie 2 4 Dave 2 5 Eve 3 <p><code>Department</code></p> DepartmentID DepartmentName 1 Sales 2 Marketing <p><code>Address</code></p> EmployeeID Address FromDate ToDate 1 1 Main Street 2001-07-21 2002-10-30 2 2 Rocky Road 2012-07-04 2018-02-11 5 5 Log Lane 2009-11-19 2020-03-15 5 6 Claw Close 2023-03-16 2024-12-31"},{"location":"from-excel-to-sql/main-concepts/join/#employee-left-join-department","title":"<code>Employee LEFT JOIN Department</code>","text":"<p>Suppose we write a query that <code>LEFT JOIN</code>s the <code>Department</code> table onto the <code>Employee</code> table using the <code>DepartmentID</code>:</p> <pre><code>SELECT\n    Employee.EmployeeID,\n    Employee.EmployeeName,\n    Employee.DepartmentID,\n    Department.DepartmentName\nFROM Employee\n    LEFT JOIN Department\n        ON Employee.DepartmentID = Department.DepartmentID\n;\n</code></pre> <p>Since the <code>Employee</code> table is the \"base\" table (it's in the <code>FROM</code> clause), let's break down what's happening with each row in this table.</p>"},{"location":"from-excel-to-sql/main-concepts/join/#employee-1","title":"Employee 1","text":"<p>The first row is employee 1:</p> EmployeeID EmployeeName DepartmentID 1 Alice 1 <p>We're joining the <code>Department</code> table to this on the <code>DepartmentID</code> column, specifically:</p> <pre><code>ON Employee.DepartmentID = Department.DepartmentID\n</code></pre> <p>This means that the information we get from the <code>Department</code> table will be where the <code>DepartmentID</code> in the <code>Department</code> table matches the <code>DepartmentID</code> in the <code>Employee</code> table which, in this case, is 1:</p> DepartmentID DepartmentName \u2192 1 Sales 2 Marketing <p>That means that we take the <code>Sales</code> value from this table for the first row of the <code>Employee</code> table:</p> EmployeeID EmployeeName DepartmentID DepartmentName 1 Alice 1 Sales <p>This is just like a lookup in Excel!</p>"},{"location":"from-excel-to-sql/main-concepts/join/#employee-2","title":"Employee 2","text":"<p>The second row is employee 2:</p> EmployeeID EmployeeName DepartmentID 2 Bob 1 <p>Bob has the same <code>DepartmentID</code> as Alice, so the steps for Bob are the same as they were for Alice -- we get the <code>Sales</code> value from the <code>Department</code> table:</p> EmployeeID EmployeeName DepartmentID DepartmentName 2 Bob 1 Sales"},{"location":"from-excel-to-sql/main-concepts/join/#employee-3","title":"Employee 3","text":"<p>The third row is employee 3:</p> EmployeeID EmployeeName DepartmentID 3 Charlie 2 <p>This time, the <code>DepartmentID</code> is 2:</p> DepartmentID DepartmentName 1 Sales \u2192 2 Marketing <p>Therefore, we get the <code>Marketing</code> value from the <code>Department</code> table for Charlie:</p> EmployeeID EmployeeName DepartmentID DepartmentName 3 Charlie 2 Marketing"},{"location":"from-excel-to-sql/main-concepts/join/#employee-4","title":"Employee 4","text":"<p>The fourth row is employee 4:</p> EmployeeID EmployeeName DepartmentID 4 Dave 2 <p>Dave also has a <code>DepartmentID</code> of 2, so we take the <code>Marketing</code> value from the <code>Department</code> table for Dave:</p> EmployeeID EmployeeName DepartmentID DepartmentName 4 Dave 2 Marketing"},{"location":"from-excel-to-sql/main-concepts/join/#employee-5","title":"Employee 5","text":"<p>The fifth row is employee 5:</p> EmployeeID EmployeeName DepartmentID 5 Eve 3 <p>This is an interesting case. Eve has a <code>DepartmentID</code> of 3, but there's no <code>DepartmentID</code> of 3 in the <code>Department</code> table:</p> DepartmentID DepartmentName 1 Sales 2 Marketing <p>This means that we can't find a <code>DepartmentName</code> for Eve, so we instead get a <code>NULL</code> value for the <code>DepartmentName</code>:</p> EmployeeID EmployeeName DepartmentID DepartmentName 5 Eve 3 NULL"},{"location":"from-excel-to-sql/main-concepts/join/#putting-it-all-together","title":"Putting it all together","text":"<p>When we put all of these rows together, we get the following result:</p> EmployeeID EmployeeName DepartmentID DepartmentName 1 Alice 1 Sales 2 Bob 1 Sales 3 Charlie 2 Marketing 4 Dave 2 Marketing 5 Eve 3 null <p>This is how the <code>LEFT JOIN</code> clause works -- it grabs whatever data it can from the \"joined\" table and fills in <code>NULL</code> values where it can't find a match.</p>"},{"location":"from-excel-to-sql/main-concepts/join/#employee-inner-join-department","title":"<code>Employee INNER JOIN Department</code>","text":"<p>Now let's look at the <code>INNER JOIN</code> clause. We'll use the exact same query as the last example, but we'll change the <code>LEFT JOIN</code> to an <code>INNER JOIN</code>:</p> <pre><code>SELECT\n    Employee.EmployeeID,\n    Employee.EmployeeName,\n    Employee.DepartmentID,\n    Department.DepartmentName\nFROM Employee\n    INNER JOIN Department\n        ON Employee.DepartmentID = Department.DepartmentID\n;\n</code></pre> <p>We could find a match for the first four rows, so they are the same as they were in the <code>LEFT JOIN</code> example:</p> EmployeeID EmployeeName DepartmentID DepartmentName 1 Alice 1 Sales 2 Bob 1 Sales 3 Charlie 2 Marketing 4 Dave 2 Marketing <p>However, what happens with employee 5 is different: an <code>INNER JOIN</code> only keeps rows where there's a match!</p> <p>Since there's no match for employee 5 in the <code>Department</code> table, we don't get a row for Eve at all! This means that the result of the <code>INNER JOIN</code> is:</p> EmployeeID EmployeeName DepartmentID DepartmentName 1 Alice 1 Sales 2 Bob 1 Sales 3 Charlie 2 Marketing 4 Dave 2 Marketing <p>This is the main difference between the <code>LEFT JOIN</code> and the <code>INNER JOIN</code> clauses and is what catches a lot of people out when they're new to SQL.</p> <p>Tip</p> <p>If you're not sure which join type to use, it's usually best to start with a <code>LEFT JOIN</code> and then change it to an <code>INNER JOIN</code> when you're comfortable that you're not missing any data.</p>"},{"location":"from-excel-to-sql/main-concepts/join/#employee-left-join-address","title":"<code>Employee LEFT JOIN Address</code>","text":"<p>This time, suppose we write a query that <code>LEFT JOIN</code>s the <code>Address</code> table onto the <code>Employee</code> table using the <code>EmployeeID</code>:</p> <pre><code>SELECT\n    Employee.EmployeeID,\n    Employee.EmployeeName,\n    Address.Address,\n    Address.FromDate,\n    Address.ToDate\nFROM Employee\n    LEFT JOIN Address\n        ON Employee.EmployeeID = Address.EmployeeID\n;\n</code></pre> <p>As with the last example, we'll break down what's happening with each row in the <code>Employee</code> table. We'll ignore the <code>DepartmentID</code> column in the <code>Employee</code> since we're not using it in the query.</p>"},{"location":"from-excel-to-sql/main-concepts/join/#employee-1_1","title":"Employee 1","text":"<p>The first row is employee 1:</p> EmployeeID EmployeeName 1 Alice <p>This time, we're joining the <code>Address</code> table to this on the <code>EmployeeID</code> column, specifically:</p> <pre><code>ON Employee.EmployeeID = Address.EmployeeID\n</code></pre> <p>This means that the information we get from the <code>Address</code> table will be where the <code>EmployeeID</code> in the <code>Address</code> table matches the <code>EmployeeID</code> in the <code>Employee</code> table which, in this case, is 1:</p> EmployeeID Address FromDate ToDate \u2192 1 1 Main Street 2001-07-21 2002-10-30 2 2 Rocky Road 2012-07-04 2018-02-11 5 5 Log Lane 2009-11-19 2020-03-15 5 6 Claw Close 2023-03-16 2024-12-31 <p>That means that we take the <code>1 Main Street</code> address (plus the dates) from this table for the first row of the <code>Employee</code> table:</p> EmployeeID EmployeeName Address FromDate ToDate 1 Alice 1 Main Street 2001-07-21 2002-10-30"},{"location":"from-excel-to-sql/main-concepts/join/#employee-2_1","title":"Employee 2","text":"<p>The second row is employee 2:</p> EmployeeID EmployeeName 2 Bob <p>Bob's <code>EmployeeID</code> is 2, so we look for the address in the <code>Address</code> table where the <code>EmployeeID</code> is 2:</p> EmployeeID Address FromDate ToDate 1 1 Main Street 2001-07-21 2002-10-30 \u2192 2 2 Rocky Road 2012-07-04 2018-02-11 5 5 Log Lane 2009-11-19 2020-03-15 5 6 Claw Close 2023-03-16 2024-12-31 <p>This means that we get the <code>2 Rocky Road</code> address (plus the dates) from the <code>Address</code> table for Bob:</p> EmployeeID EmployeeName Address FromDate ToDate 2 Bob 2 Rocky Road 2012-07-04 2018-02-11"},{"location":"from-excel-to-sql/main-concepts/join/#employee-3_1","title":"Employee 3","text":"<p>The third row is employee 3:</p> EmployeeID EmployeeName 3 Charlie <p>Charlie's <code>EmployeeID</code> is 3, so we look for the address in the <code>Address</code> table where the <code>EmployeeID</code> is 3:</p> EmployeeID Address FromDate ToDate 1 1 Main Street 2001-07-21 2002-10-30 2 2 Rocky Road 2012-07-04 2018-02-11 5 5 Log Lane 2009-11-19 2020-03-15 5 6 Claw Close 2023-03-16 2024-12-31 <p>Since we can't find a match for Charlie and we're using a <code>LEFT JOIN</code>, we'll get <code>NULL</code> values for the <code>Address</code> columns:</p> EmployeeID EmployeeName Address FromDate ToDate 3 Charlie null null null"},{"location":"from-excel-to-sql/main-concepts/join/#employee-4_1","title":"Employee 4","text":"<p>The fourth row is employee 4:</p> EmployeeID EmployeeName 4 Dave <p>Dave's <code>EmployeeID</code> is 4 which also doesn't have a match in the <code>Address</code> table, so we get <code>NULL</code> values for the <code>Address</code> columns:</p> EmployeeID EmployeeName Address FromDate ToDate 4 Dave null null null"},{"location":"from-excel-to-sql/main-concepts/join/#employee-5_1","title":"Employee 5","text":"<p>The fifth row is employee 5:</p> EmployeeID EmployeeName 5 Eve <p>Eve's <code>EmployeeID</code> is 5, and they have two addresses in the <code>Address</code> table:</p> EmployeeID Address FromDate ToDate 1 1 Main Street 2001-07-21 2002-10-30 2 2 Rocky Road 2012-07-04 2018-02-11 \u2192 5 5 Log Lane 2009-11-19 2020-03-15 \u2192 5 6 Claw Close 2023-03-16 2024-12-31 <p>This is another place where SQL is different to Excel. Since we have two matches for Eve in the <code>Address</code> table, we get two rows in the result -- we keep both!</p> EmployeeID EmployeeName Address FromDate ToDate 5 Eve 5 Log Lane 2009-11-19 2020-03-15 5 Eve 6 Claw Close 2023-03-16 2024-12-31"},{"location":"from-excel-to-sql/main-concepts/join/#putting-it-all-together_1","title":"Putting it all together","text":"<p>When we put all of these rows together, we get the following result:</p> EmployeeID EmployeeName Address FromDate ToDate 1 Alice 1 Main Street 2001-07-21 2002-10-30 2 Bob 2 Rocky Road 2012-07-04 2018-02-11 3 Charlie null null null 4 Dave null null null 5 Eve 5 Log Lane 2009-11-19 2020-03-15 5 Eve 6 Claw Close 2023-03-16 2024-12-31 <p>Warning</p> <p>In Excel, you'd only get one row for Eve (Excel would just keep the first match), but in SQL you get all the matches. This is a common source of confusion for people new to SQL.</p> <p>However, this behaviour is super useful when used correctly! Just keep an eye out for it.</p>"},{"location":"from-excel-to-sql/main-concepts/join/#employee-inner-join-address","title":"<code>Employee INNER JOIN Address</code>","text":"<p>To finish off, let's consider the same query but with an <code>INNER JOIN</code> instead of a <code>LEFT JOIN</code>:</p> <pre><code>SELECT\n    Employee.EmployeeID,\n    Employee.EmployeeName,\n    Address.Address,\n    Address.FromDate,\n    Address.ToDate\nFROM Employee\n    INNER JOIN Address\n        ON Employee.EmployeeID = Address.EmployeeID\n;\n</code></pre> <p>Can you guess what the result will be?</p> <p>It's the same as the <code>LEFT JOIN</code> example, but without rows for Charlie and Dave since they don't have a match in the <code>Address</code> table using this condition:</p> EmployeeID EmployeeName Address FromDate ToDate 1 Alice 1 Main Street 2001-07-21 2002-10-30 2 Bob 2 Rocky Road 2012-07-04 2018-02-11 5 Eve 5 Log Lane 2009-11-19 2020-03-15 5 Eve 6 Claw Close 2023-03-16 2024-12-31"},{"location":"from-excel-to-sql/main-concepts/join/#left-join-is-probably-the-most-common-join-type","title":"<code>LEFT JOIN</code> is probably the most common join type","text":"<p>Since the <code>INNER JOIN</code> will drop rows that don't have a match in the \"joined\" table, it's usually \"safer\" to use <code>LEFT JOIN</code> to make sure that you don't accidentally lose any data during the join.</p> <p>This is common practice (favouring <code>LEFT</code> over <code>INNER</code>), so you'll often see people use <code>LEFT JOIN</code> unless they have a specific reason to use a different type.</p>"},{"location":"from-excel-to-sql/main-concepts/join/#inner-join-is-the-default-join-type","title":"<code>INNER JOIN</code> is the default join type","text":"<p>If you don't specify <code>LEFT</code> or <code>INNER</code> (or any of the others), then SQL will default to an <code>INNER JOIN</code>:</p> <pre><code>SELECT\n    Employee.EmployeeID,\n    Employee.EmployeeName,\n    Employee.DepartmentID,\n    Department.DepartmentName\nFROM Employee\n    JOIN Department  /* This will be an `INNER JOIN` */\n        ON Employee.DepartmentID = Department.DepartmentID\n;\n</code></pre> <p>Although this is the default, it's always a good idea to be explicit about the join type you're using. This makes the query easier to read and understand, and it can help avoid errors if the tables change in the future.</p>"},{"location":"from-excel-to-sql/main-concepts/join/#there-can-be-several-joins-in-a-single-query","title":"There can be several joins in a single query","text":"<p>The examples above have just been joining two tables, but you can join as many tables as you like in a single query.</p> <p>For example, we could combine the <code>Employee</code>, <code>Department</code>, and <code>Address</code> tables in a single query:</p> <pre><code>SELECT\n    Employee.EmployeeID,\n    Employee.EmployeeName,\n    Employee.DepartmentID,\n    Department.DepartmentName,\n    Address.Address,\n    Address.FromDate,\n    Address.ToDate\nFROM Employee\n    LEFT JOIN Department\n        ON Employee.DepartmentID = Department.DepartmentID\n    LEFT JOIN Address\n        ON Employee.EmployeeID = Address.EmployeeID\n;\n</code></pre> EmployeeID EmployeeName DepartmentID DepartmentName Address FromDate ToDate 1 Alice 1 Sales 1 Main Street 2001-07-21 2002-10-30 2 Bob 1 Sales 2 Rocky Road 2012-07-04 2018-02-11 3 Charlie 2 Marketing null null null 4 Dave 2 Marketing null null null 5 Eve 3 null 5 Log Lane 2009-11-19 2020-03-15 5 Eve 3 null 6 Claw Close 2020-03-16 2024-12-31 <p>Notice how, although the <code>Employee</code> table only has five rows, we've ended up with six because of the (<code>LEFT</code>) join with the <code>Address</code> table.</p> <p>Similarly, can you guess what the output would be if we used <code>INNER JOIN</code>s instead of <code>LEFT JOIN</code>s?</p> <pre><code>SELECT\n    Employee.EmployeeID,\n    Employee.EmployeeName,\n    Employee.DepartmentID,\n    Department.DepartmentName,\n    Address.Address,\n    Address.FromDate,\n    Address.ToDate\nFROM Employee\n    INNER JOIN Department\n        ON Employee.DepartmentID = Department.DepartmentID\n    INNER JOIN Address\n        ON Employee.EmployeeID = Address.EmployeeID\n;\n</code></pre> EmployeeID EmployeeName DepartmentID DepartmentName Address FromDate ToDate 1 Alice 1 Sales 1 Main Street 2001-07-21 2002-10-30 2 Bob 1 Sales 2 Rocky Road 2012-07-04 2018-02-11 <p>We'd only get two rows! We'd lose the rows for Charlie and Dave in the join with the <code>Address</code> table, and we'd lose the row(s) for Eve in the join with the <code>Department</code> table.</p> <p>Info</p> <p>You can use whichever join types you want for each join, there's no need to use all the same (e.g. all <code>LEFT</code> join).</p>"},{"location":"from-excel-to-sql/main-concepts/join/#other-tips-and-tricks","title":"Other tips and tricks","text":"<p>The examples above are \"typical\" examples of joins, but you'll find that you can use joins in a lot of different ways!</p>"},{"location":"from-excel-to-sql/main-concepts/join/#you-can-use-any-condition-in-a-join","title":"You can use any condition in a join","text":"<p>We've just been using the <code>=</code> operator in the <code>ON</code> clause, but you can use any condition you like (and as many as you want!).</p> <p>For example, we could also filter on the dates in the <code>Address</code> table when we join it to the <code>Employee</code> table:</p> <pre><code>SELECT\n    Employee.EmployeeID,\n    Employee.EmployeeName,\n    Address.Address,\n    Address.FromDate,\n    Address.ToDate\nFROM Employee\n    INNER JOIN Address\n        ON  Employee.EmployeeID = Address.EmployeeID\n        AND Address.FromDate &gt;= '2010-01-01'\n;\n</code></pre> EmployeeID EmployeeName Address FromDate ToDate 2 Bob 2 Rocky Road 2012-07-04 2018-02-11 5 Eve 6 Claw Close 2020-03-16 2024-12-31"},{"location":"from-excel-to-sql/main-concepts/join/#tables-can-be-joined-to-themselves","title":"Tables can be joined to themselves","text":"<p>There's nothing stopping you from joining a table to itself! This can be useful when you want to compare rows within the same table, but this is pretty rare, so we won't cover it in this course.</p>"},{"location":"from-excel-to-sql/main-concepts/join/#the-sql-for-running-these-examples","title":"The SQL for running these examples","text":"<p>Error</p> <p>The data for these examples isn't in the AdventureWorks database that we're using, so it has been created for this section. If you want to run these examples yourself, you can adjust the SQL below. Note that this is using some features that we haven't covered yet!</p> <p>For the examples above, the rows are created on the fly. You're not expected to understand this yet, but it's provided so that you can run the SQL yourself if you want to.</p> <pre><code>WITH\n\nEmployee AS (\n    SELECT *\n    FROM (\n        VALUES\n            (1, 'Alice',   1),\n            (2, 'Bob',     1),\n            (3, 'Charlie', 2),\n            (4, 'Dave',    2),\n            (5, 'Eve',     3)\n    ) AS V(EmployeeID, EmployeeName, DepartmentID)\n),\n\nDepartment AS (\n    SELECT *\n    FROM (\n        VALUES\n            (1, 'Sales'),\n            (2, 'Marketing')\n    ) AS V(DepartmentID, DepartmentName)\n),\n\nAddress AS (\n    SELECT *\n    FROM (\n        VALUES\n            (1, '1 Main Street', '2001-07-21', '2002-10-30'),\n            (2, '2 Rocky Road',  '2012-07-04', '2018-02-11'),\n            (5, '5 Log Lane',    '2009-11-19', '2020-03-15'),\n            (5, '6 Claw Close',  '2020-03-16', '2024-12-31')\n    ) AS V(EmployeeID, Address, FromDate, ToDate)\n)\n\n/* Edit this part */\nSELECT\n    Employee.EmployeeID,\n    Employee.EmployeeName,\n    Employee.DepartmentID,\n    Department.DepartmentName,\n    Address.Address,\n    Address.FromDate,\n    Address.ToDate\nFROM Employee\n    INNER JOIN Department\n        ON Employee.DepartmentID = Department.DepartmentID\n    INNER JOIN Address\n        ON Employee.EmployeeID = Address.EmployeeID\n;\n</code></pre>"},{"location":"from-excel-to-sql/main-concepts/join/#further-reading","title":"Further reading","text":"<p>Check out the official Microsoft documentation for more information on the <code>JOIN</code> clause at:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/t-sql/queries/from-transact-sql#join-type</li> </ul> <p>The video version of this content is also available at:</p> <ul> <li>https://youtu.be/xG5CqLICKcY</li> </ul>"},{"location":"from-excel-to-sql/main-concepts/join/#additional-join-modifiers-in-other-sql-flavours","title":"Additional join modifiers in other SQL flavours","text":"<p>Microsoft SQL Server has a fairly limited number of join features, but other SQL flavours add loads of additional modifiers to the <code>JOIN</code> clause.</p> <p>If you see something that you don't recognise, make sure that you search for it in the documentation for the specific SQL flavour that you see the thing in!</p>"},{"location":"from-excel-to-sql/main-concepts/join/#visual-representation-of-joins","title":"Visual representation of joins","text":"<p>If you're a visual learner, you might find it helpful to read the following article from Atlassian (the company behind Jira and Confluence) which has some great visual representations of the different join types:</p> <ul> <li>https://www.atlassian.com/data/sql/sql-join-types-explained-visually</li> </ul> <p>For example, their \"cheat sheet\" is:</p> <p></p>"},{"location":"from-excel-to-sql/main-concepts/logical-processing-order/","title":"Logical processing order \ud83c\udfa5","text":"<p>Warning</p> <p>Confusingly, the order in which things are done in a <code>SELECT</code> statement is not exactly the same as the order in which they are written.</p> <p>This is because SQL code was designed to be written in a way that is easy to read and write, rather than to reflect what happens under the hood.</p>"},{"location":"from-excel-to-sql/main-concepts/logical-processing-order/#the-written-order-of-a-select-statement","title":"The written order of a <code>SELECT</code> statement","text":"<p>This has been mentioned in each of the clauses' respective pages, but the lexical (written) order of a <code>SELECT</code> statement is as follows:</p> <ol> <li><code>SELECT</code></li> <li><code>DISTINCT</code></li> <li><code>TOP</code></li> <li><code>FROM</code></li> <li><code>JOIN</code></li> <li><code>ON</code></li> <li><code>WHERE</code></li> <li><code>GROUP BY</code></li> <li><code>WITH CUBE</code> or <code>WITH ROLLUP</code></li> <li><code>HAVING</code></li> <li><code>WINDOW</code></li> <li><code>ORDER BY</code></li> </ol>"},{"location":"from-excel-to-sql/main-concepts/logical-processing-order/#the-logical-processing-order-of-a-select-statement","title":"The logical processing order of a <code>SELECT</code> statement","text":"<p>The logical processing order of a <code>SELECT</code> statement is not the same as the written order.</p> <p>A big part of understanding SQL is understanding the order in which things are done in a <code>SELECT</code> statement.</p> <p>Do joins happen before or after the <code>WHERE</code> clause? Are rows deduplicated before being aggregated? Are rows ordered before being \"topped\" (with <code>TOP</code>)?</p> <p>This is important to understand because it can affect the result of your query.</p> <p>As per the Microsoft SQL Server documentation, the logical processing order of a <code>SELECT</code> statement is as follows:</p> <ol> <li><code>FROM</code></li> <li><code>ON</code></li> <li><code>JOIN</code></li> <li><code>WHERE</code></li> <li><code>GROUP BY</code></li> <li><code>WITH CUBE</code> or <code>WITH ROLLUP</code></li> <li><code>HAVING</code></li> <li><code>SELECT</code> (and <code>OVER</code>/<code>WINDOW</code>)</li> <li><code>DISTINCT</code></li> <li><code>ORDER BY</code></li> <li><code>TOP</code></li> </ol> <p>SQL has this disparity between the written code and the order in which it is processed because SQL is a \"declarative\" language. You don't need to know what this means, but it's just a fancy way of saying that you tell SQL what you want, not how to get it.</p>"},{"location":"from-excel-to-sql/main-concepts/logical-processing-order/#further-reading","title":"Further reading","text":"<p>Check out the official Microsoft documentation for more information on the logical processing order of the select statement at:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/t-sql/queries/select-transact-sql#logical-processing-order-of-the-select-statement</li> </ul>"},{"location":"from-excel-to-sql/main-concepts/operators/","title":"Operators \u2795","text":"<p>Success</p> <p>Like we saw in the Filtering section, the SQL \"operators\" are very similar to the Excel operators.</p>"},{"location":"from-excel-to-sql/main-concepts/operators/#sql-has-the-same-operators-as-excel","title":"SQL has the same operators as Excel","text":"<p>We saw in the Filtering section that SQL has the comparison operators that we're familiar with from Excel:</p> <ul> <li><code>&lt;</code> (less than)</li> <li><code>&lt;=</code> (less than or equal to)</li> <li><code>&gt;</code> (greater than)</li> <li><code>&gt;=</code> (greater than or equal to)</li> <li><code>=</code> (equals)</li> <li><code>&lt;&gt;</code> (does not equal), also written as <code>!=</code></li> </ul> <p>Unsurprisingly, SQL also has the same arithmetic operators that we're familiar with from Excel:</p> <ul> <li><code>+</code> (addition)</li> <li><code>-</code> (subtraction)</li> <li><code>*</code> (multiplication)</li> <li><code>/</code> (division)</li> </ul> <p>We'd use these exactly as you'd expect:</p> <pre><code>SELECT\n    1 + 1 AS TWO,\n    2 - 1 AS ONE,\n    2 * 2 AS FOUR,\n    6 / 2 AS THREE\n;\n</code></pre> TWO ONE FOUR THREE 2 1 4 3"},{"location":"from-excel-to-sql/main-concepts/operators/#unlike-excel-be-careful-with-division","title":"Unlike Excel, be careful with division!","text":"<p>In Excel, if you divide a whole number by another whole number, Excel will give you what you expect -- for example, <code>5 / 2</code> will give you <code>2.5</code>.</p> <p>In SQL, if you divide a whole number by another whole number, you'll get a whole number back. This means that <code>5 / 2</code> will give you <code>2</code>, not <code>2.5</code>!</p> <pre><code>SELECT 5 / 2 AS FIVE_DIVIDED_BY_TWO;\n</code></pre> FIVE_DIVIDED_BY_TWO 2 <p>This is because of what we covered in the Data types section; SQL (at least, Microsoft SQL Server) is more pedantic with data types than Excel is.</p> <p>The natural question is, \"how do we get the correct number back?\"</p> <p>There are a few things that we could do that you'll see out in the wild:</p> <ul> <li>If you specify the numbers, add a decimal point to (at least) one of them</li> <li>If you're using columns, cast (at least) one column to a <code>DECIMAL</code> or <code>FLOAT</code> data type</li> <li>If you're using columns, multiply by <code>1.0</code> to convert the whole number to a decimal (this is a \"cheat\" way of doing the previous point)</li> </ul> <p>The following are all how we'd implement the points above:</p> <pre><code>SELECT\n    5.0 / 2 AS OPTION_1,\n    CAST(5 AS DECIMAL) / 2 AS OPTION_2,\n    1.0 * 5 / 2 AS OPTION_3\n;\n</code></pre> OPTION_1 OPTION_2 OPTION_3 2.500000 2.500000 2.500000 <p>Warning</p> <p>It's important to do the data type conversion before the division. For example, the following would not give you the correct answer:</p> <pre><code>SELECT CAST(5 / 2 AS DECIMAL) AS INCORRECT;\n</code></pre> <p>This is because the division will be done before the conversion, so the damage will have already been done!</p>"},{"location":"from-excel-to-sql/main-concepts/operators/#further-reading","title":"Further reading","text":"<p>Check out the official Microsoft documentation for more information on operators at:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/t-sql/language-elements/operators-transact-sql</li> </ul> <p>The video version of this content is also available at:</p> <ul> <li>https://youtu.be/bxZeIkcxYQA</li> </ul>"},{"location":"from-excel-to-sql/main-concepts/order-by/","title":"Ordering \u23ec","text":"<p>Success</p> <p>The <code>ORDER BY</code> clause is used to sort the rows in a query. It's like the \"Sort by\" feature in Excel!</p> <p>Note</p> <p>The <code>ORDER BY</code> clause is optional. If you use it, it must come at the end of the SQL statement.</p>"},{"location":"from-excel-to-sql/main-concepts/order-by/#order-by-is-how-we-sort-rows","title":"<code>ORDER BY</code> is how we sort rows","text":"<p>We know how to \"open a file\" using <code>SELECT</code> and <code>FROM</code>, and we know how to \"filter\" rows using <code>WHERE</code>.</p> <p>To sort rows, use the <code>ORDER BY</code> clause and specify the columns that you want to order by.</p> <p>In Excel, there are two ways to sort data:</p> <ol> <li>Click on the column header and use the \"Sort A to Z\" or \"Sort Z to A\" buttons.</li> <li>Use the \"Sort\" feature in the \"Data\" tab.</li> </ol> <p>Sorting in SQL is more like the second option, where you can specify the columns to sort by and the direction to sort in (ascending or descending).</p> <p>For example, we could sort the <code>HumanResources.Department</code> table by the <code>Name</code> column in ascending order using:</p> <pre><code>SELECT\n    DepartmentID,\n    Name,\n    GroupName\nFROM HumanResources.Department\nORDER BY Name\n;\n</code></pre> DepartmentID Name GroupName 12 Document Control Quality Assurance 1 Engineering Research and Development 16 Executive Executive General and Administration 14 Facilities and Maintenance Executive General and Administration 10 Finance Executive General and Administration <p>Tip</p> <p>The <code>ORDER BY</code> clause \"sounds like\" English, so the query above can be read as:</p> <p>\"Select the department ID, name, and group name from the <code>HumanResources.Department</code> table and order by the name\".</p> <p>To sort in descending order, use the <code>DESC</code> keyword:</p> <pre><code>SELECT\n    DepartmentID,\n    Name,\n    GroupName\nFROM HumanResources.Department\nORDER BY Name DESC\n;\n</code></pre> DepartmentID Name GroupName 2 Tool Design Research and Development 15 Shipping and Receiving Inventory Management 3 Sales Sales and Marketing 6 Research and Development Research and Development 13 Quality Assurance Quality Assurance <p>To sort by multiple columns, separate the columns with a comma:</p> <pre><code>SELECT\n    DepartmentID,\n    Name,\n    GroupName\nFROM HumanResources.Department\nORDER BY GroupName, DepartmentID\n;\n</code></pre> DepartmentID Name GroupName 9 Human Resources Executive General and Administration 10 Finance Executive General and Administration 11 Information Services Executive General and Administration 14 Facilities and Maintenance Executive General and Administration 16 Executive Executive General and Administration <p>Warning</p> <p>If the <code>ORDER BY</code> tries to sort repeated values, the order of the rows is not guaranteed. If you need to guarantee the order of the rows, make sure you include enough columns in the <code>ORDER BY</code> clause.</p>"},{"location":"from-excel-to-sql/main-concepts/order-by/#you-can-use-the-column-alias-in-the-order-by-clause","title":"You can use the column alias in the <code>ORDER BY</code> clause","text":"<p>Info</p> <p>The <code>ORDER BY</code> clause is one of the few places where this works; it doesn't work in most other places (in Microsoft SQL Server).</p> <p>If you rename/use an alias for a column in the <code>SELECT</code> clause, you can use that alias in the <code>ORDER BY</code> clause. For example:</p> <pre><code>SELECT\n    DepartmentID AS ID,\n    Name,\n    GroupName\nFROM HumanResources.Department\nORDER BY GroupName, ID\n;\n</code></pre>"},{"location":"from-excel-to-sql/main-concepts/order-by/#you-can-sort-by-column-numbers-but-dont","title":"You can sort by column numbers, but don't","text":"<p>Info</p> <p>This is just a FYI in case you see this out in the wild.</p> <p>Instead of using the column names in the <code>ORDER BY</code> clause, you can use their position in the query (starting at 1). For example, the following query sorts the <code>HumanResources.Department</code> table by the <code>GroupName</code> column:</p> <pre><code>SELECT\n    DepartmentID,\n    Name,\n    GroupName\nFROM HumanResources.Department\nORDER BY 3\n;\n</code></pre> <p>Although this is possible, it's not recommended (as per the Microsoft documentation):</p> <p>Avoid specifying integers in the ORDER BY clause as positional representations of the columns in the select list. For example, although a statement such as <code>SELECT ProductID, Name FROM Production.Production ORDER BY 2</code> is valid, the statement is not as easily understood by others compared with specifying the actual column name. In addition, changes to the select list, such as changing the column order or adding new columns, requires modifying the ORDER BY clause in order to avoid unexpected results.</p>"},{"location":"from-excel-to-sql/main-concepts/order-by/#further-reading","title":"Further reading","text":"<p>Check out the official Microsoft documentation for more information on the <code>ORDER BY</code> clause at:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/t-sql/queries/select-order-by-clause-transact-sql</li> </ul> <p>The video version of this content is also available at:</p> <ul> <li>https://youtu.be/yaomCldxZi4</li> </ul>"},{"location":"from-excel-to-sql/main-concepts/order-by/#additional-modifiers","title":"Additional modifiers","text":"<p>The <code>ORDER BY</code> clause also has additional modifiers which are outside the scope of this course. These include <code>COLLATE</code>, <code>OFFSET</code>, and <code>FETCH</code>:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/t-sql/queries/select-order-by-clause-transact-sql#Collation</li> <li>https://learn.microsoft.com/en-us/sql/t-sql/queries/select-order-by-clause-transact-sql#using-offset-and-fetch-to-limit-the-rows-returned</li> </ul> <p>Error</p> <p>This is a contrived example to show the additional modifiers.</p> <pre><code>SELECT\n    DepartmentID,\n    Name,\n    GroupName\nFROM HumanResources.Department\nORDER BY DepartmentID\n  OFFSET 10 ROWS\n  FETCH NEXT 5 ROWS ONLY\n;\n</code></pre>"},{"location":"from-excel-to-sql/main-concepts/rollup/","title":"Pivot tables \ud83e\uddee","text":"<p>Success</p> <p>One of Excel's most awesome features is pivot tables -- and SQL has them, too (at least, most flavours)!</p> <p>Note</p> <p>The <code>ROLLUP</code> modifier is optional. If you use it, it must be part of the <code>GROUP BY</code> clause.</p>"},{"location":"from-excel-to-sql/main-concepts/rollup/#we-use-the-rollup-modifier-to-create-pivot-tables","title":"We use the <code>ROLLUP</code> modifier to create \"pivot tables\"","text":"<p>In the aggregations section, we saw that we can use the <code>GROUP BY</code> clause to group rows together and summarise them.</p> <p>This is handy but, unlike Excel's pivot tables, it doesn't include subtotals and grand totals.</p> <p>To tell SQL to include these totals, we use the <code>ROLLUP</code> modifier! This modifier goes immediately after the <code>GROUP BY</code> text, and the grouping columns are listed in brackets.</p> <p>For example, consider the following query which produces six months of sales summaries:</p> <pre><code>SELECT\n    FORMAT(OrderDate, 'yyyy-MM') AS OrderMonth,\n    SUM(TotalDue) AS TotalSales\nFROM Sales.SalesOrderHeader\nWHERE '2013-01-01' &lt;= OrderDate AND OrderDate &lt; '2013-07-01'\nGROUP BY FORMAT(OrderDate, 'yyyy-MM')\nORDER BY OrderMonth\n;\n</code></pre> OrderMonth TotalSales 2013-01 2340061.5521 2013-02 2600218.8667 2013-03 3831605.9389 2013-04 2840711.1734 2013-05 3658084.9461 2013-06 5726265.2635 <p>We could use <code>ROLLUP</code> to add a grand total to the results:</p> <pre><code>SELECT\n    FORMAT(OrderDate, 'yyyy-MM') AS OrderMonth,\n    SUM(TotalDue) AS TotalSales\nFROM Sales.SalesOrderHeader\nWHERE '2013-01-01' &lt;= OrderDate AND OrderDate &lt; '2013-07-01'\nGROUP BY ROLLUP (FORMAT(OrderDate, 'yyyy-MM'))\nORDER BY OrderMonth\n;\n</code></pre> OrderMonth TotalSales null 20996947.7407 2013-01 2340061.5521 2013-02 2600218.8667 2013-03 3831605.9389 2013-04 2840711.1734 2013-05 3658084.9461 2013-06 5726265.2635 <p>Note that the <code>OrderMonth</code> column now includes a <code>NULL</code> value, which represents the grand total.</p> <p>This query only groups by a single column, so there are no subtotals (only the grand total). If we were to group by multiple columns, we would see subtotals for each combination of the grouping columns.</p> <p>For example, we could extend the example above (without the <code>ROLLUP</code> modifier) to also group by the <code>OnlineOrderFlag</code> column:</p> <pre><code>SELECT\n    FORMAT(OrderDate, 'yyyy-MM') AS OrderMonth,\n    OnlineOrderFlag,\n    SUM(TotalDue) AS TotalSales\nFROM Sales.SalesOrderHeader\nWHERE '2013-01-01' &lt;= OrderDate AND OrderDate &lt; '2013-07-01'\nGROUP BY\n    FORMAT(OrderDate, 'yyyy-MM'),\n    OnlineOrderFlag\nORDER BY\n    OrderMonth,\n    OnlineOrderFlag\n;\n</code></pre> OrderMonth OnlineOrderFlag TotalSales 2013-01 false 1761132.8322 2013-01 true 578928.7199 2013-02 false 2101152.5476 2013-02 true 499066.3191 2013-03 false 3244501.4287 2013-03 true 587104.5102 2013-04 false 2239156.6675 2013-04 true 601554.5059 2013-05 false 3019173.6253 2013-05 true 638911.3208 2013-06 false 4775809.3027 2013-06 true 950455.9608 <p>Adding the <code>ROLLUP</code> modifier to the <code>GROUP BY</code> clause will add subtotals for the order months and a grand total:</p> <pre><code>SELECT\n    FORMAT(OrderDate, 'yyyy-MM') AS OrderMonth,\n    OnlineOrderFlag,\n    SUM(TotalDue) AS TotalSales\nFROM Sales.SalesOrderHeader\nWHERE '2013-01-01' &lt;= OrderDate AND OrderDate &lt; '2013-07-01'\nGROUP BY ROLLUP (\n    FORMAT(OrderDate, 'yyyy-MM'),\n    OnlineOrderFlag\n)\nORDER BY\n    OrderMonth,\n    OnlineOrderFlag\n;\n</code></pre> OrderMonth OnlineOrderFlag TotalSales null null 20996947.7407 2013-01 null 2340061.5521 2013-01 false 1761132.8322 2013-01 true 578928.7199 2013-02 null 2600218.8667 2013-02 false 2101152.5476 2013-02 true 499066.3191 2013-03 null 3831605.9389 2013-03 false 3244501.4287 2013-03 true 587104.5102 2013-04 null 2840711.1734 2013-04 false 2239156.6675 2013-04 true 601554.5059 2013-05 null 3658084.9461 2013-05 false 3019173.6253 2013-05 true 638911.3208 2013-06 null 5726265.2635 2013-06 false 4775809.3027 2013-06 true 950455.9608 <p>Warning</p> <p>Did you notice that there were subtotals for the order months, but no subtotals for the <code>OnlineOrderFlag</code> column? This is because, like with Excel's pivot tables, the order of the columns is significant!</p> <p>If we switched the order of the columns in the <code>GROUP BY</code> clause, the subtotals for <code>OnlineOrderFlag</code> would be calculated but not the subtotals for the order months.</p> <p>The subtotals that are generated follow the same rules as Excel's pivot tables.</p> <p>Success</p> <p>SQL has called this modifier \"rollup\" because it rolls up the values into subtotals and grand totals!</p>"},{"location":"from-excel-to-sql/main-concepts/rollup/#theres-an-alternative-syntax-with-rollup","title":"There's an alternative syntax: <code>WITH ROLLUP</code>","text":"<p>Instead of specifying <code>ROLLUP</code> immediately after the <code>GROUP BY</code> text and listing the grouping columns in brackets, we could instead just add the text <code>WITH ROLLUP</code> after the list of columns:</p> <pre><code>SELECT\n    FORMAT(OrderDate, 'yyyy-MM') AS OrderMonth,\n    SUM(TotalDue) AS TotalSales\nFROM Sales.SalesOrderHeader\nWHERE '2013-01-01' &lt;= OrderDate AND OrderDate &lt; '2013-07-01'\nGROUP BY FORMAT(OrderDate, 'yyyy-MM') WITH ROLLUP\nORDER BY OrderMonth\n;\n</code></pre> <p>Although this syntax is supported, it's not the standard syntax to use and is only included for backwards compatibility.</p> <p>You should stick to the standard syntax of adding <code>ROLLUP</code> immediately after the <code>GROUP BY</code> text and listing the grouping columns in brackets.</p>"},{"location":"from-excel-to-sql/main-concepts/rollup/#further-reading","title":"Further reading","text":"<p>Check out the official Microsoft documentation for more information on <code>ROLLUP</code> at:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/t-sql/queries/select-group-by-transact-sql#group-by-rollup</li> </ul> <p>The video version of this content is also available at:</p> <ul> <li>https://youtu.be/65EFEtjYL9E</li> </ul>"},{"location":"from-excel-to-sql/main-concepts/rollup/#additional-grouping-functions","title":"Additional grouping functions","text":"<p>There are additional functions which are outside the scope of this course to distinguish between <code>NULL</code> values generated by the subtotals/grand totals and <code>NULL</code> values that are in the original data.</p> <p>They are the <code>GROUPING</code> and <code>GROUPING_ID</code> functions which help identify which rows correspond to different levels of the <code>ROLLUP</code> hierarchy:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/t-sql/functions/grouping-transact-sql</li> <li>https://learn.microsoft.com/en-us/sql/t-sql/functions/grouping-id-transact-sql</li> </ul> <p>Error</p> <p>This is a contrived example to show the additional grouping functions.</p> <pre><code>SELECT\n    GROUPING(FORMAT(OrderDate, 'yyyy-MM')) AS IsOrderDateSubtotal,\n    GROUPING(OnlineOrderFlag) AS IsOnlineOrderFlagSubtotal,\n    GROUPING_ID(FORMAT(OrderDate, 'yyyy-MM'), OnlineOrderFlag) AS GroupingId,\n\n    FORMAT(OrderDate, 'yyyy-MM') AS OrderMonth,\n    OnlineOrderFlag,\n    SUM(TotalDue) AS TotalSales\nFROM Sales.SalesOrderHeader\nWHERE '2013-01-01' &lt;= OrderDate AND OrderDate &lt; '2013-07-01'\nGROUP BY ROLLUP (\n    FORMAT(OrderDate, 'yyyy-MM'),\n    OnlineOrderFlag\n)\nORDER BY\n    OrderMonth,\n    OnlineOrderFlag\n;\n</code></pre>"},{"location":"from-excel-to-sql/main-concepts/select-and-from/","title":"Gimme data \ud83d\udcc1","text":"<p>Success</p> <p>Now you know that SQL is a language for getting data from a database, let's actually get some data!</p> <p>Note</p> <p>From here on, we'll be using a Microsoft SQL Server database filled with the AdventureWorks data. If you want to follow along, you can use the dbfiddle site to run the queries against the same data:</p> <ul> <li>https://dbfiddle.uk/rKIFRoNm</li> </ul> <p>Alternatively, follow the instructions linked in the homepage:</p> <ul> <li>Introduction to SQL Programming for Excel Users - SQL Server Windows Setup</li> </ul>"},{"location":"from-excel-to-sql/main-concepts/select-and-from/#select-and-from-are-how-you-open-a-file","title":"<code>SELECT</code> and <code>FROM</code> are how you \"open a file\"","text":"<p>If you wanted to get some data from an Excel file, you'd navigate through your folders, open the file, and then click on the sheet that has the data you want.</p> <p>To do the same thing in SQL, we need to write the <code>SELECT</code> and <code>FROM</code> clauses:</p> <ul> <li><code>SELECT</code> tells the database that we want to see some data</li> <li><code>FROM</code> tells the database where to get the data from</li> </ul> <p>Although your Excel file might live inside nested folders, SQL databases are a bit more structured.</p> <p>For the most part, there's only one depth of \"folders\" which we call the schema.</p> <p>The AdventureWorks database has several of these folders/schemas, which each contains tables. The folders/schemas in the AdventureWorks database are:</p> <ul> <li><code>dbo</code> (this is a special folder/schema which you can ignore for now)</li> <li><code>HumanResources</code></li> <li><code>Person</code></li> <li><code>Production</code></li> <li><code>Purchasing</code></li> <li><code>Sales</code></li> </ul> <p>Each of these folders/schemas contains a bunch of tables, and we will need to specify both the folder/schema and the table in the <code>FROM</code> clause.</p> <p>For example, the <code>HumanResources</code> folder/schema contains a <code>Department</code> table, so we would write <code>HumanResources.Department</code> in the <code>FROM</code> clause to \"open that file\":</p> <pre><code>SELECT DepartmentID, Name\nFROM HumanResources.Department\n;\n</code></pre> <p>Writing the query is the first part -- to actually get some data, we then need to run the query!</p> <p>The first few rows from the result of this query are:</p> DepartmentID Name 12 Document Control 1 Engineering 16 Executive 14 Facilities and Maintenance 10 Finance <p>Tip</p> <p>Remember that SQL \"sounds like\" English, so the query above can be read as:</p> <p>\"Select the department ID and name from the <code>HumanResources.Department</code> table\".</p> <p>Note</p> <p>The <code>FROM</code> clause always comes after the <code>SELECT</code> clause.</p>"},{"location":"from-excel-to-sql/main-concepts/select-and-from/#specify-or-column-names-in-the-select-clause","title":"Specify <code>*</code> or column names in the <code>SELECT</code> clause","text":"<p>The <code>HumanResources.Department</code> table has a bunch of columns, but we only asked for two of them in the <code>SELECT</code> clause.</p> <p>To see more columns, we would just add them to the <code>SELECT</code> clause:</p> <pre><code>SELECT\n    DepartmentID,\n    Name,\n    GroupName,\n    ModifiedDate\nFROM HumanResources.Department\n;\n</code></pre> DepartmentID Name GroupName ModifiedDate 1 Engineering Research and Development 2008-04-30 00:00:00.000 2 Tool Design Research and Development 2008-04-30 00:00:00.000 3 Sales Sales and Marketing 2008-04-30 00:00:00.000 4 Marketing Sales and Marketing 2008-04-30 00:00:00.000 5 Purchasing Inventory Management 2008-04-30 00:00:00.000 <p>To see all the columns without listing them all explicitly, we can use the <code>*</code> character in the <code>SELECT</code> clause. The <code>*</code> character is a wildcard that means \"all columns\":</p> <pre><code>SELECT *\nFROM HumanResources.Department\n;\n</code></pre> <p>Tip</p> <p>We read <code>*</code> as \"all columns\" or \"everything\", so the query above can be read as:</p> <p>\"Select all columns from the <code>HumanResources.Department</code> table\".</p> <p>Since this table only has the 4 columns that we used previously, the result is the same as the previous query!</p> <ul> <li>\u2705 The advantage of using the <code>*</code> character is that you don't need to know the names of the columns in the table</li> <li>\u274c The disadvantage is that you might get more columns than you need, which can make the result harder to read and use.</li> </ul> <p>When you write SQL, it's up to you whether you use the <code>*</code> character or list the columns explicitly. There are some best practices depending on the situation, but they won't be covered in this course.</p> <p>Note that you can also specify the same column multiple times in the <code>SELECT</code> clause if you want. The example below is a silly one to prove this, but we'll see more practical examples later:</p> <pre><code>SELECT\n    DepartmentID,\n    DepartmentID,\n    DepartmentID\nFROM HumanResources.Department\n;\n</code></pre> DepartmentID DepartmentID DepartmentID 12 12 12 1 1 1 16 16 16 14 14 14 10 10 10"},{"location":"from-excel-to-sql/main-concepts/select-and-from/#use-as-to-rename-columns","title":"Use <code>AS</code> to \"rename\" columns","text":"<p>The <code>AS</code> keyword is used to rename/alias columns in the <code>SELECT</code> clause. Just write <code>AS</code> followed by the new name that you want to give the column:</p> <pre><code>SELECT\n    DepartmentID AS ID,\n    Name\nFROM HumanResources.Department\n;\n</code></pre> ID Name 12 Document Control 1 Engineering 16 Executive 14 Facilities and Maintenance 10 Finance <p>The <code>AS</code> keyword should also be used after any calculations to give the calculated column a name. We'll see examples of this later.</p> <p>Tip</p> <p>The <code>AS</code> keyword continues to \"sound like\" English -- the query above can be read as:</p> <p>\"Select the department ID as id and the name from the <code>HumanResources.Department</code> table\".</p> <p>Info</p> <p>The <code>AS</code> keyword is optional when you're renaming columns, but it's a good idea to use it for clarity. It's clearer and easier to read <code>DepartmentID AS ID</code> than <code>DepartmentID ID</code>!</p>"},{"location":"from-excel-to-sql/main-concepts/select-and-from/#from-is-optional","title":"<code>FROM</code> is optional","text":"<p>Tip</p> <p>The <code>FROM</code> clause is only required when you're getting data from a table. If you're not getting data from a table, you can leave it out.</p> <p>Although you would use the <code>FROM</code> clause in most situations, it's worth knowing that it's optional. If you write <code>SELECT</code> without a <code>FROM</code> clause, the SQL will return a single row with the value(s) that you specify:</p> <pre><code>SELECT\n    'This is some text' AS SOME_TEXT,\n    123 AS SOME_NUMBER\n;\n</code></pre> SOME_TEXT SOME_NUMBER This is some text 123"},{"location":"from-excel-to-sql/main-concepts/select-and-from/#you-can-use-the-information_schema-to-see-whats-in-the-database","title":"You can use the <code>INFORMATION_SCHEMA</code> to see what's in the database","text":"<p>Warning</p> <p>This is slightly more advanced, but it's a useful trick to know about.</p> <p>The hardest part about SQL compared to Excel is that you can't just click around to see what's available -- when you write a query, you need to know what's in the database.</p> <p>However, most databases hold some special default tables that keep track of what's in the database. The <code>INFORMATION_SCHEMA</code> is a special folder/schema that holds these tables.</p> <p>To see the tables in the database, you can check the <code>TABLES</code> table in the <code>INFORMATION_SCHEMA</code>:</p> <pre><code>SELECT TABLE_SCHEMA, TABLE_NAME\nFROM INFORMATION_SCHEMA.TABLES\n;\n</code></pre> TABLE_SCHEMA TABLE_NAME Sales SalesTaxRate Sales PersonCreditCard Person vAdditionalContactInfo Person PersonPhone HumanResources vEmployee <p>To see the columns in the tables, you can check the <code>COLUMNS</code> table in the <code>INFORMATION_SCHEMA</code>:</p> <pre><code>SELECT TABLE_SCHEMA, TABLE_NAME, COLUMN_NAME\nFROM INFORMATION_SCHEMA.COLUMNS\n;\n</code></pre> TABLE_SCHEMA TABLE_NAME COLUMN_NAME Sales PersonCreditCard BusinessEntityID Sales PersonCreditCard CreditCardID Sales PersonCreditCard ModifiedDate Person vAdditionalContactInfo BusinessEntityID Person vAdditionalContactInfo City Person vAdditionalContactInfo CountryRegion <p>Tip</p> <p>When you first start using a new database, you might find these queries useful to see what's available.</p> <p>The Microsoft SQL Server documentation for this <code>INFORMATION_SCHEMA</code> folder/schema is available at:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/relational-databases/system-information-schema-views/system-information-schema-views-transact-sql</li> </ul>"},{"location":"from-excel-to-sql/main-concepts/select-and-from/#further-reading","title":"Further reading","text":"<p>Check out the official Microsoft documentation for more information on the <code>SELECT</code> and <code>FROM</code> clauses at:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/t-sql/queries/select-clause-transact-sql</li> <li>https://learn.microsoft.com/en-us/sql/t-sql/queries/select-examples-transact-sql</li> </ul> <p>The video version of this content is also available at:</p> <ul> <li>https://youtu.be/vQka5zeU_so</li> </ul>"},{"location":"from-excel-to-sql/main-concepts/style-guide/","title":"Style guide \u2728","text":"<p>Success</p> <p>Although SQL is super flexible in how you write it, it's a good idea to stick to a consistent style.</p> <p>This makes it easier for others to read your code which is especially important when you're working in a team or when you're sharing your code with others.</p>"},{"location":"from-excel-to-sql/main-concepts/style-guide/#sql-style-is-a-contentious-topic","title":"SQL style is a contentious topic...","text":"<p>Anyone who has written SQL for a little while will find the style that they like, and they'll typically want to stick to it (for better or worse) \ud83d\ude1d</p> <p>There are plenty of conflicting styles out there, so this guide is just to recommend the style for things that most people agree on.</p>"},{"location":"from-excel-to-sql/main-concepts/style-guide/#some-general-rules","title":"Some general rules","text":""},{"location":"from-excel-to-sql/main-concepts/style-guide/#use-consistent-capitalisation","title":"Use consistent capitalisation","text":"<p>The capitalisation that you use for your column names and table names should match their case in the database.</p> <p>For example, the AdventureWorks database uses Pascal case for its table names, so you should use Pascal case in your SQL:</p> <pre><code>/* Good */\nSELECT FirstName, LastName\nFROM Person.Person\nWHERE BusinessEntityID &lt; 5\n;\n\n/* Bad */\nSELECT firstName, LASTNAME\nFROM person.PERSON\nWHERE businessentityid &lt; 5\n;\n</code></pre> <p>The capitalisation that you use for your SQL keywords (e.g. <code>SELECT</code>, <code>FROM</code>, <code>WHERE</code>) should be consistent, and should match the standards of your team (if you have one).</p> <pre><code>/* Good, if your team use uppercase */\nSELECT FirstName, LastName\nFROM Person.Person\nWHERE BusinessEntityID &lt; 5\n;\n\n/* Good, if your team use lowercase */\nselect FirstName, LastName\nfrom Person.Person\nwhere BusinessEntityID &lt; 5\n;\n\n/* Bad */\nSELECT FirstName, LastName\nfrom Person.Person\nWhEre BusinessEntityID &lt; 5\n;\n</code></pre>"},{"location":"from-excel-to-sql/main-concepts/style-guide/#put-new-clauses-on-new-lines","title":"Put new clauses on new lines","text":"<p>The main clauses (e.g. <code>SELECT</code>, <code>FROM</code>, <code>WHERE</code>) should be on new lines:</p> <pre><code>/* Good */\nSELECT FirstName, LastName\nFROM Person.Person\nWHERE BusinessEntityID &lt; 5\n;\n\n/* Bad */\nSELECT FirstName, LastName FROM\nPerson.Person WHERE\nBusinessEntityID &lt; 5;\n\n/* Bad */\nSELECT FirstName, LastName FROM Person.Person WHERE BusinessEntityID &lt; 5;\n</code></pre>"},{"location":"from-excel-to-sql/main-concepts/style-guide/#put-lists-on-indented-new-lines","title":"Put lists on indented new lines","text":"<p>For long lists of items (the length of \"long\" is contentious), put each item on a new line and indent it:</p> <pre><code>/* Good */\nSELECT\n    BusinessEntityID,\n    PersonType,\n    Title,\n    FirstName,\n    MiddleName,\n    LastName\nFROM Person.Person\nWHERE BusinessEntityID &lt; 20\n  AND PersonType = 'EM'\n  AND MiddleName IS NOT NULL\n  AND EmailPromotion = 2\n;\n\n/* Bad */\nSELECT BusinessEntityID, PersonType, Title, FirstName, MiddleName, LastName\nFROM Person.Person\nWHERE BusinessEntityID &lt; 20 AND PersonType = 'EM' AND MiddleName IS NOT NULL AND EmailPromotion = 2\n;\n</code></pre>"},{"location":"from-excel-to-sql/main-concepts/style-guide/#further-reading","title":"Further reading","text":"<p>There are several publicly available style guides for SQL. It's worth having a look at a few of them to see what you like and what you don't like:</p> <ul> <li>https://handbook.gitlab.com/handbook/business-technology/data-team/platform/sql-style-guide/</li> <li>https://docs.telemetry.mozilla.org/concepts/sql_style</li> <li>https://www.sqlstyle.guide/</li> </ul> <p>Once you're up to speed with SQL and looking to integrate some additional tools, you might want to check out SQLFluff which is a tool that will reformat your SQL into a consistent style for you:</p> <ul> <li>https://sqlfluff.com/</li> </ul>"},{"location":"from-excel-to-sql/main-concepts/subqueries/","title":"Subqueries \ud83e\udde9","text":"<p>Success</p> <p>Subqueries are undoubtedly one of the most powerful features in SQL -- but with great power comes great responsibility.</p> <p>Warning</p> <p>Excel doesn't really have an equivalent to subqueries (without using advanced Excel features). Be warned that this therefore may be a bit tricky to get used to!</p>"},{"location":"from-excel-to-sql/main-concepts/subqueries/#subqueries-are-nested-queries","title":"Subqueries are \"nested\" queries","text":"<p>Subqueries are one of the awesome things about SQL. They are queries \"nested\" within other queries.</p> <p>This allows you to do some pretty cool things, but is also easy to abuse! Make sure you don't go overboard with subqueries.</p> <p>The most common place to use a subquery is in the <code>FROM</code> clause to use the output of another query as if it were a table:</p> <pre><code>SELECT *\nFROM (\n    SELECT\n        BusinessEntityID AS ID,\n        FirstName AS Forename,\n        LastName AS Surname\n    FROM Person.Person\n    WHERE BusinessEntityID &lt;= 5\n) AS People\nWHERE Forename IN ('Ken', 'Rob')\n;\n</code></pre> ID Forename Surname 1 Ken S\u00e1nchez 4 Rob Walters <p>In the example above, the subquery is the query inside the parentheses. This subquery is used as if it were a table in the main/outer query.</p> <p>To help understand the example, you first need to know what the output of the subquery is:</p> <pre><code>SELECT\n    BusinessEntityID AS ID,\n    FirstName AS Forename,\n    LastName AS Surname\nFROM Person.Person\nWHERE BusinessEntityID &lt;= 5\n;\n</code></pre> ID Forename Surname 1 Ken S\u00e1nchez 2 Terri Duffy 3 Roberto Tamburello 4 Rob Walters 5 Gail Erickson <p>When we use this in the <code>FROM</code> clause, we're pretending that it's like a table that already exists -- just this table only has five rows, and the columns are called <code>ID</code>, <code>Forename</code>, and <code>Surname</code>.</p> <p>Since the subquery is used as if it were a table, we need to use the column names as they are in the subquery. This is why we use <code>Forename</code> in the outer <code>WHERE</code> clause (<code>WHERE Forename IN ('Ken', 'Rob')</code>) instead of <code>FirstName</code>.</p> <p>Note</p> <p>When you use a subquery in the <code>FROM</code> clause in Microsoft SQL Server, you need to give it an alias. This is why we have <code>AS People</code> at the end of the subquery.</p> <p>This is not the case in all SQL flavours, but it's a good habit to get into.</p>"},{"location":"from-excel-to-sql/main-concepts/subqueries/#subqueries-are-good-for-using-calculated-columns","title":"Subqueries are good for using calculated columns","text":"<p>One of the reasons we like to use subqueries is to use calculated columns in places that we can't use them directly.</p> <p>We've seen that if we want to use a calculated column in places like the <code>WHERE</code> and <code>GROUP BY</code> clauses, we need to use the same calculation in the <code>SELECT</code> clause:</p> <pre><code>SELECT\n    FORMAT(OrderDate, 'yyyy-MM') AS OrderMonth,\n    SUM(TotalDue) AS TotalSales\nFROM Sales.SalesOrderHeader\nWHERE FORMAT(OrderDate, 'yyyy-MM') IN ('2013-01', '2013-02', '2013-03')\nGROUP BY FORMAT(OrderDate, 'yyyy-MM')\nORDER BY OrderMonth\n;\n</code></pre> <p>If we instead do the calculation in a subquery, we can use the calculated column in the <code>WHERE</code> and <code>GROUP BY</code> clauses without having to repeat the calculation:</p> <pre><code>SELECT\n    OrderMonth,\n    SUM(TotalDue) AS TotalSales\nFROM (\n    SELECT\n        FORMAT(OrderDate, 'yyyy-MM') AS OrderMonth,\n        TotalDue\n    FROM Sales.SalesOrderHeader\n) AS Orders\nWHERE OrderMonth IN ('2013-01', '2013-02', '2013-03')\nGROUP BY OrderMonth\nORDER BY OrderMonth\n;\n</code></pre> <p>This isn't super helpful in the example above, but it would be in cases where the calculation is more complex.</p>"},{"location":"from-excel-to-sql/main-concepts/subqueries/#subqueries-can-be-used-in-other-places-too","title":"Subqueries can be used in other places too","text":"<p>Although the most common place to use subqueries is in the <code>FROM</code> clause, they can also be used in other places like the <code>WHERE</code> and <code>SELECT</code> clauses (plus others).</p> <p>However, there are different rules for using subqueries in these places: when we use the subquery in the <code>FROM</code> clause, we return an entire table; when we use the subquery in the <code>WHERE</code> or <code>SELECT</code> clauses, we return a single value.</p> <p>We'll see advanced examples of these in the correlated subqueries section.</p>"},{"location":"from-excel-to-sql/main-concepts/subqueries/#subqueries-in-the-where-clause","title":"Subqueries in the <code>WHERE</code> clause","text":"<p>A good use of a subquery in the <code>WHERE</code> clause is to check if a value meets some condition relative to the rest of the values.</p> <p>For example, the following query returns the sales orders whose total due is greater than the average total due for the table:</p> <pre><code>SELECT TOP 5\n    SalesOrderID,\n    OrderDate,\n    TotalDue\nFROM Sales.SalesOrderHeader\nWHERE TotalDue &gt; (\n    /* This produces a single value that we can compare against */\n    SELECT AVG(TotalDue)\n    FROM Sales.SalesOrderHeader\n)\n;\n</code></pre> SalesOrderID OrderDate TotalDue 43659 2011-05-31 00:00:00.000 23153.2339 43661 2011-05-31 00:00:00.000 36865.8012 43662 2011-05-31 00:00:00.000 32474.9324 43664 2011-05-31 00:00:00.000 27510.4109 43665 2011-05-31 00:00:00.000 16158.6961"},{"location":"from-excel-to-sql/main-concepts/subqueries/#subqueries-in-the-select-clause","title":"Subqueries in the <code>SELECT</code> clause","text":"<p>Similarly, we can use subqueries in the <code>SELECT</code> clause to keep track of some aggregate value for each row.</p> <p>For example, the following query returns the sales orders and the average total due for the table:</p> <pre><code>SELECT TOP 5\n    SalesOrderID,\n    OrderDate,\n    TotalDue,\n    (\n        /* This produces a single value that we can use in a column */\n        SELECT AVG(TotalDue)\n        FROM Sales.SalesOrderHeader\n    ) AS AverageTotalDue\nFROM Sales.SalesOrderHeader\n;\n</code></pre> SalesOrderID OrderDate TotalDue AverageTotalDue 43659 2011-05-31 00:00:00.000 23153.2339 3915.9951 43660 2011-05-31 00:00:00.000 1457.3288 3915.9951 43661 2011-05-31 00:00:00.000 36865.8012 3915.9951 43662 2011-05-31 00:00:00.000 32474.9324 3915.9951 43663 2011-05-31 00:00:00.000 472.3108 3915.9951 <p>Info</p> <p>We'll see another way to do this example in the window functions section, but it's good to know that subqueries can be used in this way too.</p>"},{"location":"from-excel-to-sql/main-concepts/subqueries/#common-tables-expressions-ctes-are-another-flavour-of-subquery","title":"Common tables expressions (CTEs) are another flavour of subquery","text":"<p>Rather than using subqueries directly inside the <code>SELECT</code> statements, SQL allows us to save the subquery as a \"common table expression\" (CTE) and then use the CTE in the <code>SELECT</code> statements.</p> <p>To define a CTE, we use the <code>WITH</code> keyword followed by the name of the CTE and the subquery in parentheses. You can specify multiple CTEs after the <code>WITH</code> text by separating them with commas.</p> <p>For example, we saw the following query earlier:</p> <pre><code>SELECT\n    OrderMonth,\n    SUM(TotalDue) AS TotalSales\nFROM (\n    SELECT\n        FORMAT(OrderDate, 'yyyy-MM') AS OrderMonth,\n        TotalDue\n    FROM Sales.SalesOrderHeader\n) AS Orders\nWHERE OrderMonth IN ('2013-01', '2013-02', '2013-03')\nGROUP BY OrderMonth\nORDER BY OrderMonth\n;\n</code></pre> <p>We could instead save the <code>Orders</code> subquery as a CTE and then use the CTE in the <code>FROM</code> clause:</p> <pre><code>WITH Orders AS (\n    SELECT\n        FORMAT(OrderDate, 'yyyy-MM') AS OrderMonth,\n        TotalDue\n    FROM Sales.SalesOrderHeader\n)\n\nSELECT\n    OrderMonth,\n    SUM(TotalDue) AS TotalSales\nFROM Orders\nWHERE OrderMonth IN ('2013-01', '2013-02', '2013-03')\nGROUP BY OrderMonth\nORDER BY OrderMonth\n;\n</code></pre> <p>Tip</p> <p>In general, using CTEs (versus using subqueries directly) is a good habit to get into as it makes your queries easier to read and understand.</p> <p>Warning</p> <p>Subqueries defined as CTEs are always treated as if they were tables, so although it was an easy \"lift-and-shift\" for the subquery in the <code>FROM</code> clause, it wouldn't be as simple for subqueries in the <code>WHERE</code> and <code>SELECT</code> clauses.</p>"},{"location":"from-excel-to-sql/main-concepts/subqueries/#further-reading","title":"Further reading","text":"<p>Check out the official Microsoft documentation for more information on subqueries at:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/relational-databases/performance/subqueries</li> </ul> <p>The video version of this content is also available at:</p> <ul> <li>https://youtu.be/pcD_7n7zKFw</li> </ul>"},{"location":"from-excel-to-sql/main-concepts/top-and-distinct/","title":"<code>TOP</code> and <code>DISTINCT</code> \ud83d\udd1d","text":"<p>Success</p> <p>The <code>TOP</code> and <code>DISTINCT</code> keywords are handy modifiers to use after <code>SELECT</code>.</p> <p>Note</p> <p><code>TOP</code> and <code>DISTINCT</code> should be used immediately after the <code>SELECT</code> keyword.</p>"},{"location":"from-excel-to-sql/main-concepts/top-and-distinct/#top-will-limit-the-number-of-rows-returned","title":"<code>TOP</code> will limit the number of rows returned","text":"<p>The <code>TOP</code> keyword is used to limit the number of rows returned from a <code>SELECT</code> statement. This is particularly useful when you're only interested in the first few rows of a result.</p> <pre><code>SELECT TOP 5\n    DepartmentID,\n    Name\nFROM HumanResources.Department\n;\n</code></pre> DepartmentID Name 12 Document Control 1 Engineering 16 Executive 14 Facilities and Maintenance 10 Finance <p>If you use <code>TOP</code> with an <code>ORDER BY</code> clause, the rows will be ordered first and then the <code>TOP</code> will be applied:</p> <pre><code>SELECT TOP 5\n    DepartmentID,\n    Name\nFROM HumanResources.Department\nORDER BY DepartmentID\n;\n</code></pre> DepartmentID Name 1 Engineering 2 Tool Design 3 Sales 4 Marketing 5 Purchasing"},{"location":"from-excel-to-sql/main-concepts/top-and-distinct/#other-sql-flavours-might-use-limit-instead-of-top","title":"Other SQL flavours might use <code>LIMIT</code> instead of <code>TOP</code>","text":"<p>This is just an FYI! Although Microsoft SQL Server (and a few other SQL flavours) use <code>TOP</code>, other SQL flavours use the keyword <code>LIMIT</code> instead. Flavours that use <code>LIMIT</code> also put it at the end of the statement instead of after the <code>SELECT</code> clause, too, like this:</p> <pre><code>SELECT\n    DepartmentID,\n    Name\nFROM HumanResources.Department\nORDER BY DepartmentID\nLIMIT 5\n;\n</code></pre> <p>Make sure you know which SQL flavour you're using so that you can use the right keyword!</p>"},{"location":"from-excel-to-sql/main-concepts/top-and-distinct/#distinct-will-remove-duplicate-rows-from-the-result","title":"<code>DISTINCT</code> will remove duplicate rows from the result","text":"<p>Success</p> <p><code>DISTINCT</code> is equivalent to using the \"Remove Duplicates\" feature in Excel!</p> <p>The <code>DISTINCT</code> keyword can take a bit of getting used to, but it is used to remove duplicate rows from the result. Note that \"duplicate\" means every value in the row is the same as the corresponding value in another row.</p> <p>For example, this output has rows that are the same:</p> <pre><code>SELECT\n    GroupName,\n    ModifiedDate\nFROM HumanResources.Department\nWHERE GroupName = 'Research and Development'\n;\n</code></pre> GroupName ModifiedDate Research and Development 2008-04-30 00:00:00.000 Research and Development 2008-04-30 00:00:00.000 Research and Development 2008-04-30 00:00:00.000 <p>Adding <code>DISTINCT</code> will remove the duplicates, keeping only the unique rows:</p> <pre><code>SELECT DISTINCT\n    GroupName,\n    ModifiedDate\nFROM HumanResources.Department\nWHERE GroupName = 'Research and Development'\n;\n</code></pre> GroupName ModifiedDate Research and Development 2008-04-30 00:00:00.000 <p>This keyword is particularly useful when you're interested in the unique values of a column, such as:</p> <pre><code>SELECT DISTINCT GroupName\nFROM HumanResources.Department\n;\n</code></pre> GroupName Executive General and Administration Inventory Management Manufacturing Quality Assurance Research and Development Sales and Marketing"},{"location":"from-excel-to-sql/main-concepts/top-and-distinct/#top-and-distinct-can-be-used-together","title":"<code>TOP</code> and <code>DISTINCT</code> can be used together","text":"<p>Although it might not be super helpful, you can use <code>TOP</code> and <code>DISTINCT</code> together to get the first few unique rows from a result:</p> <pre><code>SELECT DISTINCT TOP 3\n    GroupName,\n    ModifiedDate\nFROM HumanResources.Department\n;\n</code></pre> GroupName ModifiedDate Executive General and Administration 2008-04-30 00:00:00.000 Inventory Management 2008-04-30 00:00:00.000 Manufacturing 2008-04-30 00:00:00.000 <p>The keyword order is important here -- <code>DISTINCT</code> has to be written before <code>TOP</code>.</p>"},{"location":"from-excel-to-sql/main-concepts/top-and-distinct/#further-reading","title":"Further reading","text":"<p>Check out the official Microsoft documentation for more information on <code>TOP</code> and <code>DISTINCT</code> at:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/t-sql/queries/top-transact-sql</li> <li>https://learn.microsoft.com/en-us/sql/t-sql/queries/select-transact-sql#c-using-distinct-with-select</li> </ul> <p>The video version of this content is also available at:</p> <ul> <li>https://youtu.be/-0M-kEkoDqw</li> </ul>"},{"location":"from-excel-to-sql/main-concepts/top-and-distinct/#additional-modifiers","title":"Additional modifiers","text":"<p>The <code>TOP</code> keyword also has additional modifiers which are outside the scope of this course. These include <code>PERCENT</code>, <code>WITH TIES</code>, and using an expression rather than a fixed number:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/t-sql/queries/top-transact-sql#c-specifying-a-percentage</li> <li>https://learn.microsoft.com/en-us/sql/t-sql/queries/top-transact-sql#a-using-with-ties-to-include-rows-that-match-the-values-in-the-last-row</li> <li>https://learn.microsoft.com/en-us/sql/t-sql/queries/top-transact-sql#arguments</li> </ul> <p>Error</p> <p>This is a contrived example (using a larger table) to show the additional modifiers.</p> <pre><code>SELECT TOP (10 * RAND()) PERCENT WITH TIES\n    FirstName,\n    LastName\nFROM Person.Person\nORDER BY FirstName\n;\n</code></pre>"},{"location":"from-excel-to-sql/main-concepts/union/","title":"Unions \ud83e\uddec","text":"<p>Success</p> <p>Like joins, unions are a way to combine data from tables, but they do it vertically instead of horizontally.</p> <p>That is, a <code>JOIN</code> adds columns to a table, while a <code>UNION</code> adds rows to a table.</p> <p>Note</p> <p>The <code>UNION</code> clause is optional. If you use it, it must be between two individual <code>SELECT</code> statements.</p> <p>This makes the <code>UNION</code> clause a bit different to what we've seen so far!</p> <p>Warning</p> <p>Excel doesn't really have an equivalent to SQL's <code>UNION</code> (without using advanced Excel features), but it's just like stacking two tables on top of each other \ud83d\ude1d</p>"},{"location":"from-excel-to-sql/main-concepts/union/#the-union-clause-also-combines-data-from-tables","title":"The <code>UNION</code> clause also combines data from tables","text":"<p>Sometimes you want to combine data from two tables, but you don't want to join them together. Instead, you want to \"stack\" the tables on top of each other.</p> <p>The <code>UNION</code> clause does this by taking the results of two <code>SELECT</code> statements and combining them into a single result.</p> <p>Since SQL tables are fairly rigid, there are two important rules that the separate <code>SELECT</code> statement must follow to be able to be combined with a <code>UNION</code> clause:</p> <ol> <li>The number of columns in each <code>SELECT</code> statement must be the same.</li> <li>The data types of the columns in each <code>SELECT</code> statement must be compatible.</li> </ol> <p>To be super clear, the <code>UNION</code> clause does not check the names of the columns in the <code>SELECT</code> statements. It only checks the number of columns and the data types of the columns, and only keeps the names from the first <code>SELECT</code> statement.</p> <p>This makes the <code>UNION</code> clause more prone to error than other clauses, but it's still a handy feature to know about.</p> <p>The following is a contrived example, but it shows how the <code>UNION</code> clause works:</p> <pre><code>    SELECT TOP 3\n        BusinessEntityID,\n        PersonType,\n        FirstName,\n        LastName\n    FROM Person.Person\n    WHERE PersonType = 'EM'\nUNION\n    SELECT TOP 3\n        BusinessEntityID,\n        PersonType,\n        FirstName,\n        LastName\n    FROM Person.Person\n    WHERE PersonType = 'SP'\n;\n</code></pre> BusinessEntityID PersonType FirstName LastName 1 EM Ken S\u00e1nchez 2 EM Terri Duffy 3 EM Roberto Tamburello 274 SP Stephen Jiang 275 SP Michael Blythe 276 SP Linda Mitchell"},{"location":"from-excel-to-sql/main-concepts/union/#union-vs-union-all","title":"<code>UNION</code> vs <code>UNION ALL</code>","text":"<p>By itself, the <code>UNION</code> clause removes duplicate rows from the combined result set just as if you had used the <code>DISTINCT</code> clause:</p> <pre><code>    SELECT 1 AS Number, 'a' AS Letter\nUNION\n    SELECT 1, 'a'\nUNION\n    SELECT 2, 'b'\nUNION\n    SELECT 2, 'c'\n;\n</code></pre> Number Letter 1 a 2 b 2 c <p>If you want to keep the duplicate rows, you can use the <code>UNION ALL</code> clause instead:</p> <pre><code>    SELECT 1 AS Number, 'a' AS Letter\nUNION ALL\n    SELECT 1, 'a'\nUNION ALL\n    SELECT 2, 'b'\nUNION ALL\n    SELECT 2, 'c'\n;\n</code></pre> Number Letter 1 a 1 a 2 b 2 c"},{"location":"from-excel-to-sql/main-concepts/union/#further-reading","title":"Further reading","text":"<p>Check out the official Microsoft documentation for more information on the <code>UNION</code> clause at:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/t-sql/language-elements/set-operators-union-transact-sql</li> </ul> <p>The video version of this content is also available at:</p> <ul> <li>https://youtu.be/OQSLOGelJv0</li> </ul>"},{"location":"from-excel-to-sql/main-concepts/union/#additional-set-operators","title":"Additional set operators","text":"<p>The <code>UNION</code> clause combines the results of two <code>SELECT</code> statements into a single result set.</p> <p>There are two other set operators outside the scope of this course. They are <code>EXCEPT</code> and <code>INTERSECT</code>:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/t-sql/language-elements/set-operators-except-and-intersect-transact-sql</li> <li>https://learn.microsoft.com/en-us/sql/t-sql/language-elements/set-operators-except-and-intersect-transact-sql</li> </ul>"},{"location":"from-excel-to-sql/main-concepts/where/","title":"Filtering \ud83d\udea6","text":"<p>Success</p> <p>The <code>WHERE</code> clause is used to filter the rows in a query. It's like the \"filter\" feature in Excel!</p> <p>Note</p> <p>The <code>WHERE</code> clause is optional. If you use it, it must come after the <code>FROM</code> clause.</p>"},{"location":"from-excel-to-sql/main-concepts/where/#where-is-how-we-filter-rows","title":"<code>WHERE</code> is how we filter rows","text":"<p>So far, we've been able to \"open a file\" by using <code>SELECT</code> and <code>FROM</code>.</p> <p>To filter rows, use the <code>WHERE</code> clause and specify the condition that you want to filter by.</p> <p>Excel is convenient and gives us a pop-up box to select the values we want to filter by, but for SQL we have to write the condition ourselves. The way that we write the condition is similar to how we write the conditions for the <code>IF</code> function in Excel using the following operators:</p> <ul> <li><code>&lt;</code> (less than)</li> <li><code>&lt;=</code> (less than or equal to)</li> <li><code>&gt;</code> (greater than)</li> <li><code>&gt;=</code> (greater than or equal to)</li> <li><code>=</code> (equals)</li> <li><code>&lt;&gt;</code> (does not equal), also written as <code>!=</code></li> </ul> <p>For example, we could filter the <code>HumanResources.Department</code> table for people who have the first name <code>Rob</code> using:</p> <pre><code>SELECT\n    DepartmentID,\n    Name,\n    GroupName\nFROM HumanResources.Department\nWHERE DepartmentID = 5\n;\n</code></pre> DepartmentID Name GroupName 5 Purchasing Inventory Management <p>Tip</p> <p>The <code>WHERE</code> clause \"sounds like\" English, so the query above can be read as:</p> <p>\"Select the department ID, name, and group name from the <code>HumanResources.Department</code> table where the department ID is 5\".</p> <p>Similarly, we could filter the <code>HumanResources.Department</code> table for departments whose ID is less than or equal to <code>5</code> using:</p> <pre><code>SELECT\n    DepartmentID,\n    Name,\n    GroupName\nFROM HumanResources.Department\nWHERE DepartmentID &lt;= 5\n;\n</code></pre> DepartmentID Name GroupName 1 Engineering Research and Development 2 Tool Design Research and Development 3 Sales Sales and Marketing 4 Marketing Sales and Marketing 5 Purchasing Inventory Management"},{"location":"from-excel-to-sql/main-concepts/where/#conditions-can-be-combined-with-and-and-or","title":"Conditions can be combined with <code>AND</code> and <code>OR</code>","text":"<p>If you wanted to have multiple conditions in an <code>IF</code> statement in Excel, you'd need to use the <code>AND</code> or <code>OR</code> functions:</p> <pre><code>=IF(AND(A1 = \"Alan\", B1 &lt;= 5), \"Yes\", \"No\")\n</code></pre> <p>In SQL, <code>AND</code> and <code>OR</code> aren't functions; they're keywords that you use to combine conditions in the <code>WHERE</code> clause.</p> <p>For example, we could filter the <code>HumanResources.Department</code> table for departments whose ID is less than or equal to <code>5</code> and the department group name is <code>Research and Development</code> with:</p> <pre><code>SELECT\n    DepartmentID,\n    Name,\n    GroupName\nFROM HumanResources.Department\nWHERE DepartmentID &lt;= 5 AND GroupName = 'Research and Development'\n;\n</code></pre> DepartmentID Name GroupName 1 Engineering Research and Development 2 Tool Design Research and Development <p>Tip</p> <p>Combining conditions \"sounds like\" English, so the query above can be read as:</p> <p>\"Select the department ID, name, and group name from the <code>HumanResources.Department</code> table where the department ID is less than or equal to 5 and the group name is <code>Research and Development</code>\".</p> <p>Similarly, we could filter the <code>HumanResources.Department</code> table for departments whose group name is either <code>Sales and Marketing</code> or <code>Research and Development</code> using:</p> <pre><code>SELECT\n    DepartmentID,\n    Name,\n    GroupName\nFROM HumanResources.Department\nWHERE GroupName = 'Sales and Marketing' OR GroupName = 'Research and Development'\n;\n</code></pre> DepartmentID Name GroupName 1 Engineering Research and Development 2 Tool Design Research and Development 3 Sales Sales and Marketing 4 Marketing Sales and Marketing 6 Research and Development Research and Development <p>It's common to use <code>IN</code> (and <code>NOT IN</code>) to streamline multiple <code>OR</code> conditions. For example, the previous query could be written as:</p> <pre><code>SELECT\n    DepartmentID,\n    Name,\n    GroupName\nFROM HumanResources.Department\nWHERE GroupName IN ('Sales and Marketing', 'Research and Development')\n;\n</code></pre> <p>Note that the <code>IN</code> keyword is followed by a list of values in brackets separated by commas.</p> <p>You can combine <code>AND</code> and <code>OR</code> in the same <code>WHERE</code> clause, but it's a good idea to use brackets to make the order of operations clear. For example, the following query filters the <code>HumanResources.Department</code> table for departments whose ID is less than or equal to <code>5</code> and the department group name is <code>Research and Development</code>, or the department group name is ``:</p> <pre><code>SELECT\n    DepartmentID,\n    Name,\n    GroupName\nFROM HumanResources.Department\nWHERE (DepartmentID &lt;= 5 AND GroupName = 'Research and Development')\n   OR DepartmentID = 10\n;\n</code></pre> DepartmentID Name GroupName 1 Engineering Research and Development 2 Tool Design Research and Development 10 Finance Executive General and Administration"},{"location":"from-excel-to-sql/main-concepts/where/#use-is-not-null-to-filter-null-values","title":"Use <code>IS</code> (<code>NOT</code>) <code>NULL</code> to filter <code>NULL</code> values","text":"<p>Warning</p> <p>We'll learn more about <code>NULL</code> values in the Data types section, but for now, we'll mention that <code>NULL</code> is a special value that you'll sometimes see in SQL which represents a missing value similar to how <code>(blank)</code> is used in Excel.</p> <p>Instead of using <code>=</code> or <code>!=</code> to filter <code>NULL</code> values, you need to use the special <code>IS NULL</code> or <code>IS NOT NULL</code> keywords.</p> <p>For example, we could filter the <code>HumanResources.EmployeeDepartmentHistory</code> table for employees whose department end date is missing using:</p> <pre><code>SELECT\n    BusinessEntityID,\n    DepartmentID,\n    StartDate,\n    EndDate\nFROM HumanResources.EmployeeDepartmentHistory\nWHERE EndDate IS NULL\n</code></pre> BusinessEntityID DepartmentID StartDate EndDate 1 16 2009-01-14 null 2 1 2008-01-31 null 3 1 2007-11-11 null 4 2 2010-05-31 null 5 1 2008-01-06 null"},{"location":"from-excel-to-sql/main-concepts/where/#further-reading","title":"Further reading","text":"<p>Check out the official Microsoft documentation for more information on the <code>WHERE</code> clause at:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/t-sql/queries/where-transact-sql</li> </ul> <p>The video version of this content is also available at:</p> <ul> <li>https://youtu.be/g_5OxUYPx7E</li> </ul>"},{"location":"from-excel-to-sql/main-concepts/where/#additional-comparison-operators","title":"Additional comparison operators","text":"<p>There are additional comparison operators that you can use in the <code>WHERE</code> clause which are outside the scope of this course. These include but are not limited to the <code>BETWEEN</code>, <code>LIKE</code>, and <code>EXISTS</code> operators:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/t-sql/language-elements/between-transact-sql</li> <li>https://learn.microsoft.com/en-us/sql/t-sql/language-elements/like-transact-sql</li> <li>https://learn.microsoft.com/en-us/sql/t-sql/language-elements/exists-transact-sql</li> </ul> <p>Error</p> <p>This is a contrived example to show some of the additional comparison operators.</p> <pre><code>SELECT\n    DepartmentID,\n    Name,\n    GroupName\nFROM HumanResources.Department\nWHERE DepartmentID BETWEEN 1 AND 5\n   OR GroupName NOT LIKE '%and%'\n;\n</code></pre>"},{"location":"from-excel-to-sql/main-concepts/window-functions/","title":"Window functions \ud83d\udcc8","text":"<p>Success</p> <p>Windows functions allow us to summarise rows without having to use the <code>GROUP BY</code> clause.</p> <p>There are two ways to use window functions, and we'll see that both are very similar to row aggregations that we do in Excel!</p> <p>Warning</p> <p>Window functions are widely regarded as an advanced concept in SQL. Hopefully this page will show that they're not as scary as people make them seem!</p>"},{"location":"from-excel-to-sql/main-concepts/window-functions/#window-functions-are-another-way-to-summarise-rows","title":"Window functions are another way to summarise rows","text":"<p>We saw in the aggregations section that we can use the <code>GROUP BY</code> clause to summarise rows. An important result of this is that the number of rows is reduced.</p> <p>However, there are times when we want to summarise rows without reducing the number of rows; that is, without having to use <code>GROUP BY</code>.</p> <p>This is where window functions come in: they summarise rows without reducing the row count.</p> <p>There are three extremely common use cases for window functions:</p> <ul> <li>Summarising entire columns</li> <li>Calculating running totals</li> <li>Calculating moving averages</li> </ul> <p>I'll bet you've done these in Excel before! \ud83d\ude1d</p>"},{"location":"from-excel-to-sql/main-concepts/window-functions/#theres-some-terminology-to-be-aware-of","title":"There's some terminology to be aware of","text":"<p>Info</p> <p>Although we've been throwing around the term \"window function\", the actual SQL feature that we use to achieve this is the <code>OVER</code> clause.</p> <p>Before going into an example, there's a bit of terminology to clarify.</p> <p>Window functions are functions that can be used with the <code>OVER</code> clause, such as:</p> <ul> <li><code>ROW_NUMBER</code></li> <li><code>LAG</code></li> <li><code>FIRST_VALUE</code></li> </ul> <p>The \"aggregate functions\" we've seen so far (like <code>SUM</code>, <code>AVG</code>, <code>COUNT</code>, etc.) are designed to be used with the <code>GROUP BY</code> clause, but many of them can also be used with the <code>OVER</code> clause. This means that these aggregate functions are also window functions!</p>"},{"location":"from-excel-to-sql/main-concepts/window-functions/#the-over-clause-is-the-key-to-window-functions","title":"The <code>OVER</code> clause is the key to window functions","text":"<p>So... how do we use window functions?</p> <p>Since window functions don't reduce the number of rows, we use them in the <code>SELECT</code> clause to add a new column to the result set just like we would if we were doing any other kind of column calculation.</p> <p>Specifically, we write the window function followed by the <code>OVER</code> clause, which is followed by a set of parentheses. Inside the parentheses, we can specify the window over which we want to perform the calculation.</p> <p>The following example shows how to get the count of rows in the <code>Person.Person</code> table alongside showing the first few rows and columns:</p> <pre><code>SELECT TOP 5\n    BusinessEntityID,\n    FirstName,\n    LastName,\n    COUNT(*) OVER () AS TotalRows\nFROM Person.Person\nORDER BY BusinessEntityID\n;\n</code></pre> BusinessEntityID FirstName LastName TotalRows 1 Ken S\u00e1nchez 19972 2 Terri Duffy 19972 3 Roberto Tamburello 19972 4 Rob Walters 19972 5 Gail Erickson 19972 <p>The parentheses after <code>OVER</code> are important (even if they're empty). We'll go through what goes inside the parentheses shortly.</p> <p>To confirm that the <code>TotalRows</code> number is correct, we can verify it with a separate query:</p> <pre><code>SELECT COUNT(*) AS TotalRows\nFROM Person.Person\n;\n</code></pre> TotalRows 19972 <p>Looks good!</p> <p>Warning</p> <p>We used the <code>TOP 5</code> in the query with the window function to limit the number of rows returned, so why did we get 19972 instead of 5?</p> <p>This is to do with the order that SQL processes the query, which is (briefly) covered in the logical processing order section -- <code>TOP</code> is processed after <code>OVER</code>!</p>"},{"location":"from-excel-to-sql/main-concepts/window-functions/#converting-a-subquery-to-a-window-function","title":"Converting a subquery to a window function","text":"<p>Remember the example below from the subqueries section?</p> <pre><code>SELECT TOP 5\n    SalesOrderID,\n    OrderDate,\n    TotalDue,\n    (\n        SELECT AVG(TotalDue)\n        FROM Sales.SalesOrderHeader\n    ) AS AverageTotalDue\nFROM Sales.SalesOrderHeader\n;\n</code></pre> <p>We can rewrite this example to use <code>AVG</code> with <code>OVER</code> instead of the subquery:</p> <pre><code>SELECT TOP 5\n    SalesOrderID,\n    OrderDate,\n    TotalDue,\n    AVG(TotalDue) OVER () AS AverageTotalDue\nFROM Sales.SalesOrderHeader\n;\n</code></pre> <p>Tip</p> <p>We've just seen that two very different features -- subqueries and window functions -- can be used to achieve the same result. The natural question is: which one should you use?</p> <p>The answer is: it depends on the situation. However, my personal preference is to use window functions over subqueries until I have a reason not to.</p>"},{"location":"from-excel-to-sql/main-concepts/window-functions/#defining-the-partition-of-a-window-function","title":"Defining the partition of a window function","text":"<p>The examples above just specify <code>OVER</code> with empty parentheses. This is the simplest form of the <code>OVER</code> clause, and it means that the window function is applied to all rows in the result set.</p> <p>The output that this produces is equivalent to summarising the entire column, which was proven in the first example by comparing the <code>TotalRows</code> column to the count of rows in the table.</p> <p>However, just like how we can <code>GROUP BY</code> specific columns, we can also specify a window over specific columns. We do this by writing <code>PARTITION BY</code> inside the parentheses of the <code>OVER</code> clause, followed by the column(s) that we want to partition by. Although we write <code>PARTITION BY</code>, this is just like doing <code>GROUP BY</code>!</p> <p>The following example shows how to get the count of rows in the <code>Person.Person</code> table partitioned by the <code>EmailPromotion</code> column:</p> <pre><code>SELECT TOP 10\n    BusinessEntityID,\n    FirstName,\n    LastName,\n    EmailPromotion,\n    COUNT(*) OVER (PARTITION BY EmailPromotion) AS TotalRowsPerEmailPromotion\nFROM Person.Person\nORDER BY BusinessEntityID\n;\n</code></pre> BusinessEntityID FirstName LastName EmailPromotion TotalRowsPerEmailPromotion 1 Ken S\u00e1nchez 0 11158 2 Terri Duffy 1 5044 3 Roberto Tamburello 0 11158 4 Rob Walters 0 11158 5 Gail Erickson 0 11158 6 Jossef Goldberg 0 11158 7 Dylan Miller 2 3770 8 Diane Margheim 0 11158 9 Gigi Matthew 0 11158 10 Michael Raheem 2 3770 <p>Notice that the <code>TotalRowsPerEmailPromotion</code> column shows the count of rows for each <code>EmailPromotion</code> value, and it just repeats that count for each row with the same <code>EmailPromotion</code> value. This is precisely why using <code>PARTITION BY</code> is like using <code>GROUP BY</code> -- it does the grouping just like it would if we were using <code>GROUP BY</code>, but it doesn't reduce the number of rows!</p> <p>Tip</p> <p>When we use <code>PARTITION BY</code> in a window function, we're splitting the result set into partitions based on the column(s) that we specify. The window function is then applied to each partition separately.</p> <p>Success</p> <p>When we create partitions, this is like using Excel's \"IF\" summary functions over an entire column, such as <code>=SUMIF(A:A, A2)</code>.</p>"},{"location":"from-excel-to-sql/main-concepts/window-functions/#there-are-two-other-ways-to-use-window-functions","title":"There are two other ways to use window functions","text":"<p>The examples above show how to use window functions over different partitions (including a single partition, the entire table).</p> <p>Other window functions are designed to be used with the rows considered in some order, so we can also specify <code>ORDER BY</code> inside the parentheses of the <code>OVER</code> clause.</p> <p>A great example of this is the <code>LAG</code> function which looks \"one row up\" based on the order that we specify. For example, the following query shows the first few people in the <code>Person.Person</code> table with the first names of the person who came before them (based on the <code>BusinessEntityID</code> column):</p> <pre><code>SELECT TOP 5\n    BusinessEntityID,\n    FirstName,\n    LastName,\n    LAG(FirstName) OVER (ORDER BY BusinessEntityID) AS PreviousFirstName\nFROM Person.Person\nORDER BY BusinessEntityID\n;\n</code></pre> BusinessEntityID FirstName LastName PreviousFirstName 1 Ken S\u00e1nchez null 2 Terri Duffy Ken 3 Roberto Tamburello Terri 4 Rob Walters Roberto 5 Gail Erickson Rob <p>This is just like using a relative cell reference in Excel!</p> <p>Although this isn't a super helpful example, this demonstrates a case where we need to tell SQL which order to use to understand \"previous\" and \"next\" rows. There are technical reasons for it that we won't go into, but SQL will not assume any order unless we specify it.</p> <p>With this in mind, there are two types of windows that we can specify:</p> <ul> <li>Cumulative windows, which are things like \"all rows up to this one\" and are useful for calculating running totals</li> <li>Sliding windows, which are things like \"the last three rows\" and are useful for calculating moving averages</li> </ul>"},{"location":"from-excel-to-sql/main-concepts/window-functions/#cumulative-windows-are-great-for-running-totals","title":"Cumulative windows are great for running totals","text":"<p>One of the most common things to do in Excel is to calculate running totals.</p> <p>We can do this in SQL too by using <code>SUM</code> with the <code>OVER</code> clause, making sure that we specify a row order:</p> <pre><code>SELECT TOP 10\n    SalesOrderID,\n    TotalDue,\n    SUM(TotalDue) OVER (ORDER BY SalesOrderID) AS RunningTotal\nFROM Sales.SalesOrderHeader\nORDER BY SalesOrderID\n;\n</code></pre> SalesOrderID TotalDue RunningTotal 43659 23153.2339 23153.2339 43660 1457.3288 24610.5627 43661 36865.8012 61476.3639 43662 32474.9324 93951.2963 43663 472.3108 94423.6071 43664 27510.4109 121934.0180 43665 16158.6961 138092.7141 43666 5694.8564 143787.5705 43667 6876.3649 150663.9354 43668 40487.7233 191151.6587 <p>There are a few ways that you could do this in Excel. The SQL approach is just like using the formula <code>=SUM($B$2:B2)</code> in cell <code>C2</code> and dragging it down (assuming that the <code>TotalDue</code> column is in column <code>B</code> and the <code>RunningTotal</code> column is in column <code>C</code>).</p> <p>In contrast, if we didn't specify <code>ORDER BY</code> inside the parentheses of the <code>OVER</code> clause, the <code>SUM</code> would be calculated over the entire table:</p> <pre><code>SELECT TOP 10\n    SalesOrderID,\n    TotalDue,\n    SUM(TotalDue) OVER () AS RunningTotal  /* This is now a misnomer */\nFROM Sales.SalesOrderHeader\nORDER BY SalesOrderID\n;\n</code></pre> SalesOrderID TotalDue RunningTotal 43659 23153.2339 123216786.1159 43660 1457.3288 123216786.1159 43661 36865.8012 123216786.1159 43662 32474.9324 123216786.1159 43663 472.3108 123216786.1159 43664 27510.4109 123216786.1159 43665 16158.6961 123216786.1159 43666 5694.8564 123216786.1159 43667 6876.3649 123216786.1159 43668 40487.7233 123216786.1159 <p>This is quite a different result!</p>"},{"location":"from-excel-to-sql/main-concepts/window-functions/#sliding-windows-are-great-for-moving-averages","title":"Sliding windows are great for moving averages","text":"<p>Another common thing to do in Excel is to calculate moving averages.</p> <p>We can do this in SQL too by using <code>AVG</code> with the <code>OVER</code> clause, making sure that we specify a row order and a window size. The \"window size\" is some more new syntax, so let's go through an example to see how it works.</p> <p>The following query shows the first few orders in the <code>Sales.SalesOrderHeader</code> table with the moving average of the <code>TotalDue</code> column over the last three orders (based on the <code>SalesOrderID</code> column):</p> <pre><code>SELECT TOP 10\n    SalesOrderID,\n    TotalDue,\n    AVG(TotalDue) OVER (\n        ORDER BY SalesOrderID\n        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n    ) AS MovingAverage\nFROM Sales.SalesOrderHeader\nORDER BY SalesOrderID\n;\n</code></pre> SalesOrderID TotalDue MovingAverage 43659 23153.2339 23153.2339 43660 1457.3288 12305.2813 43661 36865.8012 20492.1213 43662 32474.9324 23599.3541 43663 472.3108 23271.0148 43664 27510.4109 20152.5513 43665 16158.6961 14713.8059 43666 5694.8564 16454.6544 43667 6876.3649 9576.6391 43668 40487.7233 17686.3148 <p>To compare this to Excel, this is just like using the formula <code>=AVERAGE(B2:B4)</code> in cell <code>C4</code> and dragging it down (assuming that the <code>TotalDue</code> column is in column <code>B</code> and the <code>MovingAverage</code> column is in column <code>C</code>).</p> <p>To define the sliding window (the window size), we wrote:</p> <pre><code>ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n</code></pre> <p>This sounds a bit funny, but should be fairly intuitive:</p> <ul> <li>The <code>ROWS BETWEEN</code> tells SQL that we're going to define a window based on the number of rows</li> <li>The <code>2 PRECEDING</code> means \"two rows before the current row\"</li> <li>The <code>CURRENT ROW</code> part is clear and means \"the current row\" \ud83d\ude1d</li> </ul> <p>Altogether, this means \"the last three rows\".</p> <p>We could also look a few rows after the current row by using <code>FOLLOWING</code> instead of <code>PRECEDING</code>; the following window would be \"the current row and the next two rows\":</p> <pre><code>ROWS BETWEEN CURRENT ROW AND 2 FOLLOWING\n</code></pre> <p>In fact, we can also specify the cumulative window that we saw earlier by using <code>UNBOUNDED PRECEDING</code> (this is the default window size):</p> <pre><code>ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n</code></pre> <p>Warning</p> <p>The syntax for defining window functions is actually very flexible, and there are a few different ways to specify the window. The examples above are just the most common ways to do it to keep things simple for this course.</p>"},{"location":"from-excel-to-sql/main-concepts/window-functions/#define-common-windows-with-the-window-clause","title":"Define common windows with the <code>WINDOW</code> clause","text":"<p>If you find yourself using the same window definition in multiple places of the same statement, you can define it once in the <code>WINDOW</code> clause and then reference it by name:</p> <pre><code>SELECT TOP 10\n    SalesOrderID,\n    AVG(SubTotal) OVER LastThreeSalesOrderIDs AS SubTotalMovingAverage,\n    AVG(TaxAmt) OVER LastThreeSalesOrderIDs AS TaxAmtMovingAverage,\n    AVG(Freight) OVER LastThreeSalesOrderIDs AS FreightMovingAverage,\n    AVG(TotalDue) OVER LastThreeSalesOrderIDs AS TotalDueMovingAverage\nFROM Sales.SalesOrderHeader\nWINDOW LastThreeSalesOrderIDs AS (\n    ORDER BY SalesOrderID\n    ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n)\n</code></pre> SalesOrderID SubTotalMovingAverage TaxAmtMovingAverage FreightMovingAverage TotalDueMovingAverage 43659 20565.6206 1971.5149 616.0984 23153.2339 43660 10929.9367 1047.8816 327.4630 12305.2813 43661 18195.4507 1749.8442 546.8263 20492.1213 43662 20951.0868 2017.7275 630.5398 23599.3541 43663 20659.4888 1989.7341 621.7919 23271.0148 43664 17894.8655 1720.1416 537.5442 20152.5513 43665 13068.2796 1253.7343 391.7919 14713.8059 43666 14613.9565 1402.4365 438.2614 16454.6544 43667 8505.4476 816.1459 255.0456 9576.6391 43668 15702.5759 1511.4201 472.3188 17686.3148"},{"location":"from-excel-to-sql/main-concepts/window-functions/#you-can-use-partition-by-and-order-by-together","title":"You can use <code>PARTITION BY</code> and <code>ORDER BY</code> together","text":"<p>Wherever you're defining a window, you can use <code>PARTITION BY</code> and <code>ORDER BY</code> together to define the window.</p> <p>There are a few use-cases for this, but a great one is to compute within-year running totals.</p> <p>Consider the following query which shows the quarterly sales totals for 2012 and 2013:</p> <pre><code>SELECT\n    YEAR(OrderDate) AS OrderYear,\n    DATEPART(QUARTER, OrderDate) AS OrderQuarter,\n    SUM(TotalDue) AS TotalSales\nFROM Sales.SalesOrderHeader\nWHERE YEAR(OrderDate) IN (2012, 2013)\nGROUP BY\n    YEAR(OrderDate),\n    DATEPART(QUARTER, OrderDate)\nORDER BY\n    OrderYear,\n    OrderQuarter\n;\n</code></pre> OrderYear OrderQuarter TotalSales 2012 1 9443736.8161 2012 2 9935495.1729 2012 3 10164406.8281 2012 4 8132061.4949 2013 1 8771886.3577 2013 2 12225061.3830 2013 3 14339319.1851 2013 4 13629621.0374 <p>We'll whack this into a CTE to make using it easier, and then we can use the <code>ORDER BY</code> and <code>PARTITION BY</code> clauses to calculate the running total for each year:</p> <pre><code>WITH YearlySales AS (\n    SELECT\n        YEAR(OrderDate) AS OrderYear,\n        DATEPART(QUARTER, OrderDate) AS OrderQuarter,\n        SUM(TotalDue) AS TotalSales\n    FROM Sales.SalesOrderHeader\n    WHERE YEAR(OrderDate) IN (2012, 2013)\n    GROUP BY\n        YEAR(OrderDate),\n        DATEPART(QUARTER, OrderDate)\n)\n\nSELECT\n    OrderYear,\n    OrderQuarter,\n    TotalSales,\n    SUM(TotalSales) OVER (\n        PARTITION BY OrderYear\n        ORDER BY OrderQuarter\n    ) AS WithinYearRunningTotal\nFROM YearlySales\nORDER BY\n    OrderYear,\n    OrderQuarter\n;\n</code></pre> OrderYear OrderQuarter TotalSales WithinYearRunningTotal 2012 1 9443736.8161 9443736.8161 2012 2 9935495.1729 19379231.9890 2012 3 10164406.8281 29543638.8171 2012 4 8132061.4949 37675700.3120 2013 1 8771886.3577 8771886.3577 2013 2 12225061.3830 20996947.7407 2013 3 14339319.1851 35336266.9258 2013 4 13629621.0374 48965887.9632 <p>Super simple!</p>"},{"location":"from-excel-to-sql/main-concepts/window-functions/#further-reading","title":"Further reading","text":"<p>Check out the official Microsoft documentation for more information on the <code>OVER</code> clause at:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/t-sql/queries/select-over-clause-transact-sql</li> </ul> <p>Microsoft SQL Server splits the window functions into two categories (other than aggregate functions), ranking functions and analytic functions:</p> <ul> <li>https://learn.microsoft.com/en-us/sql/t-sql/functions/ranking-functions-transact-sql</li> <li>https://learn.microsoft.com/en-us/sql/t-sql/functions/analytic-functions-transact-sql</li> </ul> <p>The video version of this content is also available at:</p> <ul> <li>https://youtu.be/8e4mQfEDJDk</li> </ul>"}]}